{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Dataframe Import\n",
    "#Open Corpus of News Article Text\n",
    "with open('../news_articles_scrape/data_frames/bitcoin_news_text_dates_combined_df.pickle', 'rb') as file:\n",
    "     bitcoin_news_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to preprocessing\n",
    "\n",
    "1- Remove capitalization and punctuation\n",
    "\n",
    "2- Remove overfit words/phrases (including source names, format-specific words (e.g., one source listed the day of the week in the first line of every article), and phrases contained in every article –usually a header/footer); \n",
    "\n",
    "3- Remove short words (words less than 3 characters long); \n",
    "\n",
    "4- Remove stop words.\n",
    "\n",
    "5- Convert numbers into words or removing numbers\n",
    "\n",
    "6- Expanding abbreviations\n",
    "\n",
    "7- Text canoncalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Real News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation except for period function\n",
    "def remove_punctuation_and_lower_case(string): \n",
    "  \n",
    "    # punctuation marks \n",
    "    punctuations = '''\\n“”!()-[]{};:'\"\\,<>/?@#$%^&*_~–'''\n",
    "  \n",
    "    # traverse the given string and if any punctuation \n",
    "    # marks occur replace it with null \n",
    "    string = string.lower()\n",
    "    \n",
    "    for x in string: \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "  \n",
    "    # Return string without punctuation \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numebrs and make text lower case, remove punctuation from article text\n",
    "bitcoin_news_df.text = bitcoin_news_df.text.apply(lambda x: remove_punctuation_and_lower_case(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only sentences that contain a number in them \n",
    "def sentences_with_string(string):\n",
    "    #Split Article by sentence\n",
    "    string = nltk.sent_tokenize(string)\n",
    "    \n",
    "    return_list=[]\n",
    "    \n",
    "    for x in string:\n",
    "        if bool(re.search(r'\\d', x)) == True:\n",
    "            return_list.append(x)\n",
    "        else: \n",
    "            pass\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news_df['sent_with_num'] = bitcoin_news_df['text'].apply(lambda x: sentences_with_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Number from Text Column and Tokenize Column\n",
    "# Text preprocessing steps - remove numebrs and make text lower case, remove punctuation from article text\n",
    "import re\n",
    "import string\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "bitcoin_news_df['text'] = bitcoin_news_df.text.map(alphanumeric).map(punc_lower)\n",
    "\n",
    "#Remove punctuation from text\n",
    "bitcoin_news_df['text'] = bitcoin_news_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "\n",
    "#Remove All Spaces\n",
    "bitcoin_news_df['text'] = bitcoin_news_df['text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "`process of splitting the given text into smaller pieces called tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Tokeniz individual texts and remove stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Words\n",
    "bitcoin_news_df['tokenized_text'] = bitcoin_news_df['text'].apply(word_tokenize)\n",
    "\n",
    "#Remove Stop Words\n",
    "bitcoin_news_df['tokenized_text']= bitcoin_news_df['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>sent_with_num</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tfid_vec_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>pos_text</th>\n",
       "      <th>tokenized_sent_with_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>initial coin offerings icos have raised billio...</td>\n",
       "      <td>2019-05-31 14:34:00+01:00</td>\n",
       "      <td>[initial coin offerings icos have raised 20 bi...</td>\n",
       "      <td>[initial, coin, offerings, icos, raised, billi...</td>\n",
       "      <td>(0, 127992)\\t0.024305504438058914\\n  (0, 127...</td>\n",
       "      <td>[initi, coin, offer, ico, rais, billion, sinc,...</td>\n",
       "      <td>[initial, coin, offering, icos, raised, billio...</td>\n",
       "      <td>[[(initial, JJ)], [(coin, NN)], [(offerings, N...</td>\n",
       "      <td>[[initial, coin, offerings, icos, have, raised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watts miners started its operation in the begi...</td>\n",
       "      <td>2019-05-26 13:10:00+01:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[watts, miners, started, operation, beginning,...</td>\n",
       "      <td>(0, 128438)\\t0.10179798676343357\\n  (0, 1283...</td>\n",
       "      <td>[watt, miner, start, oper, begin, cryptotechno...</td>\n",
       "      <td>[watt, miner, started, operation, beginning, c...</td>\n",
       "      <td>[[(watts, NN)], [(miners, NNS)], [(started, VB...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google call screen promises to help you deal w...</td>\n",
       "      <td>2019-06-01 20:37:00+01:00</td>\n",
       "      <td>[bitcoin mining consumes a lot of energy with ...</td>\n",
       "      <td>[google, call, screen, promises, help, deal, c...</td>\n",
       "      <td>(0, 128398)\\t0.040594752388170174\\n  (0, 128...</td>\n",
       "      <td>[googl, call, screen, promis, help, deal, call...</td>\n",
       "      <td>[google, call, screen, promise, help, deal, ca...</td>\n",
       "      <td>[[(google, NN)], [(call, NN)], [(screen, NN)],...</td>\n",
       "      <td>[[bitcoin, mining, consumes, a, lot, of, energ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friday may most of the top cryptocurrencies ar...</td>\n",
       "      <td>None</td>\n",
       "      <td>[friday may 31 — most of the top 20 cryptocurr...</td>\n",
       "      <td>[friday, may, top, cryptocurrencies, reporting...</td>\n",
       "      <td>(0, 128331)\\t0.05846390674965055\\n  (0, 1276...</td>\n",
       "      <td>[friday, may, top, cryptocurr, report, moder, ...</td>\n",
       "      <td>[friday, may, top, cryptocurrencies, reporting...</td>\n",
       "      <td>[[(friday, NN)], [(may, MD)], [(top, NN)], [(c...</td>\n",
       "      <td>[[friday, may, 31, —, most, of, the, top, 20, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>highend swiss watchmaker franck muller has par...</td>\n",
       "      <td>2019-06-02 11:30:19+00:00</td>\n",
       "      <td>[highend swiss watchmaker franck muller has pa...</td>\n",
       "      <td>[highend, swiss, watchmaker, franck, muller, p...</td>\n",
       "      <td>(0, 128167)\\t0.03309387246976487\\n  (0, 1279...</td>\n",
       "      <td>[highend, swiss, watchmak, franck, muller, par...</td>\n",
       "      <td>[highend, swiss, watchmaker, franck, muller, p...</td>\n",
       "      <td>[[(highend, NN)], [(swiss, JJ)], [(watchmaker,...</td>\n",
       "      <td>[[highend, swiss, watchmaker, franck, muller, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  initial coin offerings icos have raised billio...   \n",
       "1  watts miners started its operation in the begi...   \n",
       "2  google call screen promises to help you deal w...   \n",
       "3  friday may most of the top cryptocurrencies ar...   \n",
       "4  highend swiss watchmaker franck muller has par...   \n",
       "\n",
       "                        date  \\\n",
       "0  2019-05-31 14:34:00+01:00   \n",
       "1  2019-05-26 13:10:00+01:00   \n",
       "2  2019-06-01 20:37:00+01:00   \n",
       "3                       None   \n",
       "4  2019-06-02 11:30:19+00:00   \n",
       "\n",
       "                                       sent_with_num  \\\n",
       "0  [initial coin offerings icos have raised 20 bi...   \n",
       "1                                                 []   \n",
       "2  [bitcoin mining consumes a lot of energy with ...   \n",
       "3  [friday may 31 — most of the top 20 cryptocurr...   \n",
       "4  [highend swiss watchmaker franck muller has pa...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [initial, coin, offerings, icos, raised, billi...   \n",
       "1  [watts, miners, started, operation, beginning,...   \n",
       "2  [google, call, screen, promises, help, deal, c...   \n",
       "3  [friday, may, top, cryptocurrencies, reporting...   \n",
       "4  [highend, swiss, watchmaker, franck, muller, p...   \n",
       "\n",
       "                                       tfid_vec_text  \\\n",
       "0    (0, 127992)\\t0.024305504438058914\\n  (0, 127...   \n",
       "1    (0, 128438)\\t0.10179798676343357\\n  (0, 1283...   \n",
       "2    (0, 128398)\\t0.040594752388170174\\n  (0, 128...   \n",
       "3    (0, 128331)\\t0.05846390674965055\\n  (0, 1276...   \n",
       "4    (0, 128167)\\t0.03309387246976487\\n  (0, 1279...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  [initi, coin, offer, ico, rais, billion, sinc,...   \n",
       "1  [watt, miner, start, oper, begin, cryptotechno...   \n",
       "2  [googl, call, screen, promis, help, deal, call...   \n",
       "3  [friday, may, top, cryptocurr, report, moder, ...   \n",
       "4  [highend, swiss, watchmak, franck, muller, par...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  [initial, coin, offering, icos, raised, billio...   \n",
       "1  [watt, miner, started, operation, beginning, c...   \n",
       "2  [google, call, screen, promise, help, deal, ca...   \n",
       "3  [friday, may, top, cryptocurrencies, reporting...   \n",
       "4  [highend, swiss, watchmaker, franck, muller, p...   \n",
       "\n",
       "                                            pos_text  \\\n",
       "0  [[(initial, JJ)], [(coin, NN)], [(offerings, N...   \n",
       "1  [[(watts, NN)], [(miners, NNS)], [(started, VB...   \n",
       "2  [[(google, NN)], [(call, NN)], [(screen, NN)],...   \n",
       "3  [[(friday, NN)], [(may, MD)], [(top, NN)], [(c...   \n",
       "4  [[(highend, NN)], [(swiss, JJ)], [(watchmaker,...   \n",
       "\n",
       "                             tokenized_sent_with_num  \n",
       "0  [[initial, coin, offerings, icos, have, raised...  \n",
       "1                                                 []  \n",
       "2  [[bitcoin, mining, consumes, a, lot, of, energ...  \n",
       "3  [[friday, may, 31, —, most, of, the, top, 20, ...  \n",
       "4  [[highend, swiss, watchmaker, franck, muller, ...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Column to store tokenized sentence with numbers. Will be used during sentiment analysis\n",
    "bitcoin_news_df['tokenized_sent_with_num'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=[]\n",
    "for count,ele in enumerate(bitcoin_news_df.sent_with_num): \n",
    "    for sent in ele:\n",
    "        tokenized.append(word_tokenize(sent))\n",
    "    bitcoin_news_df['tokenized_sent_with_num'][count] = tokenized\n",
    "    tokenized=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfid Vectorization\n",
    "- Gives the relative importance of a term in a corpus (text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid_vectorization function\n",
    "def tfid_vectorization(df, column_to_vectorize=None, vectorized_name=None):\n",
    "    # list of text documents\n",
    "    article= df[column_to_vectorize]\n",
    "\n",
    "    # create the transform\n",
    "    vectorizer= TfidfVectorizer()\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(article)\n",
    "\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    #print(vectorizer.idf_)\n",
    "\n",
    "    # # # encode documents\n",
    "    df[vectorized_name] = article.apply(lambda x: vectorizer.transform([x]))\n",
    "    \n",
    "    # # summarize encoded vector\n",
    "    #print(df[vectorized_name].shape)\n",
    "    #print(df[vectorized_name].toarray())\n",
    "    \n",
    "    print('Tfid Vectorization Completed \\n')\n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid vectorization of document text\n",
    "tfid_vectorization(bitcoin_news_df,'text','tfid_vec_text');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Normalization\n",
    "- Convert all disparities of a word into their normalized form as a part of feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using NLTK\n",
    "-Stemming is a process of linguistic normalization, which reduces words to their root word or chops off the derviational affices. For example, connection, connected, connecting word reduce to a common word 'connect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Stemming Text\n",
    "bitcoin_news_df['stemmed_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Lemmatizing Text\n",
    "bitcoin_news_df['lemmatized_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS and Chunking Text\n",
    "- Helps overcome bagofwords weakness which fails to capture the structure of sentences and sometimes gives its appropriate meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS\n",
    "#POS Text\n",
    "bitcoin_news_df['pos_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [nltk.pos_tag([y]) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty column to add POS of Tokenized Sentences with Numbers\n",
    "bitcoin_news_df['pos_sent_with_num'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = []\n",
    "for count, elem in enumerate(bitcoin_news_df.tokenized_sent_with_num):\n",
    "    for sent in elem:\n",
    "        parts_of_speech.append(nltk.pos_tag(sent))\n",
    "    bitcoin_news_df['pos_sent_with_num'][count] = parts_of_speech\n",
    "    parts_of_speech = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort Dataframe By Date\n",
    "bitcoin_news_df.sort_values('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Dataframe Row Index\n",
    "bitcoin_news_df = bitcoin_news_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Updated Data Frame\n",
    "with open('./data_frames/bitcoin_news_df_processed_for_modeling.pickle', 'wb') as file:\n",
    "     pickle.dump(bitcoin_news_df, file)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
