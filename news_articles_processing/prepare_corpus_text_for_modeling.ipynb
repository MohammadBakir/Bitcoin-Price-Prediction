{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Dataframe Import\n",
    "#Open Corpus of News Article Text\n",
    "with open('../news_articles_scrape/data_frames/bitcoin_news_text_dates_combined_df.pickle', 'rb') as file:\n",
    "     bitcoin_news_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to preprocessing\n",
    "\n",
    "1- Remove capitalization and punctuation\n",
    "\n",
    "2- Remove overfit words/phrases (including source names, format-specific words (e.g., one source listed the day of the week in the first line of every article), and phrases contained in every article â€“usually a header/footer); \n",
    "\n",
    "3- Remove short words (words less than 3 characters long); \n",
    "\n",
    "4- Remove stop words.\n",
    "\n",
    "5- Convert numbers into words or removing numbers\n",
    "\n",
    "6- Expanding abbreviations\n",
    "\n",
    "7- Text canoncalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Real News Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Column Without Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.search(\"\\d\", bitcoin_news_df.text[0]).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'initial coin offerings icos have raised 20 billion since the start of 2017 which is 18 billion more than the previous year according to a recent study by financial research firm autonomous research the study dubbed crypto utopia explores the cryptocurrency industry over the past year focusing on icos and the regulation to which they are exposed per the study 12 billion has been raised through icos in the course of 2018 while last year they raised 7 billion the icos of blockchain protocol eos and messaging app telegram are responsible for almost half of all ico funds in 2018 at 4 2 billion and 1 7 billion respectively though over 300 crypto funds have been launched to invest in crypto assets a vast majority of funds are concentrated within a small minority of organizations according to autonomous the research notes that icos are often exposed to fraud and scams which form 20 percent of project white papers while phishing and hacking are responsible for stealing 15 percent of all crypto assets by market capitalization more than 50 percent of icos have failed to raise funds and subsequently have closed 2017 saw over 7 billion of investment flow into icos which is fourfold greater that equity investment in crypto companies many icos were purportedly launched to take advantage of the goldrush subsequently resulting in quality and regulatory concerns regarding tokens price performance for the top 200 liquid coins during the last 1 5 years has reportedly demonstrated an unprecedented surge from 10 to 1 million percent the authors of the study suggest that such a performance shows exponential software like growth for digital currencies the study states that venture and trading funds are the most numerous and hold the most assets under management another study by autonomous research published last month stated that funding in icos has seen its hardest slump in 16 months stating that in august startups raised 326 million which is the smallest amount since may 2017 in august icorating published a study showing that the ico market more than doubled in a year icos in q12 2018 had already raised over 11 billion in investments a figure which it purports is ten times larger than the sum of investments from icos in q12 2017'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_news_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numebrs and make text lower case, remove punctuation from article text\n",
    "import re\n",
    "import string\n",
    "\n",
    "# alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "bitcoin_news_df['text'] = bitcoin_news_df.text.map(punc_lower)\n",
    "\n",
    "# bitcoin_news_df['text'] = bitcoin_news_df.text.map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation from text\n",
    "bitcoin_news_df['text'] = bitcoin_news_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove All Spaces\n",
    "bitcoin_news_df['text'] = bitcoin_news_df['text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove words of length 3 or less\n",
    "# bitcoin_news_df['text']= bitcoin_news_df['text'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text will be tokenized twice for the following reasons. \n",
    "We will first tokenized with numbers included in the text. Once text is tokenized with number, the 5 words preceding and following a number will be extracted from analysis. \n",
    "\n",
    "After this is complete, all numbers will be removed from the tokenized text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "`process of splitting the given text into smaller pieces called tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words and tokeniz individual texts\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "bitcoin_news_df['tokenized_text'] = bitcoin_news_df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "bitcoin_news_df['tokenized_text']= bitcoin_news_df['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfid Vectorization\n",
    "- Gives the relative importance of a term in a corpus (text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid_vectorization function\n",
    "def tfid_vectorization(df, column_to_vectorize=None, vectorized_name=None):\n",
    "    # list of text documents\n",
    "    article= df[column_to_vectorize]\n",
    "\n",
    "    # create the transform\n",
    "    vectorizer= TfidfVectorizer()\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(article)\n",
    "\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    #print(vectorizer.idf_)\n",
    "\n",
    "    # # # encode documents\n",
    "    df[vectorized_name] = article.apply(lambda x: vectorizer.transform([x]))\n",
    "    \n",
    "    # # summarize encoded vector\n",
    "    #print(df[vectorized_name].shape)\n",
    "    #print(df[vectorized_name].toarray())\n",
    "    \n",
    "    print('Tfid Vectorization Completed \\n')\n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid vectorization of document text\n",
    "tfid_vectorization(bitcoin_news_df,'text','tfid_vec_text');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Normalization\n",
    "- Convert all disparities of a word into their normalized form as a part of feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using NLTK\n",
    "-Stemming is a process of linguistic normalization, which reduces words to their root word or chops off the derviational affices. For example, connection, connected, connecting word reduce to a common word 'connect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Stemming Text\n",
    "bitcoin_news_df['stemmed_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Lemmatizing Text\n",
    "bitcoin_news_df['lemmatized_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS and Chunking Text\n",
    "- Helps overcome bagofwords weakness which fails to capture the structure of sentences and sometimes gives its appropriate meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS\n",
    "#POS Text\n",
    "bitcoin_news_df['pos_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [nltk.pos_tag([y]) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Updated Data Frame\n",
    "with open('./data_frames/bitcoin_news_df_preprocessed.pickle', 'wb') as file:\n",
    "     pickle.dump(bitcoin_news_df, file)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
