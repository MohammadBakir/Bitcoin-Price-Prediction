{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Dataframe Import\n",
    "#Open Corpus of News Article Text\n",
    "with open('../news_articles_scrape/data_frames/bitcoin_news_text_dates_combined_df.pickle', 'rb') as file:\n",
    "     bitcoin_news_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to preprocessing\n",
    "\n",
    "1- Remove Articles Without Dates\n",
    "\n",
    "2- Remove capitalization and punctuation\n",
    "\n",
    "3- Remove overfit words/phrases (including source names, format-specific words (e.g., one source listed the day of the week in the first line of every article), and phrases contained in every article –usually a header/footer); \n",
    "\n",
    "4- Remove short words (words less than 3 characters long); \n",
    "\n",
    "5- Remove stop words.\n",
    "\n",
    "6- Convert numbers into words or removing numbers\n",
    "\n",
    "7- Expanding abbreviations\n",
    "\n",
    "8- Text canoncalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Real News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation except for period function\n",
    "def remove_punctuation_and_lower_case(string): \n",
    "  \n",
    "    # punctuation marks \n",
    "    punctuations = '''\\n“”!()-[]{};:'\"\\,<>/?@#$%^&*_~–'''\n",
    "  \n",
    "    # traverse the given string and if any punctuation \n",
    "    # marks occur replace it with null \n",
    "    string = string.lower()\n",
    "    \n",
    "    for x in string: \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "  \n",
    "    # Return string without punctuation \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numebrs and make text lower case, remove punctuation from article text\n",
    "bitcoin_news_df.text = bitcoin_news_df.text.apply(lambda x: remove_punctuation_and_lower_case(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only sentences that contain a number in them \n",
    "def sentences_with_string(string):\n",
    "    #Split Article by sentence\n",
    "    string = nltk.sent_tokenize(string)\n",
    "    \n",
    "    return_list=[]\n",
    "    \n",
    "    for x in string:\n",
    "        if bool(re.search(r'\\d', x)) == True:\n",
    "            return_list.append(x)\n",
    "        else: \n",
    "            pass\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news_df['sent_with_num'] = bitcoin_news_df['text'].apply(lambda x: sentences_with_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Number from Text Column and Tokenize Column\n",
    "# Text preprocessing steps - remove numebrs and make text lower case, remove punctuation from article text\n",
    "import re\n",
    "import string\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "bitcoin_news_df['text'] = bitcoin_news_df.text.map(alphanumeric).map(punc_lower)\n",
    "\n",
    "#Remove punctuation from text\n",
    "bitcoin_news_df['text'] = bitcoin_news_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "\n",
    "#Remove All Spaces\n",
    "bitcoin_news_df['text'] = bitcoin_news_df['text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep rows with a date and remove all else\n",
    "bitcoin_news_df = bitcoin_news_df[(bitcoin_news_df.date != '0001-11-30 00:00:00+00:00') & (bitcoin_news_df.date !='None')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strip Timezone Info\n",
    "bitcoin_news_df.date = bitcoin_news_df.date.astype(str).str[:-6]\n",
    "\n",
    "#Convert all string dates to datetime\n",
    "bitcoin_news_df.date = pd.to_datetime(bitcoin_news_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Dataframe Row Index\n",
    "bitcoin_news_df = bitcoin_news_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort Dataframe By Date\n",
    "bitcoin_news_df.sort_values('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round date to nearest hour\n",
    "def hour_rounder(t):\n",
    "    # Rounds to nearest hour by adding a timedelta hour if minute >= 30\n",
    "    return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour)\n",
    "               +timedelta(hours=t.minute//30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news_df.date = bitcoin_news_df.date.apply(hour_rounder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "`process of splitting the given text into smaller pieces called tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Tokeniz individual texts and remove stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Words\n",
    "bitcoin_news_df['tokenized_text'] = bitcoin_news_df['text'].apply(word_tokenize)\n",
    "\n",
    "#Remove Stop Words\n",
    "bitcoin_news_df['tokenized_text']= bitcoin_news_df['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Column to store tokenized sentence with numbers. Will be used during sentiment analysis\n",
    "bitcoin_news_df['tokenized_sent_with_num'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=[]\n",
    "for count,ele in enumerate(bitcoin_news_df.sent_with_num): \n",
    "    for sent in ele:\n",
    "        tokenized.append(word_tokenize(sent))\n",
    "    bitcoin_news_df['tokenized_sent_with_num'][count] = tokenized\n",
    "    tokenized=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfid Vectorization\n",
    "- Gives the relative importance of a term in a corpus (text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid_vectorization function\n",
    "def tfid_vectorization(df, column_to_vectorize=None, vectorized_name=None):\n",
    "    # list of text documents\n",
    "    article= df[column_to_vectorize]\n",
    "\n",
    "    # create the transform\n",
    "    vectorizer= TfidfVectorizer()\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(article)\n",
    "\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    #print(vectorizer.idf_)\n",
    "\n",
    "    # # # encode documents\n",
    "    df[vectorized_name] = article.apply(lambda x: vectorizer.transform([x]))\n",
    "    \n",
    "    # # summarize encoded vector\n",
    "    #print(df[vectorized_name].shape)\n",
    "    #print(df[vectorized_name].toarray())\n",
    "    \n",
    "    print('Tfid Vectorization Completed \\n')\n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid vectorization of document text\n",
    "tfid_vectorization(bitcoin_news_df,'text','tfid_vec_text');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Normalization\n",
    "- Convert all disparities of a word into their normalized form as a part of feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using NLTK\n",
    "-Stemming is a process of linguistic normalization, which reduces words to their root word or chops off the derviational affices. For example, connection, connected, connecting word reduce to a common word 'connect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Stemming Text\n",
    "bitcoin_news_df['stemmed_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Lemmatizing Text\n",
    "bitcoin_news_df['lemmatized_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS and Chunking Text\n",
    "- Helps overcome bagofwords weakness which fails to capture the structure of sentences and sometimes gives its appropriate meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS\n",
    "#POS Text\n",
    "bitcoin_news_df['pos_text'] = bitcoin_news_df['tokenized_text'].apply(lambda x: [nltk.pos_tag([y]) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty column to add POS of Tokenized Sentences with Numbers\n",
    "bitcoin_news_df['pos_sent_with_num'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = []\n",
    "for count, elem in enumerate(bitcoin_news_df.tokenized_sent_with_num):\n",
    "    for sent in elem:\n",
    "        parts_of_speech.append(nltk.pos_tag(sent))\n",
    "    bitcoin_news_df['pos_sent_with_num'][count] = parts_of_speech\n",
    "    parts_of_speech = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Updated Data Frame\n",
    "with open('./data_frames/bitcoin_news_df_processed_for_modeling.pickle', 'wb') as file:\n",
    "     pickle.dump(bitcoin_news_df, file)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
