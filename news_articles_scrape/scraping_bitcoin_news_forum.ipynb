{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following Cell contains a function that will scrape bitcoin talk forum using selenium webdriver for links to individual posts that users generate to discuss bitcoin news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set chromedriver executable path. \n",
    "chromedriver = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#Define function to extract forum discussion links. \n",
    "def get_bitcoin_news_links(url):\n",
    "    \"\"\"\n",
    "    A function utilizing selenium to scrape the links to individual posts within a forum table.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    print('Scraping Bitcoin News Links\\n')\n",
    "    \n",
    "    #Empty list to store forum post links\n",
    "    a_elements = []\n",
    "    \n",
    "    #The forum posting links begin with 77.'# of posts ' and increment by 40. Will loop through to obtain last ~34,000 links\n",
    "    i = 0\n",
    "    while i <= 34160:\n",
    "        #Establish link\n",
    "        bitcoinnews_url=url+str(i)\n",
    "        \n",
    "        #Utilize selenium drive to launch link\n",
    "        driver.get(bitcoinnews_url)\n",
    "        table_id = driver.find_element(By.XPATH,'//*[@id=\"bodyarea\"]/div[2]/table/tbody') #Define xpath to forum article discussion\n",
    "        rows = table_id.find_elements_by_tag_name(\"tr\")\n",
    "    \n",
    "        for row in rows:\n",
    "            link = row.find_element(By.TAG_NAME, \"a\")\n",
    "            a_elements.append(link.get_attribute('href'))\n",
    "        i+=40\n",
    "        \n",
    "    print('Scraping Done!\\n')\n",
    "    \n",
    "    return a_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define site URL for scraping\n",
    "url=\"https://bitcointalk.org/index.php?board=77.\"\n",
    "\n",
    "#Scrap News Article Posts. Only run once then comment out, will take approximately 5-10min\n",
    "forum_subject_links = get_bitcoin_news_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove First Few Links that are forum overhead links (example, Privacy Policy, Terms of Agreement, Rules, ect.)\n",
    "n = 4\n",
    "del forum_subject_links[:n]\n",
    "\n",
    "#Remove Access Links\n",
    "forum_subject_links = [x for x in forum_subject_links if \"sort=subject\" not in x]\n",
    "\n",
    "#Save forumn links list\n",
    "with open('./article_data/forum_subject_links.pkl', 'wb') as f:\n",
    "    pickle.dump(forum_subject_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Post Links\n",
    "with open('./data_frames/forum_subject_links.pkl', 'rb') as file:\n",
    "     forum_subject_links = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell defines a function that scrapes the individual posts located above for the news link mentioned in the posts. This was done in parts. Then saved as lists to drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to extract news article links from individual forum discussions\n",
    "def get_article_links_from_forumns():\n",
    "    \"\"\"\n",
    "    Function utilizing selenium webdriver to scrape the news article link posted within forum posts. \n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    print('Scraping Bitcoin News Links\\n')\n",
    "    \n",
    "    href_elements = []\n",
    "    for url in forum_subject_links[15000:30000]:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(.5)\n",
    "            post_location = driver.find_element(By.XPATH,'//*[@id=\"quickModForm\"]/table[1]/tbody/tr[1]/td/table/tbody/tr/td/table/tbody/tr[1]/td[2]/div')\n",
    "            a_element = post_location.find_element_by_tag_name('a')\n",
    "            href = a_element.get_attribute('href')\n",
    "            href_elements.append(href)\n",
    "        except:\n",
    "            pass\n",
    "   \n",
    "\n",
    "    print('Scraping Done!')\n",
    "    return href_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Bitcoin News Links\n",
      "\n",
      "Scraping Done!\n"
     ]
    }
   ],
   "source": [
    "news_article_links_15000_30000 = get_article_links_from_forumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate empty list to store links. \n",
    "news_article_links =[]\n",
    "# news_article_links.append(news_article_links_0_2000)\n",
    "# news_article_links.append(news_article_links_4000_10000)\n",
    "# news_article_links.append(news_article_links_10000_15000)\n",
    "news_article_links.append(news_article_links_15000_30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save News Links\n",
    "with open('./data_frames/news_article_links_15000_30000.pkl', 'wb') as f:\n",
    "    pickle.dump(news_article_links, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
