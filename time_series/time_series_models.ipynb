{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import math\n",
    "\n",
    "# import pyflux as pf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM\n",
    "#from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2019)\n",
    "set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('./processed_data/df_combined.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>vix</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08 00:00:00</td>\n",
       "      <td>1054.03</td>\n",
       "      <td>18.879</td>\n",
       "      <td>2732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-08 01:00:00</td>\n",
       "      <td>1060.48</td>\n",
       "      <td>18.915</td>\n",
       "      <td>2693.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds        y     vix    gold\n",
       "0 2017-02-08 00:00:00  1054.03  18.879  2732.0\n",
       "1 2017-02-08 01:00:00  1060.48  18.915  2693.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_chronological(df, ratio = 0.9, use_ratio = True, index = 1000):\n",
    "    \n",
    "    '''\n",
    "    Input is a dataframe, and a ratio. Splits dataframe into 2 dataframes chronologically.\n",
    "    Returns first dataframe up to the index of the length of the input dataframe times the input ratio, \n",
    "    and returns second dataframe of remaining elements.\n",
    "    use_ratio is a flag, wether ratio should be used or indicies instead.\n",
    "    \n",
    "    df = input dataframe\n",
    "    ratio = ratio to be used for splitting\n",
    "    use_ratio = if True, use ratio, \n",
    "    index = index to split input dataframe on\n",
    "    \n",
    "    '''\n",
    "    if use_ratio:\n",
    "        size = len(df) * ratio\n",
    "        size_round = round(size)\n",
    "\n",
    "        df_train = df[0:(size_round)]\n",
    "        df_test = df[size_round:]\n",
    "    else:\n",
    "        df_train = df[0:(index)]\n",
    "        df_test = df[index:]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(train_perc, stocks_to_trade, return_df_2):\n",
    "    train_len = int(return_df.shape[0] * (train_perc))\n",
    "    volume_to_trade = ['v_'+ticker for ticker in stocks_to_trade]\n",
    "    for i in volume_to_trade:\n",
    "        stocks_to_trade.append(i)\n",
    "\n",
    "    train = return_df_2[stocks_to_trade][1:train_len].copy()\n",
    "    train['diff'] = train[stocks_to_trade[0]] - train[stocks_to_trade[1]]\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    test = return_df_2[stocks_to_trade][train_len:-400].copy()\n",
    "    test['diff'] = test[stocks_to_trade[0]] - test[stocks_to_trade[1]]\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    true_test = return_df_2[stocks_to_trade][-400:].copy()\n",
    "    true_test['diff'] = true_test[stocks_to_trade[0]] - true_test[stocks_to_trade[1]]\n",
    "    true_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    feature_names = volume_to_trade\n",
    "    feature_names.append('diff')\n",
    "    \n",
    "    return train, test, true_test, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(df, lags, look_back, predict_window):\n",
    "    \n",
    "    '''\n",
    "    In order to use keras LSTM, we need to convert the input into a keras-friendly input.\n",
    "    \n",
    "    df = input dataframe\n",
    "    lags = number of lags\n",
    "    look_back = number of preceding elements to be considered\n",
    "    predict_window = size of window for predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    features = lags + 1\n",
    "    start = look_back\n",
    "    stop = len(df) - lags - predict_window\n",
    "\n",
    "    lstm_in_X = np.zeros(shape=(stop-start, look_back+1, features))\n",
    "    lstm_in_Y = np.zeros(shape=(stop-start, look_back+1))\n",
    "\n",
    "    iter_list = [num for num in range(look_back+1)][::-1]\n",
    "    for i in range(start, stop):\n",
    "        for index, j in enumerate(iter_list):\n",
    "            X = df[i - j : i - j + lags + 1, -1]\n",
    "            lstm_in_X[i - start, index] = np.ravel(X)\n",
    "            Y = df[i - j + lags + 1, -1]\n",
    "            lstm_in_Y[i-start, index] = Y\n",
    "            \n",
    "    return lstm_in_X, lstm_in_Y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(train_X, train_Y, lags, look_back, predict_window, lstm_nodes,\n",
    "               dense_layers, dropout = 0.1, loss_type = 'hinge', optimizer_type = 'adam',\n",
    "               number_epochs = 300, batch_size = 64, ):\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_nodes, input_shape=(look_back+1, features)))\n",
    "    model.add(Dropout(dropout))\n",
    "    for nodes in dense_layers:\n",
    "        model.add(Dense(nodes))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(look_back + 1))\n",
    "    model.compile(loss=loss_type, optimizer= optimizer_type)\n",
    "    model.fit(train_X, train_Y, epochs=number_epochs, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "    pred_Y_train = model.predict(train_X)\n",
    "    predictions = pred_Y_train[:,-1]\n",
    "    actuals = train_Y[:,-1]\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model, dataset, train_X, train_Y, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model, train_x, train_y, test_x, test_y):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(test_x[i].reshape(1,1,23), (test_y[i].reshape(1 ,))),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        train_x = np.concatenate((train_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "        le = len(test_y) + 1\n",
    "        train_y = np.concatenate((train_y, (test_y[i].reshape(1 ,))))\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_x[-1,-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     predictions_test = []\n",
    "#     actuals_test = []\n",
    "    \n",
    "#     for i in range(0, len(test_y) - 1):\n",
    "#         model.fit(train_x, train_y, \n",
    "#                     epochs= 2, \n",
    "#                     batch_size = 128, \n",
    "# #                     validation_data=(test_x[i], test_y[i]),\n",
    "#                     verbose=2,\n",
    "# #                     callbacks=[earlystopper],\n",
    "#                     shuffle=False)\n",
    "#         pred_Y_test = model.predict(test_x)\n",
    "#         test_x = np.concatenate((test_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "#         le = len(test_y) + 1\n",
    "#         test_y = np.concatenate((test_y, (test_y[i].reshape(1 ,))))\n",
    "#         predict_test = pred_Y_test[-1,-1]\n",
    "#         actual_test = train_x[-1,-1]\n",
    "#         predictions_test.append(predict_test)\n",
    "#         actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "#     return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = split_train_test_chronological(df_combined, ratio = .99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = split_train_test_chronological(df_combined, ratio = .95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var2(t-7)</th>\n",
       "      <th>var3(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var2(t-6)</th>\n",
       "      <th>var3(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var2(t-5)</th>\n",
       "      <th>var3(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>var3(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var2(t-2)</th>\n",
       "      <th>var3(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.179461</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-7)  var2(t-7)  var3(t-7)  var1(t-6)  var2(t-6)  var3(t-6)  \\\n",
       "7    0.005247   0.179461   0.003901   0.005594   0.180275   0.001623   \n",
       "8    0.005594   0.180275   0.001623   0.005727   0.179032   0.002740   \n",
       "9    0.005727   0.179032   0.002740   0.005882   0.176997   0.004602   \n",
       "10   0.005882   0.176997   0.004602   0.006007   0.176997   0.004491   \n",
       "11   0.006007   0.176997   0.004491   0.005908   0.179438   0.004092   \n",
       "\n",
       "    var1(t-5)  var2(t-5)  var3(t-5)  var1(t-4)  ...  var3(t-3)  var1(t-2)  \\\n",
       "7    0.005727   0.179032   0.002740   0.005882  ...   0.004491   0.005908   \n",
       "8    0.005882   0.176997   0.004602   0.006007  ...   0.004092   0.005895   \n",
       "9    0.006007   0.176997   0.004491   0.005908  ...   0.003908   0.005905   \n",
       "10   0.005908   0.179438   0.004092   0.005895  ...   0.003899   0.004164   \n",
       "11   0.005895   0.179642   0.003908   0.005905  ...   0.002629   0.004071   \n",
       "\n",
       "    var2(t-2)  var3(t-2)  var1(t-1)  var2(t-1)  var3(t-1)   var1(t)   var2(t)  \\\n",
       "7    0.179438   0.004092   0.005895   0.179642   0.003908  0.005905  0.179642   \n",
       "8    0.179642   0.003908   0.005905   0.179642   0.003899  0.004164  0.179642   \n",
       "9    0.179642   0.003899   0.004164   0.179642   0.002629  0.004071  0.179642   \n",
       "10   0.179642   0.002629   0.004071   0.179642   0.003160  0.004373  0.179642   \n",
       "11   0.179642   0.003160   0.004373   0.179642   0.003959  0.004777  0.179642   \n",
       "\n",
       "     var3(t)  \n",
       "7   0.003899  \n",
       "8   0.002629  \n",
       "9   0.003160  \n",
       "10  0.003959  \n",
       "11  0.003199  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "values = df_combined.drop('ds', axis = 1).values\n",
    "# integer encode direction\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 7, 1)\n",
    "\n",
    "reframed_2 = series_to_supervised(values, 7, 1)\n",
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed.iloc[:,-11].values\n",
    "X = reframed.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop columns we don't want to predict\n",
    "# y = reframed_2.iloc[:,-11].values\n",
    "# X = reframed_2.drop('var1(t)', axis =1).values\n",
    "# # split into train and test sets\n",
    "\n",
    "# n_train_hours = 12000\n",
    "# n_test_hours = 320\n",
    "# train_X = X[:n_train_hours,:]\n",
    "# train_y = y[:n_train_hours]\n",
    "\n",
    "# val_X= X[n_train_hours:-n_test_hours,]\n",
    "# val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "# test_X = X[-n_test_hours:,:]\n",
    "# test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# # reshape input to be 3D [samples, timesteps, features]\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "# val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "# print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_3 = Sequential()\n",
    "LSTM_model_3.add(LSTM(64, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), dropout=0.05,recurrent_dropout=0.05))\n",
    "LSTM_model_3.add(Dense(64))\n",
    "# LSTM_model_3.add(LSTM(16, activation='relu'))\n",
    "LSTM_model_3.add(Dense(1))\n",
    "LSTM_model_3.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 341 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 73327.3796 - val_loss: 177.9110\n",
      "Epoch 2/100\n",
      " - 2s - loss: 2701.4853 - val_loss: 481.3569\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1154.6047 - val_loss: 560.7679\n",
      "Epoch 4/100\n",
      " - 2s - loss: 569.0086 - val_loss: 295.9989\n",
      "Epoch 5/100\n",
      " - 2s - loss: 411.9404 - val_loss: 301.1240\n",
      "Epoch 6/100\n",
      " - 2s - loss: 349.9914 - val_loss: 318.5336\n",
      "Epoch 7/100\n",
      " - 2s - loss: 311.3779 - val_loss: 296.5194\n",
      "Epoch 8/100\n",
      " - 2s - loss: 288.4732 - val_loss: 291.8084\n",
      "Epoch 9/100\n",
      " - 2s - loss: 259.1932 - val_loss: 302.5205\n",
      "Epoch 10/100\n",
      " - 2s - loss: 267.0308 - val_loss: 285.9072\n",
      "Epoch 11/100\n",
      " - 2s - loss: 261.2335 - val_loss: 305.0317\n",
      "Epoch 12/100\n",
      " - 2s - loss: 254.0558 - val_loss: 311.7035\n",
      "Epoch 13/100\n",
      " - 2s - loss: 259.2388 - val_loss: 306.1729\n",
      "Epoch 14/100\n",
      " - 2s - loss: 254.5865 - val_loss: 306.9204\n",
      "Epoch 15/100\n",
      " - 2s - loss: 247.7920 - val_loss: 299.5172\n",
      "Epoch 16/100\n",
      " - 2s - loss: 238.7192 - val_loss: 287.3755\n",
      "Epoch 17/100\n",
      " - 2s - loss: 256.7868 - val_loss: 289.1116\n",
      "Epoch 18/100\n",
      " - 2s - loss: 250.4063 - val_loss: 254.1336\n",
      "Epoch 19/100\n",
      " - 2s - loss: 287.6457 - val_loss: 225.4846\n",
      "Epoch 20/100\n",
      " - 2s - loss: 275.7153 - val_loss: 236.8632\n",
      "Epoch 21/100\n",
      " - 2s - loss: 305.6718 - val_loss: 364.4907\n",
      "Epoch 22/100\n",
      " - 2s - loss: 309.2386 - val_loss: 436.7993\n",
      "Epoch 23/100\n",
      " - 2s - loss: 319.1659 - val_loss: 579.6448\n",
      "Epoch 24/100\n",
      " - 2s - loss: 380.3493 - val_loss: 741.1658\n",
      "Epoch 25/100\n",
      " - 2s - loss: 512.1608 - val_loss: 820.9574\n",
      "Epoch 26/100\n",
      " - 2s - loss: 574.0959 - val_loss: 869.3570\n",
      "Epoch 27/100\n",
      " - 2s - loss: 478.2395 - val_loss: 710.4812\n",
      "Epoch 28/100\n",
      " - 2s - loss: 536.8005 - val_loss: 734.2089\n",
      "Epoch 29/100\n",
      " - 2s - loss: 450.4608 - val_loss: 683.6005\n",
      "Epoch 30/100\n",
      " - 2s - loss: 462.0377 - val_loss: 634.9114\n",
      "Epoch 31/100\n",
      " - 2s - loss: 478.4536 - val_loss: 630.4305\n",
      "Epoch 32/100\n",
      " - 2s - loss: 426.2239 - val_loss: 566.0341\n",
      "Epoch 33/100\n",
      " - 2s - loss: 416.6512 - val_loss: 566.2591\n",
      "Epoch 34/100\n",
      " - 2s - loss: 399.5123 - val_loss: 542.1250\n",
      "Epoch 35/100\n",
      " - 2s - loss: 360.3034 - val_loss: 499.7011\n",
      "Epoch 36/100\n",
      " - 2s - loss: 380.6214 - val_loss: 475.5037\n",
      "Epoch 37/100\n",
      " - 2s - loss: 364.2548 - val_loss: 464.8027\n",
      "Epoch 38/100\n",
      " - 2s - loss: 381.7968 - val_loss: 469.7156\n",
      "Epoch 39/100\n",
      " - 2s - loss: 298.8227 - val_loss: 429.3157\n",
      "Epoch 40/100\n",
      " - 2s - loss: 307.1415 - val_loss: 417.4911\n",
      "Epoch 41/100\n",
      " - 2s - loss: 345.7941 - val_loss: 437.3851\n",
      "Epoch 42/100\n",
      " - 2s - loss: 302.6029 - val_loss: 371.9529\n",
      "Epoch 43/100\n",
      " - 2s - loss: 305.1176 - val_loss: 358.5290\n",
      "Epoch 44/100\n",
      " - 2s - loss: 339.7579 - val_loss: 364.8230\n",
      "Epoch 45/100\n",
      " - 2s - loss: 301.9714 - val_loss: 409.5263\n",
      "Epoch 46/100\n",
      " - 2s - loss: 243.8180 - val_loss: 342.8039\n",
      "Epoch 47/100\n",
      " - 2s - loss: 268.8030 - val_loss: 305.7203\n",
      "Epoch 48/100\n",
      " - 2s - loss: 283.3473 - val_loss: 325.9345\n",
      "Epoch 49/100\n",
      " - 2s - loss: 276.5353 - val_loss: 321.6499\n",
      "Epoch 50/100\n",
      " - 2s - loss: 219.7607 - val_loss: 271.7592\n",
      "Epoch 51/100\n",
      " - 2s - loss: 270.6283 - val_loss: 294.0351\n",
      "Epoch 52/100\n",
      " - 2s - loss: 210.8678 - val_loss: 242.0718\n",
      "Epoch 53/100\n",
      " - 2s - loss: 246.0706 - val_loss: 236.4238\n",
      "Epoch 54/100\n",
      " - 2s - loss: 241.8310 - val_loss: 239.0518\n",
      "Epoch 55/100\n",
      " - 2s - loss: 235.0493 - val_loss: 241.4251\n",
      "Epoch 56/100\n",
      " - 2s - loss: 211.7781 - val_loss: 184.0519\n",
      "Epoch 57/100\n",
      " - 2s - loss: 290.5220 - val_loss: 225.9723\n",
      "Epoch 58/100\n",
      " - 2s - loss: 266.8552 - val_loss: 305.7343\n",
      "Epoch 59/100\n",
      " - 2s - loss: 211.5079 - val_loss: 228.9728\n",
      "Epoch 60/100\n",
      " - 2s - loss: 241.2122 - val_loss: 244.0983\n",
      "Epoch 61/100\n",
      " - 2s - loss: 242.5365 - val_loss: 267.1678\n",
      "Epoch 62/100\n",
      " - 2s - loss: 353.5338 - val_loss: 169.0845\n",
      "Epoch 63/100\n",
      " - 2s - loss: 213.8077 - val_loss: 89.2907\n",
      "Epoch 64/100\n",
      " - 2s - loss: 262.1071 - val_loss: 163.6339\n",
      "Epoch 65/100\n",
      " - 2s - loss: 290.7370 - val_loss: 212.4632\n",
      "Epoch 66/100\n",
      " - 2s - loss: 384.1934 - val_loss: 1047.8243\n",
      "Epoch 67/100\n",
      " - 2s - loss: 327.8237 - val_loss: 88.4374\n",
      "Epoch 68/100\n",
      " - 2s - loss: 388.6237 - val_loss: 914.9094\n",
      "Epoch 69/100\n",
      " - 2s - loss: 184.4879 - val_loss: 183.6654\n",
      "Epoch 70/100\n",
      " - 2s - loss: 216.8479 - val_loss: 136.3355\n",
      "Epoch 71/100\n",
      " - 2s - loss: 299.7293 - val_loss: 44.3767\n",
      "Epoch 72/100\n",
      " - 2s - loss: 351.9021 - val_loss: 945.2396\n",
      "Epoch 73/100\n",
      " - 2s - loss: 319.4173 - val_loss: 889.0509\n",
      "Epoch 74/100\n",
      " - 2s - loss: 315.5754 - val_loss: 823.4980\n",
      "Epoch 75/100\n",
      " - 2s - loss: 287.5511 - val_loss: 759.8675\n",
      "Epoch 76/100\n",
      " - 2s - loss: 248.0029 - val_loss: 700.3234\n",
      "Epoch 77/100\n",
      " - 2s - loss: 218.9275 - val_loss: 646.4968\n",
      "Epoch 78/100\n",
      " - 2s - loss: 195.9279 - val_loss: 598.7646\n",
      "Epoch 79/100\n",
      " - 2s - loss: 181.1171 - val_loss: 554.3054\n",
      "Epoch 80/100\n",
      " - 2s - loss: 158.2094 - val_loss: 513.2231\n",
      "Epoch 81/100\n",
      " - 2s - loss: 144.4287 - val_loss: 474.6348\n",
      "Epoch 82/100\n",
      " - 2s - loss: 131.1119 - val_loss: 440.1871\n",
      "Epoch 83/100\n",
      " - 2s - loss: 123.7121 - val_loss: 409.7583\n",
      "Epoch 84/100\n",
      " - 2s - loss: 114.3328 - val_loss: 381.6552\n",
      "Epoch 85/100\n",
      " - 2s - loss: 107.1172 - val_loss: 356.7112\n",
      "Epoch 86/100\n",
      " - 2s - loss: 100.5516 - val_loss: 334.2898\n",
      "Epoch 87/100\n",
      " - 2s - loss: 93.9138 - val_loss: 314.3898\n",
      "Epoch 88/100\n",
      " - 2s - loss: 91.3140 - val_loss: 296.0991\n",
      "Epoch 89/100\n",
      " - 2s - loss: 87.5532 - val_loss: 279.5735\n",
      "Epoch 90/100\n",
      " - 2s - loss: 84.3139 - val_loss: 264.5773\n",
      "Epoch 91/100\n",
      " - 2s - loss: 82.2197 - val_loss: 251.5448\n",
      "Epoch 92/100\n",
      " - 2s - loss: 79.5138 - val_loss: 240.3311\n",
      "Epoch 93/100\n",
      " - 2s - loss: 73.4056 - val_loss: 228.8600\n",
      "Epoch 94/100\n",
      " - 2s - loss: 77.0504 - val_loss: 220.2567\n",
      "Epoch 95/100\n",
      " - 2s - loss: 79.4001 - val_loss: 213.8335\n",
      "Epoch 96/100\n",
      " - 2s - loss: 77.4984 - val_loss: 208.3964\n",
      "Epoch 97/100\n",
      " - 2s - loss: 77.5110 - val_loss: 203.6153\n",
      "Epoch 98/100\n",
      " - 2s - loss: 79.6095 - val_loss: 200.3849\n",
      "Epoch 99/100\n",
      " - 2s - loss: 76.9453 - val_loss: 197.2351\n",
      "Epoch 100/100\n",
      " - 2s - loss: 78.4215 - val_loss: 195.0481\n"
     ]
    }
   ],
   "source": [
    "history_3 = LSTM_model_3.fit(train_X, train_y, \n",
    "                    epochs= 100, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(val_X, val_y),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model, train_x, train_y, test_x, test_y):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(test_x[i].reshape(1,1,23), (test_y[i].reshape(1 ,))),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        train_x = np.concatenate((train_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "        le = len(test_y) + 1\n",
    "        train_y = np.concatenate((train_y, (test_y[i].reshape(1 ,))))\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_x[-1,-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.6094 - val_loss: 81.6133\n",
      "Epoch 2/2\n",
      " - 2s - loss: 76.9663 - val_loss: 80.5709\n",
      "Train on 12001 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.3052 - val_loss: 79.6638\n",
      "Epoch 2/2\n",
      " - 1s - loss: 76.0717 - val_loss: 78.8749\n",
      "Train on 12002 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 75.9548 - val_loss: 78.1809\n",
      "Epoch 2/2\n",
      " - 2s - loss: 75.9781 - val_loss: 77.6605\n",
      "Train on 12003 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.2601 - val_loss: 77.1252\n",
      "Epoch 2/2\n",
      " - 2s - loss: 78.8641 - val_loss: 76.9604\n",
      "Train on 12004 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.8638 - val_loss: 76.4889\n",
      "Epoch 2/2\n",
      " - 2s - loss: 86.3460 - val_loss: 77.2550\n",
      "Train on 12005 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.8443 - val_loss: 76.9335\n",
      "Epoch 2/2\n",
      " - 2s - loss: 76.5043 - val_loss: 76.7730\n",
      "Train on 12006 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.5458 - val_loss: 76.6073\n",
      "Epoch 2/2\n",
      " - 2s - loss: 76.0995 - val_loss: 76.3937\n",
      "Train on 12007 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.5447 - val_loss: 76.2191\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.1518 - val_loss: 76.4439\n",
      "Train on 12008 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.1517 - val_loss: 76.2934\n",
      "Epoch 2/2\n",
      " - 2s - loss: 76.8140 - val_loss: 76.0887\n",
      "Train on 12009 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.1329 - val_loss: 74.8924\n",
      "Epoch 2/2\n",
      " - 1s - loss: 86.5192 - val_loss: 77.0930\n",
      "Train on 12010 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 76.9812 - val_loss: 76.6277\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.0625 - val_loss: 76.3222\n",
      "Train on 12011 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.1202 - val_loss: 76.1768\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.1367 - val_loss: 75.9382\n",
      "Train on 12012 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.2286 - val_loss: 75.7151\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.2386 - val_loss: 75.6601\n",
      "Train on 12013 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.3287 - val_loss: 75.6030\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.3512 - val_loss: 75.4071\n",
      "Train on 12014 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.5746 - val_loss: 95.8727\n",
      "Epoch 2/2\n",
      " - 1s - loss: 77.5165 - val_loss: 95.7923\n",
      "Train on 12015 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.6307 - val_loss: 92.0332\n",
      "Epoch 2/2\n",
      " - 1s - loss: 77.6443 - val_loss: 91.8548\n",
      "Train on 12016 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.7653 - val_loss: 87.8885\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.7366 - val_loss: 87.7020\n",
      "Train on 12017 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.7655 - val_loss: 124.0301\n",
      "Epoch 2/2\n",
      " - 2s - loss: 77.7021 - val_loss: 123.8508\n",
      "Train on 12018 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 77.9121 - val_loss: 118.9597\n",
      "Epoch 2/2\n",
      " - 2s - loss: 78.1485 - val_loss: 118.8396\n",
      "Train on 12019 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 78.0671 - val_loss: 137.7143\n",
      "Epoch 2/2\n",
      " - 1s - loss: 78.7356 - val_loss: 137.7559\n",
      "Train on 12020 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 78.4048 - val_loss: 116.4992\n",
      "Epoch 2/2\n",
      " - 2s - loss: 78.3716 - val_loss: 116.3101\n",
      "Train on 12021 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 78.4858 - val_loss: 115.4784\n",
      "Epoch 2/2\n",
      " - 2s - loss: 78.7081 - val_loss: 115.5573\n",
      "Train on 12022 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 78.7939 - val_loss: 115.2382\n",
      "Epoch 2/2\n",
      " - 1s - loss: 78.8142 - val_loss: 115.1188\n",
      "Train on 12023 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 78.9176 - val_loss: 114.9891\n",
      "Epoch 2/2\n",
      " - 1s - loss: 79.1197 - val_loss: 114.8915\n",
      "Train on 12024 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 79.1728 - val_loss: 114.6538\n",
      "Epoch 2/2\n",
      " - 2s - loss: 79.2591 - val_loss: 114.5304\n",
      "Train on 12025 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 79.2815 - val_loss: 114.4974\n",
      "Epoch 2/2\n",
      " - 2s - loss: 79.3554 - val_loss: 114.3632\n",
      "Train on 12026 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 79.4840 - val_loss: 114.1613\n",
      "Epoch 2/2\n",
      " - 2s - loss: 79.6791 - val_loss: 114.1179\n",
      "Train on 12027 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 79.7519 - val_loss: 114.0752\n",
      "Epoch 2/2\n",
      " - 2s - loss: 79.8029 - val_loss: 113.8527\n",
      "Train on 12028 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 79.9195 - val_loss: 113.8174\n",
      "Epoch 2/2\n",
      " - 2s - loss: 79.9992 - val_loss: 113.6912\n",
      "Train on 12029 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.3080 - val_loss: 113.4281\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.2533 - val_loss: 113.6147\n",
      "Train on 12030 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.4121 - val_loss: 113.7069\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.4015 - val_loss: 113.5238\n",
      "Train on 12031 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.4549 - val_loss: 113.3718\n",
      "Epoch 2/2\n",
      " - 1s - loss: 80.5346 - val_loss: 113.3393\n",
      "Train on 12032 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5964 - val_loss: 113.2869\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.7229 - val_loss: 113.2861\n",
      "Train on 12033 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 80.9059 - val_loss: 112.5044\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.9147 - val_loss: 111.4890\n",
      "Train on 12034 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.8262 - val_loss: 111.3498\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.8999 - val_loss: 111.3414\n",
      "Train on 12035 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.9561 - val_loss: 111.3588\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.9958 - val_loss: 111.2798\n",
      "Train on 12036 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.2257 - val_loss: 111.3636\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.1076 - val_loss: 111.2556\n",
      "Train on 12037 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.1605 - val_loss: 111.2478\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.1899 - val_loss: 111.2045\n",
      "Train on 12038 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.2868 - val_loss: 113.3130\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.3162 - val_loss: 113.3140\n",
      "Train on 12039 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.3162 - val_loss: 116.4913\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.3870 - val_loss: 116.5070\n",
      "Train on 12040 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.3968 - val_loss: 119.7441\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.4327 - val_loss: 119.7416\n",
      "Train on 12041 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.4604 - val_loss: 123.2880\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.4943 - val_loss: 123.3067\n",
      "Train on 12042 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.5274 - val_loss: 149.8519\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6374 - val_loss: 149.9343\n",
      "Train on 12043 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.5544 - val_loss: 142.6004\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.5961 - val_loss: 142.5655\n",
      "Train on 12044 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.5922 - val_loss: 149.8458\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6220 - val_loss: 150.0073\n",
      "Train on 12045 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.6205 - val_loss: 167.7434\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7676 - val_loss: 167.8156\n",
      "Train on 12046 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.6525 - val_loss: 167.7338\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6614 - val_loss: 167.6995\n",
      "Train on 12047 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.6820 - val_loss: 167.6959\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6843 - val_loss: 167.7000\n",
      "Train on 12048 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.6982 - val_loss: 167.6775\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7027 - val_loss: 167.6689\n",
      "Train on 12049 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7283 - val_loss: 167.7241\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7156 - val_loss: 167.6612\n",
      "Train on 12050 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7290 - val_loss: 167.6527\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7221 - val_loss: 167.6470\n",
      "Train on 12051 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7470 - val_loss: 167.6804\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7412 - val_loss: 167.6406\n",
      "Train on 12052 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7511 - val_loss: 167.6303\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7515 - val_loss: 167.6250\n",
      "Train on 12053 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7599 - val_loss: 167.6215\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7596 - val_loss: 167.6136\n",
      "Train on 12054 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 6s - loss: 81.7647 - val_loss: 167.6141\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7662 - val_loss: 167.6100\n",
      "Train on 12055 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7734 - val_loss: 167.6071\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7720 - val_loss: 167.6034\n",
      "Train on 12056 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7894 - val_loss: 167.5806\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7821 - val_loss: 167.6070\n",
      "Train on 12057 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7830 - val_loss: 167.5961\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7829 - val_loss: 167.5912\n",
      "Train on 12058 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7876 - val_loss: 167.5899\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7854 - val_loss: 167.5871\n",
      "Train on 12059 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7912 - val_loss: 167.5852\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7889 - val_loss: 167.5825\n",
      "Train on 12060 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7945 - val_loss: 167.5807\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7921 - val_loss: 167.5782\n",
      "Train on 12061 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7975 - val_loss: 167.5766\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8704 - val_loss: 167.7113\n",
      "Train on 12062 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7948 - val_loss: 167.5888\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7965 - val_loss: 167.5691\n",
      "Train on 12063 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8018 - val_loss: 167.5652\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8003 - val_loss: 167.5670\n",
      "Train on 12064 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8183 - val_loss: 167.6394\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7993 - val_loss: 167.5715\n",
      "Train on 12065 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8093 - val_loss: 167.5645\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1724 - val_loss: 167.7045\n",
      "Train on 12066 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7984 - val_loss: 167.5717\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8026 - val_loss: 167.5595\n",
      "Train on 12067 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8085 - val_loss: 167.5576\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8061 - val_loss: 167.5561\n",
      "Train on 12068 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8114 - val_loss: 167.5551\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8089 - val_loss: 167.5538\n",
      "Train on 12069 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8141 - val_loss: 167.5530\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8133 - val_loss: 167.5526\n",
      "Train on 12070 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8166 - val_loss: 167.5509\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8141 - val_loss: 167.5496\n",
      "Train on 12071 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8191 - val_loss: 167.5489\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0260 - val_loss: 167.5580\n",
      "Train on 12072 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 6s - loss: 81.8203 - val_loss: 167.5478\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8183 - val_loss: 167.5461\n",
      "Train on 12073 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 6s - loss: 81.8233 - val_loss: 167.5454\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8208 - val_loss: 167.5444\n",
      "Train on 12074 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8543 - val_loss: 167.5523\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8226 - val_loss: 167.5435\n",
      "Train on 12075 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 6s - loss: 81.8279 - val_loss: 167.5424\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8253 - val_loss: 167.5415\n",
      "Train on 12076 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8303 - val_loss: 167.5410\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8277 - val_loss: 167.5402\n",
      "Train on 12077 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8325 - val_loss: 167.5397\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8299 - val_loss: 167.5390\n",
      "Train on 12078 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8348 - val_loss: 167.5387\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8322 - val_loss: 167.5380\n",
      "Train on 12079 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 6s - loss: 81.8370 - val_loss: 167.5377\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8344 - val_loss: 167.5369\n",
      "Train on 12080 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8393 - val_loss: 167.5365\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8366 - val_loss: 167.5360\n",
      "Train on 12081 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 6s - loss: 81.8415 - val_loss: 167.5358\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8388 - val_loss: 167.5352\n",
      "Train on 12082 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 6s - loss: 81.8437 - val_loss: 167.5349\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8411 - val_loss: 167.5344\n",
      "Train on 12083 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 88.1084 - val_loss: 171.4318\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.3710 - val_loss: 167.9370\n",
      "Train on 12084 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7377 - val_loss: 167.7350\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6075 - val_loss: 167.5708\n",
      "Train on 12085 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.6397 - val_loss: 167.5530\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6558 - val_loss: 167.5485\n",
      "Train on 12086 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.6773 - val_loss: 167.5461\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7293 - val_loss: 167.5770\n",
      "Train on 12087 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7072 - val_loss: 167.5449\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7192 - val_loss: 167.5405\n",
      "Train on 12088 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7362 - val_loss: 167.5388\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7447 - val_loss: 167.5375\n",
      "Train on 12089 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7596 - val_loss: 167.5365\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7663 - val_loss: 167.5356\n",
      "Train on 12090 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.7796 - val_loss: 167.5348\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.7848 - val_loss: 167.5340\n",
      "Train on 12091 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.7967 - val_loss: 167.5334\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8007 - val_loss: 167.5327\n",
      "Train on 12092 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8114 - val_loss: 167.5323\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8142 - val_loss: 167.5319\n",
      "Train on 12093 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8240 - val_loss: 167.5316\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8259 - val_loss: 167.5312\n",
      "Train on 12094 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8348 - val_loss: 167.5311\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8359 - val_loss: 167.5307\n",
      "Train on 12095 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8441 - val_loss: 167.5305\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8446 - val_loss: 167.5303\n",
      "Train on 12096 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8522 - val_loss: 167.5303\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8522 - val_loss: 167.5300\n",
      "Train on 12097 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8592 - val_loss: 167.5301\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8588 - val_loss: 167.5299\n",
      "Train on 12098 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8654 - val_loss: 167.5299\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8646 - val_loss: 167.5298\n",
      "Train on 12099 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8710 - val_loss: 167.5298\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8698 - val_loss: 167.5297\n",
      "Train on 12100 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8759 - val_loss: 167.5298\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8745 - val_loss: 167.5297\n",
      "Train on 12101 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8803 - val_loss: 167.5299\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8787 - val_loss: 167.5298\n",
      "Train on 12102 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8844 - val_loss: 167.5300\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8826 - val_loss: 167.5300\n",
      "Train on 12103 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8881 - val_loss: 167.5301\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8863 - val_loss: 167.5302\n",
      "Train on 12104 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.8545 - val_loss: 167.5138\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7541 - val_loss: 167.6833\n",
      "Train on 12105 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8845 - val_loss: 167.5432\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8898 - val_loss: 167.5315\n",
      "Train on 12106 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8958 - val_loss: 167.5307\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8940 - val_loss: 167.5309\n",
      "Train on 12107 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.8993 - val_loss: 167.5311\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8973 - val_loss: 167.5311\n",
      "Train on 12108 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 81.9025 - val_loss: 167.5313\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9011 - val_loss: 167.5566\n",
      "Train on 12109 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9043 - val_loss: 167.5345\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9031 - val_loss: 167.5322\n",
      "Train on 12110 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9083 - val_loss: 179.9200\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9061 - val_loss: 179.9202\n",
      "Train on 12111 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9122 - val_loss: 198.0333\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9098 - val_loss: 198.0330\n",
      "Train on 12112 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.9173 - val_loss: 205.1599\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9147 - val_loss: 205.1588\n",
      "Train on 12113 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9228 - val_loss: 222.9747\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9202 - val_loss: 222.9734\n",
      "Train on 12114 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.9297 - val_loss: 227.4745\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9269 - val_loss: 227.4724\n",
      "Train on 12115 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9369 - val_loss: 227.4710\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9340 - val_loss: 227.4689\n",
      "Train on 12116 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.9439 - val_loss: 227.4677\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9410 - val_loss: 227.4656\n",
      "Train on 12117 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 81.9509 - val_loss: 227.4643\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9480 - val_loss: 227.4623\n",
      "Train on 12118 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.9579 - val_loss: 227.4611\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9550 - val_loss: 227.4593\n",
      "Train on 12119 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9648 - val_loss: 227.4581\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9620 - val_loss: 227.4563\n",
      "Train on 12120 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9718 - val_loss: 227.4551\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9689 - val_loss: 227.4534\n",
      "Train on 12121 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9787 - val_loss: 227.4524\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9759 - val_loss: 227.4506\n",
      "Train on 12122 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9857 - val_loss: 227.4497\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9828 - val_loss: 227.4480\n",
      "Train on 12123 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9926 - val_loss: 227.4471\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9898 - val_loss: 227.4455\n",
      "Train on 12124 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 81.9995 - val_loss: 227.4445\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9967 - val_loss: 227.4429\n",
      "Train on 12125 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 82.0065 - val_loss: 227.4420\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0006 - val_loss: 227.4494\n",
      "Train on 12126 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 82.0138 - val_loss: 227.4437\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0105 - val_loss: 227.4387\n",
      "Train on 12127 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 5s - loss: 88.7225 - val_loss: 236.2928\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6635 - val_loss: 228.8206\n",
      "Train on 12128 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.8608 - val_loss: 227.5876\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9194 - val_loss: 227.4660\n",
      "Train on 12129 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.9432 - val_loss: 227.4523\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9492 - val_loss: 227.4480\n",
      "Train on 12130 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.9666 - val_loss: 227.4456\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9708 - val_loss: 227.4427\n",
      "Train on 12131 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.9869 - val_loss: 227.4407\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9900 - val_loss: 227.4383\n",
      "Train on 12132 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0051 - val_loss: 227.4367\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0072 - val_loss: 227.4344\n",
      "Train on 12133 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0214 - val_loss: 227.4330\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0227 - val_loss: 227.4311\n",
      "Train on 12134 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0362 - val_loss: 227.4299\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0369 - val_loss: 227.4282\n",
      "Train on 12135 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0497 - val_loss: 227.4272\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0498 - val_loss: 227.4256\n",
      "Train on 12136 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0621 - val_loss: 227.4245\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0617 - val_loss: 227.4231\n",
      "Train on 12137 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0736 - val_loss: 227.4223\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0728 - val_loss: 227.4211\n",
      "Train on 12138 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0843 - val_loss: 227.4203\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0832 - val_loss: 227.4191\n",
      "Train on 12139 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0944 - val_loss: 227.4184\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0930 - val_loss: 227.4173\n",
      "Train on 12140 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1040 - val_loss: 227.4167\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1024 - val_loss: 227.4157\n",
      "Train on 12141 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1131 - val_loss: 227.4152\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1113 - val_loss: 227.4140\n",
      "Train on 12142 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1219 - val_loss: 227.4138\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1200 - val_loss: 227.4129\n",
      "Train on 12143 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1374 - val_loss: 227.3983\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1290 - val_loss: 227.4102\n",
      "Train on 12144 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1388 - val_loss: 227.4111\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1366 - val_loss: 227.4104\n",
      "Train on 12145 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1468 - val_loss: 227.4101\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1446 - val_loss: 227.4093\n",
      "Train on 12146 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1547 - val_loss: 227.4091\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1525 - val_loss: 227.4083\n",
      "Train on 12147 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1625 - val_loss: 227.4081\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1602 - val_loss: 227.4075\n",
      "Train on 12148 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1703 - val_loss: 227.4073\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1679 - val_loss: 227.4066\n",
      "Train on 12149 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1779 - val_loss: 227.4064\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1755 - val_loss: 227.4058\n",
      "Train on 12150 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1854 - val_loss: 227.4058\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1830 - val_loss: 227.4051\n",
      "Train on 12151 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1929 - val_loss: 227.4051\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1905 - val_loss: 227.4045\n",
      "Train on 12152 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2004 - val_loss: 227.4044\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1979 - val_loss: 227.4039\n",
      "Train on 12153 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2078 - val_loss: 227.4038\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2053 - val_loss: 227.4034\n",
      "Train on 12154 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2152 - val_loss: 227.4034\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2128 - val_loss: 227.4029\n",
      "Train on 12155 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2226 - val_loss: 227.4029\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2201 - val_loss: 227.4024\n",
      "Train on 12156 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2300 - val_loss: 227.4024\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2275 - val_loss: 227.4021\n",
      "Train on 12157 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2374 - val_loss: 227.4023\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2349 - val_loss: 227.4017\n",
      "Train on 12158 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2447 - val_loss: 221.1422\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2423 - val_loss: 221.1418\n",
      "Train on 12159 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2516 - val_loss: 251.8837\n",
      "Epoch 2/2\n",
      " - 1s - loss: 82.2491 - val_loss: 251.8836\n",
      "Train on 12160 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2610 - val_loss: 257.6292\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2584 - val_loss: 257.6282\n",
      "Train on 12161 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2706 - val_loss: 212.7541\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1085 - val_loss: 211.2692\n",
      "Train on 12162 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 82.1840 - val_loss: 173.4890\n",
      "Epoch 2/2\n",
      " - 3s - loss: 82.1943 - val_loss: 173.5377\n",
      "Train on 12163 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1937 - val_loss: 175.1614\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1981 - val_loss: 175.2226\n",
      "Train on 12164 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1982 - val_loss: 165.3032\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1984 - val_loss: 165.3323\n",
      "Train on 12165 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2001 - val_loss: 156.9709\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2004 - val_loss: 156.9947\n",
      "Train on 12166 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2021 - val_loss: 157.0087\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2024 - val_loss: 157.0293\n",
      "Train on 12167 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2047 - val_loss: 157.0399\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2044 - val_loss: 157.0548\n",
      "Train on 12168 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2073 - val_loss: 157.0628\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2066 - val_loss: 157.0742\n",
      "Train on 12169 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2562 - val_loss: 157.1268\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2064 - val_loss: 157.0944\n",
      "Train on 12170 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2123 - val_loss: 157.0952\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2115 - val_loss: 157.1023\n",
      "Train on 12171 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2153 - val_loss: 157.1065\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2141 - val_loss: 157.1126\n",
      "Train on 12172 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2181 - val_loss: 157.1164\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2168 - val_loss: 157.1217\n",
      "Train on 12173 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2208 - val_loss: 157.1249\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2194 - val_loss: 157.1295\n",
      "Train on 12174 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2235 - val_loss: 157.1323\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2221 - val_loss: 157.1364\n",
      "Train on 12175 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2263 - val_loss: 157.1389\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2248 - val_loss: 157.1425\n",
      "Train on 12176 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2290 - val_loss: 157.1450\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2275 - val_loss: 157.1481\n",
      "Train on 12177 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.2317 - val_loss: 157.1503\n",
      "Epoch 2/2\n",
      " - 3s - loss: 82.2301 - val_loss: 157.1533\n",
      "Train on 12178 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2343 - val_loss: 157.1553\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2328 - val_loss: 157.1579\n",
      "Train on 12179 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2370 - val_loss: 157.1599\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2354 - val_loss: 157.1623\n",
      "Train on 12180 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2396 - val_loss: 157.1641\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2380 - val_loss: 157.1664\n",
      "Train on 12181 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2423 - val_loss: 157.1681\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2406 - val_loss: 157.1702\n",
      "Train on 12182 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2449 - val_loss: 234.2987\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2431 - val_loss: 234.3013\n",
      "Train on 12183 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2538 - val_loss: 232.1582\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2503 - val_loss: 232.1481\n",
      "Train on 12184 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2612 - val_loss: 270.5114\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2579 - val_loss: 270.5016\n",
      "Train on 12185 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2719 - val_loss: 254.2967\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2678 - val_loss: 254.2830\n",
      "Train on 12186 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2806 - val_loss: 266.1805\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2770 - val_loss: 266.1696\n",
      "Train on 12187 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2906 - val_loss: 282.0886\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2868 - val_loss: 282.0768\n",
      "Train on 12188 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3016 - val_loss: 270.0735\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2977 - val_loss: 270.0610\n",
      "Train on 12189 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3116 - val_loss: 208.0614\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3079 - val_loss: 208.0524\n",
      "Train on 12190 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3166 - val_loss: 208.0509\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3140 - val_loss: 208.0492\n",
      "Train on 12191 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3223 - val_loss: 208.0486\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3198 - val_loss: 208.0469\n",
      "Train on 12192 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3281 - val_loss: 208.0464\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3256 - val_loss: 208.0451\n",
      "Train on 12193 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3340 - val_loss: 208.0445\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3315 - val_loss: 208.0432\n",
      "Train on 12194 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3400 - val_loss: 208.0429\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3375 - val_loss: 208.0416\n",
      "Train on 12195 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3460 - val_loss: 208.0413\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3436 - val_loss: 208.0403\n",
      "Train on 12196 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3520 - val_loss: 208.0401\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3497 - val_loss: 208.0391\n",
      "Train on 12197 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3581 - val_loss: 208.0391\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3558 - val_loss: 208.0381\n",
      "Train on 12198 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3643 - val_loss: 208.0380\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4416 - val_loss: 208.0186\n",
      "Train on 12199 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3712 - val_loss: 208.0350\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3682 - val_loss: 208.0364\n",
      "Train on 12200 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3766 - val_loss: 208.0365\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3743 - val_loss: 208.0360\n",
      "Train on 12201 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3828 - val_loss: 208.0359\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3806 - val_loss: 208.0356\n",
      "Train on 12202 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3891 - val_loss: 208.0356\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3869 - val_loss: 208.0352\n",
      "Train on 12203 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3954 - val_loss: 208.0353\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3932 - val_loss: 208.0350\n",
      "Train on 12204 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4017 - val_loss: 208.0350\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3995 - val_loss: 208.0347\n",
      "Train on 12205 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4081 - val_loss: 208.0349\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4059 - val_loss: 208.0348\n",
      "Train on 12206 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4144 - val_loss: 210.3201\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4123 - val_loss: 210.3199\n",
      "Train on 12207 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4210 - val_loss: 206.0206\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4188 - val_loss: 206.0203\n",
      "Train on 12208 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4272 - val_loss: 247.5404\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4251 - val_loss: 247.5404\n",
      "Train on 12209 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4368 - val_loss: 233.5818\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4305 - val_loss: 233.5727\n",
      "Train on 12210 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 84.2631 - val_loss: 219.2355\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4127 - val_loss: 218.8605\n",
      "Train on 12211 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4402 - val_loss: 213.2620\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4410 - val_loss: 213.2564\n",
      "Train on 12212 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4510 - val_loss: 180.9907\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4498 - val_loss: 180.9902\n",
      "Train on 12213 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4568 - val_loss: 208.5799\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4557 - val_loss: 208.5816\n",
      "Train on 12214 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4648 - val_loss: 208.5821\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4632 - val_loss: 208.5822\n",
      "Train on 12215 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4723 - val_loss: 208.5824\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4707 - val_loss: 208.5824\n",
      "Train on 12216 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 82.4797 - val_loss: 208.5829\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4780 - val_loss: 208.5829\n",
      "Train on 12217 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4870 - val_loss: 208.5834\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4852 - val_loss: 208.5835\n",
      "Train on 12218 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4941 - val_loss: 208.5838\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4923 - val_loss: 208.5841\n",
      "Train on 12219 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4917 - val_loss: 208.5932\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.8657 - val_loss: 209.1091\n",
      "Train on 12220 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4772 - val_loss: 208.6635\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4945 - val_loss: 208.5976\n",
      "Train on 12221 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.5071 - val_loss: 208.5903\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5064 - val_loss: 208.5895\n",
      "Train on 12222 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5158 - val_loss: 208.5896\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5146 - val_loss: 208.5895\n",
      "Train on 12223 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5238 - val_loss: 208.5899\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5224 - val_loss: 208.5900\n",
      "Train on 12224 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5316 - val_loss: 208.5903\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5301 - val_loss: 208.5907\n",
      "Train on 12225 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5391 - val_loss: 208.5911\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5376 - val_loss: 208.5913\n",
      "Train on 12226 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5466 - val_loss: 208.5919\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5449 - val_loss: 208.5921\n",
      "Train on 12227 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.5538 - val_loss: 208.5927\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5521 - val_loss: 208.5930\n",
      "Train on 12228 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.5610 - val_loss: 208.5937\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5593 - val_loss: 208.5941\n",
      "Train on 12229 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5681 - val_loss: 208.5947\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5663 - val_loss: 208.5951\n",
      "Train on 12230 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.5751 - val_loss: 208.5956\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5733 - val_loss: 208.5961\n",
      "Train on 12231 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5820 - val_loss: 208.5969\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5802 - val_loss: 208.5973\n",
      "Train on 12232 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5889 - val_loss: 208.5981\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5871 - val_loss: 208.5986\n",
      "Train on 12233 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5958 - val_loss: 208.5992\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5939 - val_loss: 208.5998\n",
      "Train on 12234 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.6026 - val_loss: 208.6005\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6007 - val_loss: 208.6010\n",
      "Train on 12235 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6094 - val_loss: 208.6018\n",
      "Epoch 2/2\n",
      " - 3s - loss: 82.6075 - val_loss: 208.6022\n",
      "Train on 12236 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6162 - val_loss: 208.6031\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6143 - val_loss: 208.6037\n",
      "Train on 12237 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6229 - val_loss: 208.6045\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6211 - val_loss: 208.6050\n",
      "Train on 12238 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6297 - val_loss: 208.6059\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6278 - val_loss: 208.6064\n",
      "Train on 12239 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6364 - val_loss: 208.6072\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6346 - val_loss: 208.6079\n",
      "Train on 12240 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6432 - val_loss: 208.6087\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6413 - val_loss: 208.6094\n",
      "Train on 12241 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7453 - val_loss: 208.7829\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6406 - val_loss: 208.6326\n",
      "Train on 12242 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6560 - val_loss: 208.6142\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6549 - val_loss: 208.6126\n",
      "Train on 12243 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6636 - val_loss: 208.6133\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6618 - val_loss: 208.6139\n",
      "Train on 12244 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6704 - val_loss: 208.6147\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6685 - val_loss: 208.6155\n",
      "Train on 12245 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6771 - val_loss: 208.6163\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6752 - val_loss: 208.6169\n",
      "Train on 12246 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6838 - val_loss: 208.6179\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6819 - val_loss: 208.6186\n",
      "Train on 12247 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6905 - val_loss: 208.6194\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6886 - val_loss: 208.6202\n",
      "Train on 12248 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6972 - val_loss: 208.6210\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6953 - val_loss: 208.6218\n",
      "Train on 12249 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7039 - val_loss: 208.6226\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7021 - val_loss: 208.6234\n",
      "Train on 12250 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7107 - val_loss: 208.6244\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7088 - val_loss: 208.6251\n",
      "Train on 12251 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7174 - val_loss: 208.6261\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7155 - val_loss: 208.6268\n",
      "Train on 12252 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7241 - val_loss: 208.6277\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7223 - val_loss: 208.6285\n",
      "Train on 12253 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7309 - val_loss: 208.6294\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7313 - val_loss: 208.6311\n",
      "Train on 12254 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7376 - val_loss: 208.6312\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7357 - val_loss: 208.6320\n",
      "Train on 12255 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7444 - val_loss: 208.6328\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7425 - val_loss: 208.6337\n",
      "Train on 12256 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7511 - val_loss: 208.6345\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7492 - val_loss: 208.6354\n",
      "Train on 12257 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7579 - val_loss: 208.6364\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7560 - val_loss: 208.6372\n",
      "Train on 12258 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7646 - val_loss: 208.6381\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7628 - val_loss: 208.6388\n",
      "Train on 12259 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7714 - val_loss: 208.6399\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7695 - val_loss: 208.6407\n",
      "Train on 12260 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7781 - val_loss: 208.6417\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7763 - val_loss: 208.6426\n",
      "Train on 12261 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7849 - val_loss: 208.6435\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8282 - val_loss: 208.6560\n",
      "Train on 12262 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7910 - val_loss: 208.6468\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7896 - val_loss: 208.6464\n",
      "Train on 12263 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7983 - val_loss: 208.6473\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7965 - val_loss: 208.6481\n",
      "Train on 12264 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.8051 - val_loss: 208.6492\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8033 - val_loss: 208.6501\n",
      "Train on 12265 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.8119 - val_loss: 208.6510\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8101 - val_loss: 208.6520\n",
      "Train on 12266 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.8187 - val_loss: 208.6530\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8168 - val_loss: 208.6538\n",
      "Train on 12267 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 124.6247 - val_loss: 207.8005\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.2072 - val_loss: 209.7374\n",
      "Train on 12268 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.1794 - val_loss: 210.0798\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.2191 - val_loss: 210.1273\n",
      "Train on 12269 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.2803 - val_loss: 210.0871\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.3341 - val_loss: 210.0217\n",
      "Train on 12270 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 81.3995 - val_loss: 209.9507\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.4545 - val_loss: 209.8796\n",
      "Train on 12271 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.5200 - val_loss: 209.8099\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.5745 - val_loss: 209.7422\n",
      "Train on 12272 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.6391 - val_loss: 209.6770\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.6926 - val_loss: 209.6139\n",
      "Train on 12273 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.7559 - val_loss: 209.5534\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8079 - val_loss: 209.4954\n",
      "Train on 12274 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.8695 - val_loss: 209.4400\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9195 - val_loss: 209.3871\n",
      "Train on 12275 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.9789 - val_loss: 209.3369\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0266 - val_loss: 209.2892\n",
      "Train on 12276 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0835 - val_loss: 209.2444\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1284 - val_loss: 209.2017\n",
      "Train on 12277 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1824 - val_loss: 209.1620\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2244 - val_loss: 209.1245\n",
      "Train on 12278 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2752 - val_loss: 172.4213\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3140 - val_loss: 172.3915\n",
      "Train on 12279 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3585 - val_loss: 173.1531\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3940 - val_loss: 173.1286\n",
      "Train on 12280 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 96.0540 - val_loss: 190.9003\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.3765 - val_loss: 183.2975\n",
      "Train on 12281 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.6987 - val_loss: 175.2698\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.8302 - val_loss: 174.7831\n",
      "Train on 12282 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 81.9125 - val_loss: 165.2440\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.9719 - val_loss: 165.1719\n",
      "Train on 12283 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.0331 - val_loss: 174.7651\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.0850 - val_loss: 174.7183\n",
      "Train on 12284 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.1423 - val_loss: 166.5508\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.1902 - val_loss: 166.5117\n",
      "Train on 12285 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.2430 - val_loss: 159.8588\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.2869 - val_loss: 159.8261\n",
      "Train on 12286 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.3350 - val_loss: 159.7955\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.3749 - val_loss: 159.7679\n",
      "Train on 12287 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4190 - val_loss: 159.7419\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.4549 - val_loss: 159.7188\n",
      "Train on 12288 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.4950 - val_loss: 159.6969\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5269 - val_loss: 159.6776\n",
      "Train on 12289 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.5630 - val_loss: 157.8230\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.5445 - val_loss: 156.6546\n",
      "Train on 12290 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6373 - val_loss: 156.4583\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.6699 - val_loss: 156.4160\n",
      "Train on 12291 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.6988 - val_loss: 156.3988\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7186 - val_loss: 156.3864\n",
      "Train on 12292 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7429 - val_loss: 156.3756\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.7597 - val_loss: 156.3660\n",
      "Train on 12293 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.7812 - val_loss: 156.3571\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8135 - val_loss: 156.3444\n",
      "Train on 12294 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.8150 - val_loss: 156.3417\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8267 - val_loss: 156.3358\n",
      "Train on 12295 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.8437 - val_loss: 156.3299\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8536 - val_loss: 156.3247\n",
      "Train on 12296 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.8688 - val_loss: 156.3199\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8769 - val_loss: 156.3155\n",
      "Train on 12297 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 3s - loss: 82.8905 - val_loss: 156.3117\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.8972 - val_loss: 156.3083\n",
      "Train on 12298 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9094 - val_loss: 156.3051\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9148 - val_loss: 156.3026\n",
      "Train on 12299 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9257 - val_loss: 156.3001\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9539 - val_loss: 156.3046\n",
      "Train on 12300 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9395 - val_loss: 156.2967\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9432 - val_loss: 156.2945\n",
      "Train on 12301 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9524 - val_loss: 156.2931\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9549 - val_loss: 156.2921\n",
      "Train on 12302 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9633 - val_loss: 156.2911\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9651 - val_loss: 156.2904\n",
      "Train on 12303 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9728 - val_loss: 156.2898\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9741 - val_loss: 156.2894\n",
      "Train on 12304 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9812 - val_loss: 156.2891\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9820 - val_loss: 156.2890\n",
      "Train on 12305 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9887 - val_loss: 156.2889\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9891 - val_loss: 156.2890\n",
      "Train on 12306 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 82.9954 - val_loss: 156.2891\n",
      "Epoch 2/2\n",
      " - 2s - loss: 82.9955 - val_loss: 156.2893\n",
      "Train on 12307 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0015 - val_loss: 156.2896\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0012 - val_loss: 156.2900\n",
      "Train on 12308 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0070 - val_loss: 156.2903\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0065 - val_loss: 156.2908\n",
      "Train on 12309 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0120 - val_loss: 156.2913\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0114 - val_loss: 156.2920\n",
      "Train on 12310 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0167 - val_loss: 156.2926\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0159 - val_loss: 156.2933\n",
      "Train on 12311 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0211 - val_loss: 156.2940\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0201 - val_loss: 156.2947\n",
      "Train on 12312 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0252 - val_loss: 156.2955\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0242 - val_loss: 156.2962\n",
      "Train on 12313 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0292 - val_loss: 156.2970\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0280 - val_loss: 156.2978\n",
      "Train on 12314 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0329 - val_loss: 156.2986\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0317 - val_loss: 156.2996\n",
      "Train on 12315 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0365 - val_loss: 156.3005\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0353 - val_loss: 156.3013\n",
      "Train on 12316 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0401 - val_loss: 156.3023\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0387 - val_loss: 156.3033\n",
      "Train on 12317 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0435 - val_loss: 156.3042\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0421 - val_loss: 156.3051\n",
      "Train on 12318 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0468 - val_loss: 156.3061\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0454 - val_loss: 156.3071\n",
      "Train on 12319 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0501 - val_loss: 156.3081\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0487 - val_loss: 156.3090\n",
      "Train on 12320 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0533 - val_loss: 156.3101\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0519 - val_loss: 156.3111\n",
      "Train on 12321 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0565 - val_loss: 156.3121\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0550 - val_loss: 156.3131\n",
      "Train on 12322 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0597 - val_loss: 156.3141\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0582 - val_loss: 156.3151\n",
      "Train on 12323 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0628 - val_loss: 156.3162\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0613 - val_loss: 156.3172\n",
      "Train on 12324 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 83.0659 - val_loss: 156.3181\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0644 - val_loss: 156.3192\n",
      "Train on 12325 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0690 - val_loss: 156.3203\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0675 - val_loss: 156.3214\n",
      "Train on 12326 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0721 - val_loss: 198.3271\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0710 - val_loss: 198.3278\n",
      "Train on 12327 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0802 - val_loss: 185.3206\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0764 - val_loss: 185.3123\n",
      "Train on 12328 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0842 - val_loss: 23.2505\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0827 - val_loss: 23.2480\n",
      "Train on 12329 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0762 - val_loss: 23.2514\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0766 - val_loss: 23.2591\n",
      "Train on 12330 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0693 - val_loss: 23.2635\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0698 - val_loss: 23.2704\n",
      "Train on 12331 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0627 - val_loss: 23.2744\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0633 - val_loss: 23.2810\n",
      "Train on 12332 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0564 - val_loss: 23.2849\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0570 - val_loss: 23.2912\n",
      "Train on 12333 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0502 - val_loss: 23.2950\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0508 - val_loss: 23.3012\n",
      "Train on 12334 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0442 - val_loss: 23.3048\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0449 - val_loss: 23.3100\n",
      "Train on 12335 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0382 - val_loss: 23.3130\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0384 - val_loss: 23.3187\n",
      "Train on 12336 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0316 - val_loss: 23.3222\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0319 - val_loss: 23.3277\n",
      "Train on 12337 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0252 - val_loss: 23.3309\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0255 - val_loss: 23.3361\n",
      "Train on 12338 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0189 - val_loss: 23.3391\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0192 - val_loss: 23.3441\n",
      "Train on 12339 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 83.0127 - val_loss: 23.3471\n",
      "Epoch 2/2\n",
      " - 2s - loss: 83.0130 - val_loss: 23.3519\n"
     ]
    }
   ],
   "source": [
    "res = make_refitting_window(LSTM_model_3, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x576 with 0 Axes>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "# sns.lineplot(data=res[1])\n",
    "# sns.lineplot(data=res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33.601868,\n",
       " 33.696842,\n",
       " 33.76548,\n",
       " 33.80529,\n",
       " 33.788517,\n",
       " 33.81598,\n",
       " 33.83765,\n",
       " 33.834778,\n",
       " 33.855114,\n",
       " 33.797737,\n",
       " 33.84174,\n",
       " 33.863747,\n",
       " 33.87972,\n",
       " 33.89427,\n",
       " 33.899647,\n",
       " 33.91391,\n",
       " 33.933064,\n",
       " 33.949173,\n",
       " 33.95564,\n",
       " 33.971054,\n",
       " 33.983288,\n",
       " 33.98824,\n",
       " 34.008656,\n",
       " 34.019253,\n",
       " 34.03611,\n",
       " 34.043926,\n",
       " 34.0554,\n",
       " 34.06782,\n",
       " 34.075394,\n",
       " 34.07898,\n",
       " 34.083244,\n",
       " 34.091904,\n",
       " 34.094402,\n",
       " 34.179165,\n",
       " 34.186157,\n",
       " 34.189075,\n",
       " 34.190224,\n",
       " 34.192646,\n",
       " 34.192097,\n",
       " 34.193157,\n",
       " 34.19435,\n",
       " 34.193645,\n",
       " 34.192234,\n",
       " 34.196922,\n",
       " 34.190254,\n",
       " 34.193634,\n",
       " 34.198116,\n",
       " 34.198093,\n",
       " 34.199295,\n",
       " 34.199593,\n",
       " 34.20014,\n",
       " 34.20039,\n",
       " 34.200993,\n",
       " 34.20143,\n",
       " 34.201572,\n",
       " 34.201824,\n",
       " 34.201687,\n",
       " 34.202297,\n",
       " 34.202454,\n",
       " 34.202633,\n",
       " 34.202797,\n",
       " 34.19766,\n",
       " 34.20315,\n",
       " 34.20323,\n",
       " 34.203056,\n",
       " 34.19792,\n",
       " 34.20352,\n",
       " 34.20365,\n",
       " 34.203743,\n",
       " 34.20379,\n",
       " 34.203903,\n",
       " 34.20358,\n",
       " 34.20404,\n",
       " 34.204105,\n",
       " 34.20414,\n",
       " 34.204216,\n",
       " 34.204266,\n",
       " 34.20431,\n",
       " 34.204353,\n",
       " 34.204395,\n",
       " 34.20443,\n",
       " 34.20446,\n",
       " 34.20449,\n",
       " 34.18895,\n",
       " 34.203083,\n",
       " 34.203945,\n",
       " 34.202847,\n",
       " 34.204254,\n",
       " 34.204372,\n",
       " 34.204445,\n",
       " 34.204506,\n",
       " 34.204556,\n",
       " 34.204586,\n",
       " 34.204613,\n",
       " 34.20463,\n",
       " 34.204647,\n",
       " 34.204662,\n",
       " 34.204666,\n",
       " 34.20467,\n",
       " 34.204674,\n",
       " 34.204674,\n",
       " 34.20467,\n",
       " 34.204662,\n",
       " 34.204655,\n",
       " 34.19874,\n",
       " 34.2046,\n",
       " 34.204628,\n",
       " 34.204617,\n",
       " 34.203632,\n",
       " 34.204575,\n",
       " 34.204567,\n",
       " 34.204582,\n",
       " 34.204636,\n",
       " 34.20471,\n",
       " 34.20481,\n",
       " 34.204926,\n",
       " 34.205036,\n",
       " 34.205147,\n",
       " 34.205246,\n",
       " 34.205345,\n",
       " 34.20544,\n",
       " 34.205532,\n",
       " 34.20562,\n",
       " 34.205704,\n",
       " 34.205788,\n",
       " 34.205574,\n",
       " 34.20593,\n",
       " 34.160183,\n",
       " 34.20502,\n",
       " 34.20562,\n",
       " 34.205795,\n",
       " 34.20594,\n",
       " 34.20607,\n",
       " 34.20618,\n",
       " 34.206276,\n",
       " 34.206364,\n",
       " 34.206444,\n",
       " 34.206512,\n",
       " 34.206577,\n",
       " 34.20664,\n",
       " 34.20669,\n",
       " 34.206745,\n",
       " 34.206783,\n",
       " 34.20687,\n",
       " 34.206867,\n",
       " 34.2069,\n",
       " 34.206936,\n",
       " 34.206963,\n",
       " 34.206993,\n",
       " 34.20702,\n",
       " 34.207043,\n",
       " 34.20706,\n",
       " 34.20708,\n",
       " 34.2071,\n",
       " 34.207115,\n",
       " 34.20713,\n",
       " 34.20714,\n",
       " 34.207153,\n",
       " 34.20716,\n",
       " 34.207157,\n",
       " 34.2072,\n",
       " 34.332897,\n",
       " 34.33463,\n",
       " 34.330833,\n",
       " 34.32884,\n",
       " 34.327248,\n",
       " 34.325867,\n",
       " 34.32485,\n",
       " 34.324074,\n",
       " 34.32327,\n",
       " 34.322952,\n",
       " 34.32254,\n",
       " 34.322178,\n",
       " 34.32187,\n",
       " 34.321594,\n",
       " 34.32135,\n",
       " 34.321125,\n",
       " 34.32092,\n",
       " 34.320736,\n",
       " 34.32056,\n",
       " 34.320396,\n",
       " 34.320244,\n",
       " 34.320095,\n",
       " 34.32059,\n",
       " 34.321068,\n",
       " 34.321747,\n",
       " 34.322296,\n",
       " 34.322857,\n",
       " 34.323467,\n",
       " 34.32398,\n",
       " 34.32409,\n",
       " 34.32417,\n",
       " 34.324234,\n",
       " 34.3243,\n",
       " 34.324352,\n",
       " 34.324398,\n",
       " 34.32444,\n",
       " 34.324474,\n",
       " 34.32515,\n",
       " 34.324535,\n",
       " 34.324547,\n",
       " 34.324562,\n",
       " 34.324577,\n",
       " 34.32458,\n",
       " 34.324593,\n",
       " 34.32459,\n",
       " 34.32459,\n",
       " 34.324593,\n",
       " 34.32458,\n",
       " 34.324917,\n",
       " 34.323067,\n",
       " 34.3247,\n",
       " 34.32474,\n",
       " 34.324642,\n",
       " 34.324623,\n",
       " 34.324615,\n",
       " 34.3246,\n",
       " 34.324577,\n",
       " 34.32456,\n",
       " 34.306393,\n",
       " 34.32409,\n",
       " 34.32437,\n",
       " 34.32437,\n",
       " 34.324352,\n",
       " 34.32433,\n",
       " 34.324306,\n",
       " 34.32428,\n",
       " 34.32425,\n",
       " 34.32421,\n",
       " 34.324177,\n",
       " 34.324142,\n",
       " 34.3241,\n",
       " 34.324055,\n",
       " 34.324013,\n",
       " 34.32397,\n",
       " 34.32393,\n",
       " 34.32388,\n",
       " 34.323833,\n",
       " 34.323784,\n",
       " 34.323734,\n",
       " 34.32368,\n",
       " 34.32288,\n",
       " 34.32357,\n",
       " 34.323524,\n",
       " 34.32347,\n",
       " 34.32342,\n",
       " 34.323364,\n",
       " 34.323307,\n",
       " 34.323254,\n",
       " 34.323196,\n",
       " 34.32314,\n",
       " 34.32308,\n",
       " 34.32302,\n",
       " 34.32293,\n",
       " 34.3229,\n",
       " 34.32284,\n",
       " 34.32278,\n",
       " 34.32272,\n",
       " 34.322662,\n",
       " 34.322598,\n",
       " 34.322533,\n",
       " 34.322067,\n",
       " 34.3224,\n",
       " 34.322342,\n",
       " 34.322273,\n",
       " 34.32221,\n",
       " 34.322144,\n",
       " 34.284683,\n",
       " 34.27123,\n",
       " 34.27487,\n",
       " 34.279778,\n",
       " 34.28452,\n",
       " 34.288948,\n",
       " 34.29304,\n",
       " 34.296783,\n",
       " 34.300167,\n",
       " 34.303192,\n",
       " 34.305862,\n",
       " 34.308205,\n",
       " 34.310165,\n",
       " 34.198257,\n",
       " 34.287445,\n",
       " 34.296078,\n",
       " 34.299896,\n",
       " 34.30306,\n",
       " 34.305767,\n",
       " 34.30807,\n",
       " 34.310013,\n",
       " 34.31164,\n",
       " 34.431828,\n",
       " 34.441364,\n",
       " 34.442547,\n",
       " 34.443363,\n",
       " 34.444225,\n",
       " 34.444572,\n",
       " 34.445015,\n",
       " 34.44538,\n",
       " 34.44567,\n",
       " 34.4459,\n",
       " 34.44582,\n",
       " 34.44622,\n",
       " 34.44632,\n",
       " 34.446384,\n",
       " 34.446426,\n",
       " 34.44644,\n",
       " 34.44644,\n",
       " 34.44643,\n",
       " 34.446404,\n",
       " 34.44637,\n",
       " 34.446323,\n",
       " 34.44627,\n",
       " 34.446213,\n",
       " 34.44615,\n",
       " 34.44609,\n",
       " 34.44602,\n",
       " 34.44595,\n",
       " 34.44587,\n",
       " 34.445797,\n",
       " 34.445717,\n",
       " 34.44564,\n",
       " 34.445557,\n",
       " 34.445477,\n",
       " 34.445396,\n",
       " 34.445312,\n",
       " 34.445232,\n",
       " 34.44515,\n",
       " 34.44511,\n",
       " 34.445053,\n",
       " 34.44538,\n",
       " 34.444233,\n",
       " 34.443058,\n",
       " 34.44196,\n",
       " 34.4409,\n",
       " 34.43987,\n",
       " 34.438953,\n",
       " 34.43805,\n",
       " 34.437122,\n",
       " 34.436256,\n",
       " 34.43542,\n",
       " 34.434616]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3733.65 ,   42.728, 3806.16 , 3725.8  ,   42.578, 3766.11 ,\n",
       "        3734.95 ,   42.578, 3793.27 , 3699.15 ,   42.578, 3780.49 ,\n",
       "        3712.18 ,   42.578, 3761.76 , 3742.8  ,   42.578, 3774.97 ,\n",
       "        3758.81 ,   42.578, 3782.67 ,   42.578, 3824.77 ], dtype=float32),\n",
       " array([3725.8  ,   42.578, 3766.11 , 3734.95 ,   42.578, 3793.27 ,\n",
       "        3699.15 ,   42.578, 3780.49 , 3712.18 ,   42.578, 3761.76 ,\n",
       "        3742.8  ,   42.578, 3774.97 , 3758.81 ,   42.578, 3782.67 ,\n",
       "        3805.71 ,   42.578, 3824.77 ,   42.578, 3863.66 ], dtype=float32),\n",
       " array([3734.95 ,   42.578, 3793.27 , 3699.15 ,   42.578, 3780.49 ,\n",
       "        3712.18 ,   42.578, 3761.76 , 3742.8  ,   42.578, 3774.97 ,\n",
       "        3758.81 ,   42.578, 3782.67 , 3805.71 ,   42.578, 3824.77 ,\n",
       "        3813.19 ,   42.578, 3863.66 ,   42.578, 3880.15 ], dtype=float32),\n",
       " array([3699.15 ,   42.578, 3780.49 , 3712.18 ,   42.578, 3761.76 ,\n",
       "        3742.8  ,   42.578, 3774.97 , 3758.81 ,   42.578, 3782.67 ,\n",
       "        3805.71 ,   42.578, 3824.77 , 3813.19 ,   42.578, 3863.66 ,\n",
       "        3883.33 ,   42.578, 3880.15 ,   42.578, 3916.42 ], dtype=float32),\n",
       " array([3712.18 ,   42.578, 3761.76 , 3742.8  ,   42.578, 3774.97 ,\n",
       "        3758.81 ,   42.578, 3782.67 , 3805.71 ,   42.578, 3824.77 ,\n",
       "        3813.19 ,   42.578, 3863.66 , 3883.33 ,   42.578, 3880.15 ,\n",
       "        3818.   ,   42.578, 3916.42 ,   42.578, 3834.48 ], dtype=float32),\n",
       " array([3742.8  ,   42.578, 3774.97 , 3758.81 ,   42.578, 3782.67 ,\n",
       "        3805.71 ,   42.578, 3824.77 , 3813.19 ,   42.578, 3863.66 ,\n",
       "        3883.33 ,   42.578, 3880.15 , 3818.   ,   42.578, 3916.42 ,\n",
       "        3782.54 ,   42.578, 3834.48 ,   42.578, 3824.93 ], dtype=float32),\n",
       " array([3758.81 ,   42.578, 3782.67 , 3805.71 ,   42.578, 3824.77 ,\n",
       "        3813.19 ,   42.578, 3863.66 , 3883.33 ,   42.578, 3880.15 ,\n",
       "        3818.   ,   42.578, 3916.42 , 3782.54 ,   42.578, 3834.48 ,\n",
       "        3782.21 ,   42.578, 3824.93 ,   42.578, 3852.09 ], dtype=float32),\n",
       " array([3805.71 ,   42.578, 3824.77 , 3813.19 ,   42.578, 3863.66 ,\n",
       "        3883.33 ,   42.578, 3880.15 , 3818.   ,   42.578, 3916.42 ,\n",
       "        3782.54 ,   42.578, 3834.48 , 3782.21 ,   42.578, 3824.93 ,\n",
       "        3796.2  ,   42.578, 3852.09 ,   42.578, 3842.97 ], dtype=float32),\n",
       " array([3813.19 ,   42.578, 3863.66 , 3883.33 ,   42.578, 3880.15 ,\n",
       "        3818.   ,   42.578, 3916.42 , 3782.54 ,   42.578, 3834.48 ,\n",
       "        3782.21 ,   42.578, 3824.93 , 3796.2  ,   42.578, 3852.09 ,\n",
       "        3789.11 ,   42.578, 3842.97 ,   42.578, 3846.4  ], dtype=float32),\n",
       " array([3883.33 ,   42.578, 3880.15 , 3818.   ,   42.578, 3916.42 ,\n",
       "        3782.54 ,   42.578, 3834.48 , 3782.21 ,   42.578, 3824.93 ,\n",
       "        3796.2  ,   42.578, 3852.09 , 3789.11 ,   42.578, 3842.97 ,\n",
       "        3792.66 ,   42.578, 3846.4  ,   42.578, 3818.64 ], dtype=float32),\n",
       " array([3818.   ,   42.578, 3916.42 , 3782.54 ,   42.578, 3834.48 ,\n",
       "        3782.21 ,   42.578, 3824.93 , 3796.2  ,   42.578, 3852.09 ,\n",
       "        3789.11 ,   42.578, 3842.97 , 3792.66 ,   42.578, 3846.4  ,\n",
       "        3730.29 ,   42.578, 3818.64 ,   42.578, 3782.4  ], dtype=float32),\n",
       " array([3782.54 ,   42.578, 3834.48 , 3782.21 ,   42.578, 3824.93 ,\n",
       "        3796.2  ,   42.578, 3852.09 , 3789.11 ,   42.578, 3842.97 ,\n",
       "        3792.66 ,   42.578, 3846.4  , 3730.29 ,   42.578, 3818.64 ,\n",
       "        3683.78 ,   42.578, 3782.4  ,   43.687, 3719.94 ], dtype=float32),\n",
       " array([3782.21 ,   42.578, 3824.93 , 3796.2  ,   42.578, 3852.09 ,\n",
       "        3789.11 ,   42.578, 3842.97 , 3792.66 ,   42.578, 3846.4  ,\n",
       "        3730.29 ,   42.578, 3818.64 , 3683.78 ,   42.578, 3782.4  ,\n",
       "        3693.86 ,   43.687, 3719.94 ,   43.498, 3768.67 ], dtype=float32),\n",
       " array([3796.2  ,   42.578, 3852.09 , 3789.11 ,   42.578, 3842.97 ,\n",
       "        3792.66 ,   42.578, 3846.4  , 3730.29 ,   42.578, 3818.64 ,\n",
       "        3683.78 ,   42.578, 3782.4  , 3693.86 ,   43.687, 3719.94 ,\n",
       "        3702.3  ,   43.498, 3768.67 ,   43.298, 3736.54 ], dtype=float32),\n",
       " array([3789.11 ,   42.578, 3842.97 , 3792.66 ,   42.578, 3846.4  ,\n",
       "        3730.29 ,   42.578, 3818.64 , 3683.78 ,   42.578, 3782.4  ,\n",
       "        3693.86 ,   43.687, 3719.94 , 3702.3  ,   43.498, 3768.67 ,\n",
       "        3697.13 ,   43.298, 3736.54 ,   45.078, 3764.74 ], dtype=float32),\n",
       " array([3792.66 ,   42.578, 3846.4  , 3730.29 ,   42.578, 3818.64 ,\n",
       "        3683.78 ,   42.578, 3782.4  , 3693.86 ,   43.687, 3719.94 ,\n",
       "        3702.3  ,   43.498, 3768.67 , 3697.13 ,   43.298, 3736.54 ,\n",
       "        3727.7  ,   45.078, 3764.74 ,   44.857, 3789.99 ], dtype=float32),\n",
       " array([3730.29 ,   42.578, 3818.64 , 3683.78 ,   42.578, 3782.4  ,\n",
       "        3693.86 ,   43.687, 3719.94 , 3702.3  ,   43.498, 3768.67 ,\n",
       "        3697.13 ,   43.298, 3736.54 , 3727.7  ,   45.078, 3764.74 ,\n",
       "        3737.75 ,   44.857, 3789.99 ,   45.708, 3786.47 ], dtype=float32),\n",
       " array([3683.78 ,   42.578, 3782.4  , 3693.86 ,   43.687, 3719.94 ,\n",
       "        3702.3  ,   43.498, 3768.67 , 3697.13 ,   43.298, 3736.54 ,\n",
       "        3727.7  ,   45.078, 3764.74 , 3737.75 ,   44.857, 3789.99 ,\n",
       "        3719.7  ,   45.708, 3786.47 ,   44.768, 3774.69 ], dtype=float32),\n",
       " array([3693.86 ,   43.687, 3719.94 , 3702.3  ,   43.498, 3768.67 ,\n",
       "        3697.13 ,   43.298, 3736.54 , 3727.7  ,   45.078, 3764.74 ,\n",
       "        3737.75 ,   44.857, 3789.99 , 3719.7  ,   45.708, 3786.47 ,\n",
       "        3730.11 ,   44.768, 3774.69 ,   44.738, 3796.59 ], dtype=float32),\n",
       " array([3702.3  ,   43.498, 3768.67 , 3697.13 ,   43.298, 3736.54 ,\n",
       "        3727.7  ,   45.078, 3764.74 , 3737.75 ,   44.857, 3789.99 ,\n",
       "        3719.7  ,   45.708, 3786.47 , 3730.11 ,   44.768, 3774.69 ,\n",
       "        3710.16 ,   44.738, 3796.59 ,   44.738, 3749.34 ], dtype=float32),\n",
       " array([3697.13 ,   43.298, 3736.54 , 3727.7  ,   45.078, 3764.74 ,\n",
       "        3737.75 ,   44.857, 3789.99 , 3719.7  ,   45.708, 3786.47 ,\n",
       "        3730.11 ,   44.768, 3774.69 , 3710.16 ,   44.738, 3796.59 ,\n",
       "        3699.47 ,   44.738, 3749.34 ,   44.738, 3761.69 ], dtype=float32),\n",
       " array([3727.7  ,   45.078, 3764.74 , 3737.75 ,   44.857, 3789.99 ,\n",
       "        3719.7  ,   45.708, 3786.47 , 3730.11 ,   44.768, 3774.69 ,\n",
       "        3710.16 ,   44.738, 3796.59 , 3699.47 ,   44.738, 3749.34 ,\n",
       "        3706.75 ,   44.738, 3761.69 ,   44.738, 3894.03 ], dtype=float32),\n",
       " array([3737.75 ,   44.857, 3789.99 , 3719.7  ,   45.708, 3786.47 ,\n",
       "        3730.11 ,   44.768, 3774.69 , 3710.16 ,   44.738, 3796.59 ,\n",
       "        3699.47 ,   44.738, 3749.34 , 3706.75 ,   44.738, 3761.69 ,\n",
       "        3928.4  ,   44.738, 3894.03 ,   44.738, 4026.36 ], dtype=float32),\n",
       " array([3719.7  ,   45.708, 3786.47 , 3730.11 ,   44.768, 3774.69 ,\n",
       "        3710.16 ,   44.738, 3796.59 , 3699.47 ,   44.738, 3749.34 ,\n",
       "        3706.75 ,   44.738, 3761.69 , 3928.4  ,   44.738, 3894.03 ,\n",
       "        4031.7  ,   44.738, 4026.36 ,   44.738, 4124.56 ], dtype=float32),\n",
       " array([3730.11 ,   44.768, 3774.69 , 3710.16 ,   44.738, 3796.59 ,\n",
       "        3699.47 ,   44.738, 3749.34 , 3706.75 ,   44.738, 3761.69 ,\n",
       "        3928.4  ,   44.738, 3894.03 , 4031.7  ,   44.738, 4026.36 ,\n",
       "        4051.15 ,   44.738, 4124.56 ,   44.738, 4100.84 ], dtype=float32),\n",
       " array([3710.16 ,   44.738, 3796.59 , 3699.47 ,   44.738, 3749.34 ,\n",
       "        3706.75 ,   44.738, 3761.69 , 3928.4  ,   44.738, 3894.03 ,\n",
       "        4031.7  ,   44.738, 4026.36 , 4051.15 ,   44.738, 4124.56 ,\n",
       "        4039.42 ,   44.738, 4100.84 ,   44.738, 4076.27 ], dtype=float32),\n",
       " array([3699.47 ,   44.738, 3749.34 , 3706.75 ,   44.738, 3761.69 ,\n",
       "        3928.4  ,   44.738, 3894.03 , 4031.7  ,   44.738, 4026.36 ,\n",
       "        4051.15 ,   44.738, 4124.56 , 4039.42 ,   44.738, 4100.84 ,\n",
       "        4051.51 ,   44.738, 4076.27 ,   44.738, 4131.06 ], dtype=float32),\n",
       " array([3706.75 ,   44.738, 3761.69 , 3928.4  ,   44.738, 3894.03 ,\n",
       "        4031.7  ,   44.738, 4026.36 , 4051.15 ,   44.738, 4124.56 ,\n",
       "        4039.42 ,   44.738, 4100.84 , 4051.51 ,   44.738, 4076.27 ,\n",
       "        4009.81 ,   44.738, 4131.06 ,   44.738, 4076.56 ], dtype=float32),\n",
       " array([3928.4  ,   44.738, 3894.03 , 4031.7  ,   44.738, 4026.36 ,\n",
       "        4051.15 ,   44.738, 4124.56 , 4039.42 ,   44.738, 4100.84 ,\n",
       "        4051.51 ,   44.738, 4076.27 , 4009.81 ,   44.738, 4131.06 ,\n",
       "        4019.83 ,   44.738, 4076.56 ,   44.738, 4085.95 ], dtype=float32),\n",
       " array([4031.7  ,   44.738, 4026.36 , 4051.15 ,   44.738, 4124.56 ,\n",
       "        4039.42 ,   44.738, 4100.84 , 4051.51 ,   44.738, 4076.27 ,\n",
       "        4009.81 ,   44.738, 4131.06 , 4019.83 ,   44.738, 4076.56 ,\n",
       "        4032.44 ,   44.738, 4085.95 ,   44.738, 4085.85 ], dtype=float32),\n",
       " array([4051.15 ,   44.738, 4124.56 , 4039.42 ,   44.738, 4100.84 ,\n",
       "        4051.51 ,   44.738, 4076.27 , 4009.81 ,   44.738, 4131.06 ,\n",
       "        4019.83 ,   44.738, 4076.56 , 4032.44 ,   44.738, 4085.95 ,\n",
       "        4074.37 ,   44.738, 4085.85 ,   44.738, 4189.8  ], dtype=float32),\n",
       " array([4039.42 ,   44.738, 4100.84 , 4051.51 ,   44.738, 4076.27 ,\n",
       "        4009.81 ,   44.738, 4131.06 , 4019.83 ,   44.738, 4076.56 ,\n",
       "        4032.44 ,   44.738, 4085.95 , 4074.37 ,   44.738, 4085.85 ,\n",
       "        4090.16 ,   44.738, 4189.8  ,   44.738, 4118.44 ], dtype=float32),\n",
       " array([4051.51 ,   44.738, 4076.27 , 4009.81 ,   44.738, 4131.06 ,\n",
       "        4019.83 ,   44.738, 4076.56 , 4032.44 ,   44.738, 4085.95 ,\n",
       "        4074.37 ,   44.738, 4085.85 , 4090.16 ,   44.738, 4189.8  ,\n",
       "        4056.07 ,   44.738, 4118.44 ,   44.738, 4103.77 ], dtype=float32),\n",
       " array([4009.81 ,   44.738, 4131.06 , 4019.83 ,   44.738, 4076.56 ,\n",
       "        4032.44 ,   44.738, 4085.95 , 4074.37 ,   44.738, 4085.85 ,\n",
       "        4090.16 ,   44.738, 4189.8  , 4056.07 ,   44.738, 4118.44 ,\n",
       "        4048.05 ,   44.738, 4103.77 ,   44.738, 4103.83 ], dtype=float32),\n",
       " array([4019.83 ,   44.738, 4076.56 , 4032.44 ,   44.738, 4085.95 ,\n",
       "        4074.37 ,   44.738, 4085.85 , 4090.16 ,   44.738, 4189.8  ,\n",
       "        4056.07 ,   44.738, 4118.44 , 4048.05 ,   44.738, 4103.77 ,\n",
       "        3940.39 ,   44.738, 4103.83 ,   44.738, 3953.37 ], dtype=float32),\n",
       " array([4032.44 ,   44.738, 4085.95 , 4074.37 ,   44.738, 4085.85 ,\n",
       "        4090.16 ,   44.738, 4189.8  , 4056.07 ,   44.738, 4118.44 ,\n",
       "        4048.05 ,   44.738, 4103.77 , 3940.39 ,   44.738, 4103.83 ,\n",
       "        3943.7  ,   44.738, 3953.37 ,   44.837, 4052.89 ], dtype=float32),\n",
       " array([4074.37 ,   44.738, 4085.85 , 4090.16 ,   44.738, 4189.8  ,\n",
       "        4056.07 ,   44.738, 4118.44 , 4048.05 ,   44.738, 4103.77 ,\n",
       "        3940.39 ,   44.738, 4103.83 , 3943.7  ,   44.738, 3953.37 ,\n",
       "        4042.4  ,   44.837, 4052.89 ,   44.987, 4127.96 ], dtype=float32),\n",
       " array([4090.16 ,   44.738, 4189.8  , 4056.07 ,   44.738, 4118.44 ,\n",
       "        4048.05 ,   44.738, 4103.77 , 3940.39 ,   44.738, 4103.83 ,\n",
       "        3943.7  ,   44.738, 3953.37 , 4042.4  ,   44.837, 4052.89 ,\n",
       "        4070.96 ,   44.987, 4127.96 ,   45.137, 4137.66 ], dtype=float32),\n",
       " array([4056.07 ,   44.738, 4118.44 , 4048.05 ,   44.738, 4103.77 ,\n",
       "        3940.39 ,   44.738, 4103.83 , 3943.7  ,   44.738, 3953.37 ,\n",
       "        4042.4  ,   44.837, 4052.89 , 4070.96 ,   44.987, 4127.96 ,\n",
       "        4040.78 ,   45.137, 4137.66 ,   45.298, 4119.55 ], dtype=float32),\n",
       " array([4048.05 ,   44.738, 4103.77 , 3940.39 ,   44.738, 4103.83 ,\n",
       "        3943.7  ,   44.738, 3953.37 , 4042.4  ,   44.837, 4052.89 ,\n",
       "        4070.96 ,   44.987, 4127.96 , 4040.78 ,   45.137, 4137.66 ,\n",
       "        4097.71 ,   45.298, 4119.55 ,   46.437, 4191.13 ], dtype=float32),\n",
       " array([3940.39 ,   44.738, 4103.83 , 3943.7  ,   44.738, 3953.37 ,\n",
       "        4042.4  ,   44.837, 4052.89 , 4070.96 ,   44.987, 4127.96 ,\n",
       "        4040.78 ,   45.137, 4137.66 , 4097.71 ,   45.298, 4119.55 ,\n",
       "        4056.8  ,   46.437, 4191.13 ,   46.137, 4097.72 ], dtype=float32),\n",
       " array([3943.7  ,   44.738, 3953.37 , 4042.4  ,   44.837, 4052.89 ,\n",
       "        4070.96 ,   44.987, 4127.96 , 4040.78 ,   45.137, 4137.66 ,\n",
       "        4097.71 ,   45.298, 4119.55 , 4056.8  ,   46.437, 4191.13 ,\n",
       "        4018.45 ,   46.137, 4097.72 ,   46.438, 4102.35 ], dtype=float32),\n",
       " array([4042.4  ,   44.837, 4052.89 , 4070.96 ,   44.987, 4127.96 ,\n",
       "        4040.78 ,   45.137, 4137.66 , 4097.71 ,   45.298, 4119.55 ,\n",
       "        4056.8  ,   46.437, 4191.13 , 4018.45 ,   46.137, 4097.72 ,\n",
       "        4025.08 ,   46.438, 4102.35 ,   47.148, 4071.66 ], dtype=float32),\n",
       " array([4070.96 ,   44.987, 4127.96 , 4040.78 ,   45.137, 4137.66 ,\n",
       "        4097.71 ,   45.298, 4119.55 , 4056.8  ,   46.437, 4191.13 ,\n",
       "        4018.45 ,   46.137, 4097.72 , 4025.08 ,   46.438, 4102.35 ,\n",
       "        3930.05 ,   47.148, 4071.66 ,   47.148, 3997.59 ], dtype=float32),\n",
       " array([4040.78 ,   45.137, 4137.66 , 4097.71 ,   45.298, 4119.55 ,\n",
       "        4056.8  ,   46.437, 4191.13 , 4018.45 ,   46.137, 4097.72 ,\n",
       "        4025.08 ,   46.438, 4102.35 , 3930.05 ,   47.148, 4071.66 ,\n",
       "        3973.58 ,   47.148, 3997.59 ,   47.148, 4058.31 ], dtype=float32),\n",
       " array([4097.71 ,   45.298, 4119.55 , 4056.8  ,   46.437, 4191.13 ,\n",
       "        4018.45 ,   46.137, 4097.72 , 4025.08 ,   46.438, 4102.35 ,\n",
       "        3930.05 ,   47.148, 4071.66 , 3973.58 ,   47.148, 3997.59 ,\n",
       "        3990.4  ,   47.148, 4058.31 ,   47.148, 4070.18 ], dtype=float32),\n",
       " array([4056.8  ,   46.437, 4191.13 , 4018.45 ,   46.137, 4097.72 ,\n",
       "        4025.08 ,   46.438, 4102.35 , 3930.05 ,   47.148, 4071.66 ,\n",
       "        3973.58 ,   47.148, 3997.59 , 3990.4  ,   47.148, 4058.31 ,\n",
       "        4034.15 ,   47.148, 4070.18 ,   47.148, 4092.86 ], dtype=float32),\n",
       " array([4018.45 ,   46.137, 4097.72 , 4025.08 ,   46.438, 4102.35 ,\n",
       "        3930.05 ,   47.148, 4071.66 , 3973.58 ,   47.148, 3997.59 ,\n",
       "        3990.4  ,   47.148, 4058.31 , 4034.15 ,   47.148, 4070.18 ,\n",
       "        4000.04 ,   47.148, 4092.86 ,   47.148, 4035.67 ], dtype=float32),\n",
       " array([4025.08 ,   46.438, 4102.35 , 3930.05 ,   47.148, 4071.66 ,\n",
       "        3973.58 ,   47.148, 3997.59 , 3990.4  ,   47.148, 4058.31 ,\n",
       "        4034.15 ,   47.148, 4070.18 , 4000.04 ,   47.148, 4092.86 ,\n",
       "        3984.61 ,   47.148, 4035.67 ,   47.148, 4065.   ], dtype=float32),\n",
       " array([3930.05 ,   47.148, 4071.66 , 3973.58 ,   47.148, 3997.59 ,\n",
       "        3990.4  ,   47.148, 4058.31 , 4034.15 ,   47.148, 4070.18 ,\n",
       "        4000.04 ,   47.148, 4092.86 , 3984.61 ,   47.148, 4035.67 ,\n",
       "        4016.76 ,   47.148, 4065.   ,   47.148, 4072.28 ], dtype=float32),\n",
       " array([3973.58 ,   47.148, 3997.59 , 3990.4  ,   47.148, 4058.31 ,\n",
       "        4034.15 ,   47.148, 4070.18 , 4000.04 ,   47.148, 4092.86 ,\n",
       "        3984.61 ,   47.148, 4035.67 , 4016.76 ,   47.148, 4065.   ,\n",
       "        4038.09 ,   47.148, 4072.28 ,   47.148, 4089.41 ], dtype=float32),\n",
       " array([3990.4  ,   47.148, 4058.31 , 4034.15 ,   47.148, 4070.18 ,\n",
       "        4000.04 ,   47.148, 4092.86 , 3984.61 ,   47.148, 4035.67 ,\n",
       "        4016.76 ,   47.148, 4065.   , 4038.09 ,   47.148, 4072.28 ,\n",
       "        4043.79 ,   47.148, 4089.41 ,   47.148, 4087.69 ], dtype=float32),\n",
       " array([4034.15 ,   47.148, 4070.18 , 4000.04 ,   47.148, 4092.86 ,\n",
       "        3984.61 ,   47.148, 4035.67 , 4016.76 ,   47.148, 4065.   ,\n",
       "        4038.09 ,   47.148, 4072.28 , 4043.79 ,   47.148, 4089.41 ,\n",
       "        4035.2  ,   47.148, 4087.69 ,   47.148, 4076.83 ], dtype=float32),\n",
       " array([4000.04 ,   47.148, 4092.86 , 3984.61 ,   47.148, 4035.67 ,\n",
       "        4016.76 ,   47.148, 4065.   , 4038.09 ,   47.148, 4072.28 ,\n",
       "        4043.79 ,   47.148, 4089.41 , 4035.2  ,   47.148, 4087.69 ,\n",
       "        3993.59 ,   47.148, 4076.83 ,   47.148, 4020.17 ], dtype=float32),\n",
       " array([3984.61 ,   47.148, 4035.67 , 4016.76 ,   47.148, 4065.   ,\n",
       "        4038.09 ,   47.148, 4072.28 , 4043.79 ,   47.148, 4089.41 ,\n",
       "        4035.2  ,   47.148, 4087.69 , 3993.59 ,   47.148, 4076.83 ,\n",
       "        3971.87 ,   47.148, 4020.17 ,   47.148, 3995.3  ], dtype=float32),\n",
       " array([4016.76 ,   47.148, 4065.   , 4038.09 ,   47.148, 4072.28 ,\n",
       "        4043.79 ,   47.148, 4089.41 , 4035.2  ,   47.148, 4087.69 ,\n",
       "        3993.59 ,   47.148, 4076.83 , 3971.87 ,   47.148, 4020.17 ,\n",
       "        3956.28 ,   47.148, 3995.3  ,   47.148, 3995.3  ], dtype=float32),\n",
       " array([4038.09 ,   47.148, 4072.28 , 4043.79 ,   47.148, 4089.41 ,\n",
       "        4035.2  ,   47.148, 4087.69 , 3993.59 ,   47.148, 4076.83 ,\n",
       "        3971.87 ,   47.148, 4020.17 , 3956.28 ,   47.148, 3995.3  ,\n",
       "        3920.44 ,   47.148, 3995.3  ,   47.148, 3985.86 ], dtype=float32),\n",
       " array([4043.79 ,   47.148, 4089.41 , 4035.2  ,   47.148, 4087.69 ,\n",
       "        3993.59 ,   47.148, 4076.83 , 3971.87 ,   47.148, 4020.17 ,\n",
       "        3956.28 ,   47.148, 3995.3  , 3920.44 ,   47.148, 3995.3  ,\n",
       "        3862.93 ,   47.148, 3985.86 ,   47.148, 3870.43 ], dtype=float32),\n",
       " array([4035.2  ,   47.148, 4087.69 , 3993.59 ,   47.148, 4076.83 ,\n",
       "        3971.87 ,   47.148, 4020.17 , 3956.28 ,   47.148, 3995.3  ,\n",
       "        3920.44 ,   47.148, 3995.3  , 3862.93 ,   47.148, 3985.86 ,\n",
       "        3806.06 ,   47.148, 3870.43 ,   47.148, 3850.81 ], dtype=float32),\n",
       " array([3993.59 ,   47.148, 4076.83 , 3971.87 ,   47.148, 4020.17 ,\n",
       "        3956.28 ,   47.148, 3995.3  , 3920.44 ,   47.148, 3995.3  ,\n",
       "        3862.93 ,   47.148, 3985.86 , 3806.06 ,   47.148, 3870.43 ,\n",
       "        3819.92 ,   47.148, 3850.81 ,   47.148, 3905.43 ], dtype=float32),\n",
       " array([3971.87 ,   47.148, 4020.17 , 3956.28 ,   47.148, 3995.3  ,\n",
       "        3920.44 ,   47.148, 3995.3  , 3862.93 ,   47.148, 3985.86 ,\n",
       "        3806.06 ,   47.148, 3870.43 , 3819.92 ,   47.148, 3850.81 ,\n",
       "        3835.76 ,   47.148, 3905.43 ,   47.148, 3882.68 ], dtype=float32),\n",
       " array([3956.28 ,   47.148, 3995.3  , 3920.44 ,   47.148, 3995.3  ,\n",
       "        3862.93 ,   47.148, 3985.86 , 3806.06 ,   47.148, 3870.43 ,\n",
       "        3819.92 ,   47.148, 3850.81 , 3835.76 ,   47.148, 3905.43 ,\n",
       "        3832.9  ,   47.148, 3882.68 ,   47.148, 3898.81 ], dtype=float32),\n",
       " array([3920.44 ,   47.148, 3995.3  , 3862.93 ,   47.148, 3985.86 ,\n",
       "        3806.06 ,   47.148, 3870.43 , 3819.92 ,   47.148, 3850.81 ,\n",
       "        3835.76 ,   47.148, 3905.43 , 3832.9  ,   47.148, 3882.68 ,\n",
       "        3846.68 ,   47.148, 3898.81 ,   47.148, 3899.06 ], dtype=float32),\n",
       " array([3862.93 ,   47.148, 3985.86 , 3806.06 ,   47.148, 3870.43 ,\n",
       "        3819.92 ,   47.148, 3850.81 , 3835.76 ,   47.148, 3905.43 ,\n",
       "        3832.9  ,   47.148, 3882.68 , 3846.68 ,   47.148, 3898.81 ,\n",
       "        3820.43 ,   47.148, 3899.06 ,   47.148, 3886.47 ], dtype=float32),\n",
       " array([3806.06 ,   47.148, 3870.43 , 3819.92 ,   47.148, 3850.81 ,\n",
       "        3835.76 ,   47.148, 3905.43 , 3832.9  ,   47.148, 3882.68 ,\n",
       "        3846.68 ,   47.148, 3898.81 , 3820.43 ,   47.148, 3899.06 ,\n",
       "        3825.94 ,   47.148, 3886.47 ,   47.148, 3903.67 ], dtype=float32),\n",
       " array([3819.92 ,   47.148, 3850.81 , 3835.76 ,   47.148, 3905.43 ,\n",
       "        3832.9  ,   47.148, 3882.68 , 3846.68 ,   47.148, 3898.81 ,\n",
       "        3820.43 ,   47.148, 3899.06 , 3825.94 ,   47.148, 3886.47 ,\n",
       "        3830.71 ,   47.148, 3903.67 ,   47.148, 3881.43 ], dtype=float32),\n",
       " array([3835.76 ,   47.148, 3905.43 , 3832.9  ,   47.148, 3882.68 ,\n",
       "        3846.68 ,   47.148, 3898.81 , 3820.43 ,   47.148, 3899.06 ,\n",
       "        3825.94 ,   47.148, 3886.47 , 3830.71 ,   47.148, 3903.67 ,\n",
       "        3808.98 ,   47.148, 3881.43 ,   47.148, 3864.86 ], dtype=float32),\n",
       " array([3832.9  ,   47.148, 3882.68 , 3846.68 ,   47.148, 3898.81 ,\n",
       "        3820.43 ,   47.148, 3899.06 , 3825.94 ,   47.148, 3886.47 ,\n",
       "        3830.71 ,   47.148, 3903.67 , 3808.98 ,   47.148, 3881.43 ,\n",
       "        3799.09 ,   47.148, 3864.86 ,   47.148, 3877.85 ], dtype=float32),\n",
       " array([3846.68 ,   47.148, 3898.81 , 3820.43 ,   47.148, 3899.06 ,\n",
       "        3825.94 ,   47.148, 3886.47 , 3830.71 ,   47.148, 3903.67 ,\n",
       "        3808.98 ,   47.148, 3881.43 , 3799.09 ,   47.148, 3864.86 ,\n",
       "        3835.21 ,   47.148, 3877.85 ,   47.148, 3886.54 ], dtype=float32),\n",
       " array([3820.43 ,   47.148, 3899.06 , 3825.94 ,   47.148, 3886.47 ,\n",
       "        3830.71 ,   47.148, 3903.67 , 3808.98 ,   47.148, 3881.43 ,\n",
       "        3799.09 ,   47.148, 3864.86 , 3835.21 ,   47.148, 3877.85 ,\n",
       "        3842.58 ,   47.148, 3886.54 ,   47.148, 3898.19 ], dtype=float32),\n",
       " array([3825.94 ,   47.148, 3886.47 , 3830.71 ,   47.148, 3903.67 ,\n",
       "        3808.98 ,   47.148, 3881.43 , 3799.09 ,   47.148, 3864.86 ,\n",
       "        3835.21 ,   47.148, 3877.85 , 3842.58 ,   47.148, 3886.54 ,\n",
       "        3844.54 ,   47.148, 3898.19 ,   47.148, 3894.64 ], dtype=float32),\n",
       " array([3830.71 ,   47.148, 3903.67 , 3808.98 ,   47.148, 3881.43 ,\n",
       "        3799.09 ,   47.148, 3864.86 , 3835.21 ,   47.148, 3877.85 ,\n",
       "        3842.58 ,   47.148, 3886.54 , 3844.54 ,   47.148, 3898.19 ,\n",
       "        3890.3  ,   47.148, 3894.64 ,   47.148, 3993.69 ], dtype=float32),\n",
       " array([3808.98 ,   47.148, 3881.43 , 3799.09 ,   47.148, 3864.86 ,\n",
       "        3835.21 ,   47.148, 3877.85 , 3842.58 ,   47.148, 3886.54 ,\n",
       "        3844.54 ,   47.148, 3898.19 , 3890.3  ,   47.148, 3894.64 ,\n",
       "        3905.13 ,   47.148, 3993.69 ,   47.148, 3969.18 ], dtype=float32),\n",
       " array([3799.09 ,   47.148, 3864.86 , 3835.21 ,   47.148, 3877.85 ,\n",
       "        3842.58 ,   47.148, 3886.54 , 3844.54 ,   47.148, 3898.19 ,\n",
       "        3890.3  ,   47.148, 3894.64 , 3905.13 ,   47.148, 3993.69 ,\n",
       "        3876.19 ,   47.148, 3969.18 ,   47.148, 3947.93 ], dtype=float32),\n",
       " array([3835.21 ,   47.148, 3877.85 , 3842.58 ,   47.148, 3886.54 ,\n",
       "        3844.54 ,   47.148, 3898.19 , 3890.3  ,   47.148, 3894.64 ,\n",
       "        3905.13 ,   47.148, 3993.69 , 3876.19 ,   47.148, 3969.18 ,\n",
       "        3850.44 ,   47.148, 3947.93 ,   47.148, 3904.28 ], dtype=float32),\n",
       " array([3842.58 ,   47.148, 3886.54 , 3844.54 ,   47.148, 3898.19 ,\n",
       "        3890.3  ,   47.148, 3894.64 , 3905.13 ,   47.148, 3993.69 ,\n",
       "        3876.19 ,   47.148, 3969.18 , 3850.44 ,   47.148, 3947.93 ,\n",
       "        3835.15 ,   47.148, 3904.28 ,   47.148, 3897.35 ], dtype=float32),\n",
       " array([3844.54 ,   47.148, 3898.19 , 3890.3  ,   47.148, 3894.64 ,\n",
       "        3905.13 ,   47.148, 3993.69 , 3876.19 ,   47.148, 3969.18 ,\n",
       "        3850.44 ,   47.148, 3947.93 , 3835.15 ,   47.148, 3904.28 ,\n",
       "        3817.91 ,   47.148, 3897.35 ,   47.148, 3899.62 ], dtype=float32),\n",
       " array([3890.3  ,   47.148, 3894.64 , 3905.13 ,   47.148, 3993.69 ,\n",
       "        3876.19 ,   47.148, 3969.18 , 3850.44 ,   47.148, 3947.93 ,\n",
       "        3835.15 ,   47.148, 3904.28 , 3817.91 ,   47.148, 3897.35 ,\n",
       "        3830.85 ,   47.148, 3899.62 ,   47.148, 3900.83 ], dtype=float32),\n",
       " array([3905.13 ,   47.148, 3993.69 , 3876.19 ,   47.148, 3969.18 ,\n",
       "        3850.44 ,   47.148, 3947.93 , 3835.15 ,   47.148, 3904.28 ,\n",
       "        3817.91 ,   47.148, 3897.35 , 3830.85 ,   47.148, 3899.62 ,\n",
       "        3845.49 ,   47.148, 3900.83 ,   47.148, 3913.86 ], dtype=float32),\n",
       " array([3876.19 ,   47.148, 3969.18 , 3850.44 ,   47.148, 3947.93 ,\n",
       "        3835.15 ,   47.148, 3904.28 , 3817.91 ,   47.148, 3897.35 ,\n",
       "        3830.85 ,   47.148, 3899.62 , 3845.49 ,   47.148, 3900.83 ,\n",
       "        3838.27 ,   47.148, 3913.86 ,   47.148, 3911.75 ], dtype=float32),\n",
       " array([3850.44 ,   47.148, 3947.93 , 3835.15 ,   47.148, 3904.28 ,\n",
       "        3817.91 ,   47.148, 3897.35 , 3830.85 ,   47.148, 3899.62 ,\n",
       "        3845.49 ,   47.148, 3900.83 , 3838.27 ,   47.148, 3913.86 ,\n",
       "        3856.92 ,   47.148, 3911.75 ,   47.148, 3923.76 ], dtype=float32),\n",
       " array([3835.15 ,   47.148, 3904.28 , 3817.91 ,   47.148, 3897.35 ,\n",
       "        3830.85 ,   47.148, 3899.62 , 3845.49 ,   47.148, 3900.83 ,\n",
       "        3838.27 ,   47.148, 3913.86 , 3856.92 ,   47.148, 3911.75 ,\n",
       "        3870.11 ,   47.148, 3923.76 ,   47.148, 3952.53 ], dtype=float32),\n",
       " array([3817.91 ,   47.148, 3897.35 , 3830.85 ,   47.148, 3899.62 ,\n",
       "        3845.49 ,   47.148, 3900.83 , 3838.27 ,   47.148, 3913.86 ,\n",
       "        3856.92 ,   47.148, 3911.75 , 3870.11 ,   47.148, 3923.76 ,\n",
       "        3884.84 ,   47.148, 3952.53 ,   47.148, 3952.65 ], dtype=float32),\n",
       " array([3830.85 ,   47.148, 3899.62 , 3845.49 ,   47.148, 3900.83 ,\n",
       "        3838.27 ,   47.148, 3913.86 , 3856.92 ,   47.148, 3911.75 ,\n",
       "        3870.11 ,   47.148, 3923.76 , 3884.84 ,   47.148, 3952.53 ,\n",
       "        3900.95 ,   47.148, 3952.65 ,   47.148, 3946.56 ], dtype=float32),\n",
       " array([3845.49 ,   47.148, 3900.83 , 3838.27 ,   47.148, 3913.86 ,\n",
       "        3856.92 ,   47.148, 3911.75 , 3870.11 ,   47.148, 3923.76 ,\n",
       "        3884.84 ,   47.148, 3952.53 , 3900.95 ,   47.148, 3952.65 ,\n",
       "        3849.91 ,   47.148, 3946.56 ,   47.148, 3924.87 ], dtype=float32),\n",
       " array([3838.27 ,   47.148, 3913.86 , 3856.92 ,   47.148, 3911.75 ,\n",
       "        3870.11 ,   47.148, 3923.76 , 3884.84 ,   47.148, 3952.53 ,\n",
       "        3900.95 ,   47.148, 3952.65 , 3849.91 ,   47.148, 3946.56 ,\n",
       "        3962.31 ,   47.148, 3924.87 ,   47.148, 4045.24 ], dtype=float32),\n",
       " array([3856.92 ,   47.148, 3911.75 , 3870.11 ,   47.148, 3923.76 ,\n",
       "        3884.84 ,   47.148, 3952.53 , 3900.95 ,   47.148, 3952.65 ,\n",
       "        3849.91 ,   47.148, 3946.56 , 3962.31 ,   47.148, 3924.87 ,\n",
       "        3982.13 ,   47.148, 4045.24 ,   47.148, 4062.97 ], dtype=float32),\n",
       " array([3870.11 ,   47.148, 3923.76 , 3884.84 ,   47.148, 3952.53 ,\n",
       "        3900.95 ,   47.148, 3952.65 , 3849.91 ,   47.148, 3946.56 ,\n",
       "        3962.31 ,   47.148, 3924.87 , 3982.13 ,   47.148, 4045.24 ,\n",
       "        3994.63 ,   47.148, 4062.97 ,   47.148, 4054.67 ], dtype=float32),\n",
       " array([3884.84 ,   47.148, 3952.53 , 3900.95 ,   47.148, 3952.65 ,\n",
       "        3849.91 ,   47.148, 3946.56 , 3962.31 ,   47.148, 3924.87 ,\n",
       "        3982.13 ,   47.148, 4045.24 , 3994.63 ,   47.148, 4062.97 ,\n",
       "        3978.92 ,   47.148, 4054.67 ,   47.148, 4037.26 ], dtype=float32),\n",
       " array([3900.95 ,   47.148, 3952.65 , 3849.91 ,   47.148, 3946.56 ,\n",
       "        3962.31 ,   47.148, 3924.87 , 3982.13 ,   47.148, 4045.24 ,\n",
       "        3994.63 ,   47.148, 4062.97 , 3978.92 ,   47.148, 4054.67 ,\n",
       "        3998.2  ,   47.148, 4037.26 ,   47.148, 4075.93 ], dtype=float32),\n",
       " array([3849.91 ,   47.148, 3946.56 , 3962.31 ,   47.148, 3924.87 ,\n",
       "        3982.13 ,   47.148, 4045.24 , 3994.63 ,   47.148, 4062.97 ,\n",
       "        3978.92 ,   47.148, 4054.67 , 3998.2  ,   47.148, 4037.26 ,\n",
       "        4006.3  ,   47.148, 4075.93 ,   47.148, 4060.87 ], dtype=float32),\n",
       " array([3962.31 ,   47.148, 3924.87 , 3982.13 ,   47.148, 4045.24 ,\n",
       "        3994.63 ,   47.148, 4062.97 , 3978.92 ,   47.148, 4054.67 ,\n",
       "        3998.2  ,   47.148, 4037.26 , 4006.3  ,   47.148, 4075.93 ,\n",
       "        3994.42 ,   47.148, 4060.87 ,   47.148, 4071.04 ], dtype=float32),\n",
       " array([3982.13 ,   47.148, 4045.24 , 3994.63 ,   47.148, 4062.97 ,\n",
       "        3978.92 ,   47.148, 4054.67 , 3998.2  ,   47.148, 4037.26 ,\n",
       "        4006.3  ,   47.148, 4075.93 , 3994.42 ,   47.148, 4060.87 ,\n",
       "        3999.43 ,   47.148, 4071.04 ,   47.148, 4053.57 ], dtype=float32),\n",
       " array([3994.63 ,   47.148, 4062.97 , 3978.92 ,   47.148, 4054.67 ,\n",
       "        3998.2  ,   47.148, 4037.26 , 4006.3  ,   47.148, 4075.93 ,\n",
       "        3994.42 ,   47.148, 4060.87 , 3999.43 ,   47.148, 4071.04 ,\n",
       "        3974.9  ,   47.148, 4053.57 ,   47.148, 4043.1  ], dtype=float32),\n",
       " array([3978.92 ,   47.148, 4054.67 , 3998.2  ,   47.148, 4037.26 ,\n",
       "        4006.3  ,   47.148, 4075.93 , 3994.42 ,   47.148, 4060.87 ,\n",
       "        3999.43 ,   47.148, 4071.04 , 3974.9  ,   47.148, 4053.57 ,\n",
       "        3976.51 ,   47.148, 4043.1  ,   47.148, 4042.59 ], dtype=float32),\n",
       " array([3998.2  ,   47.148, 4037.26 , 4006.3  ,   47.148, 4075.93 ,\n",
       "        3994.42 ,   47.148, 4060.87 , 3999.43 ,   47.148, 4071.04 ,\n",
       "        3974.9  ,   47.148, 4053.57 , 3976.51 ,   47.148, 4043.1  ,\n",
       "        3986.94 ,   47.148, 4042.59 ,   47.148, 4054.15 ], dtype=float32),\n",
       " array([4006.3  ,   47.148, 4075.93 , 3994.42 ,   47.148, 4060.87 ,\n",
       "        3999.43 ,   47.148, 4071.04 , 3974.9  ,   47.148, 4053.57 ,\n",
       "        3976.51 ,   47.148, 4043.1  , 3986.94 ,   47.148, 4042.59 ,\n",
       "        4016.62 ,   47.148, 4054.15 ,   47.148, 4094.97 ], dtype=float32),\n",
       " array([3994.42 ,   47.148, 4060.87 , 3999.43 ,   47.148, 4071.04 ,\n",
       "        3974.9  ,   47.148, 4053.57 , 3976.51 ,   47.148, 4043.1  ,\n",
       "        3986.94 ,   47.148, 4042.59 , 4016.62 ,   47.148, 4054.15 ,\n",
       "        4018.86 ,   47.148, 4094.97 ,   47.148, 4081.57 ], dtype=float32),\n",
       " array([3999.43 ,   47.148, 4071.04 , 3974.9  ,   47.148, 4053.57 ,\n",
       "        3976.51 ,   47.148, 4043.1  , 3986.94 ,   47.148, 4042.59 ,\n",
       "        4016.62 ,   47.148, 4054.15 , 4018.86 ,   47.148, 4094.97 ,\n",
       "        4009.1  ,   47.148, 4081.57 ,   47.148, 4072.15 ], dtype=float32),\n",
       " array([3974.9  ,   47.148, 4053.57 , 3976.51 ,   47.148, 4043.1  ,\n",
       "        3986.94 ,   47.148, 4042.59 , 4016.62 ,   47.148, 4054.15 ,\n",
       "        4018.86 ,   47.148, 4094.97 , 4009.1  ,   47.148, 4081.57 ,\n",
       "        4010.15 ,   47.148, 4072.15 ,   47.148, 4084.66 ], dtype=float32),\n",
       " array([3976.51 ,   47.148, 4043.1  , 3986.94 ,   47.148, 4042.59 ,\n",
       "        4016.62 ,   47.148, 4054.15 , 4018.86 ,   47.148, 4094.97 ,\n",
       "        4009.1  ,   47.148, 4081.57 , 4010.15 ,   47.148, 4072.15 ,\n",
       "        3982.27 ,   47.148, 4084.66 ,   47.148, 4027.   ], dtype=float32),\n",
       " array([3986.94 ,   47.148, 4042.59 , 4016.62 ,   47.148, 4054.15 ,\n",
       "        4018.86 ,   47.148, 4094.97 , 4009.1  ,   47.148, 4081.57 ,\n",
       "        4010.15 ,   47.148, 4072.15 , 3982.27 ,   47.148, 4084.66 ,\n",
       "        3926.17 ,   47.148, 4027.   ,   47.148, 3993.33 ], dtype=float32),\n",
       " array([4016.62 ,   47.148, 4054.15 , 4018.86 ,   47.148, 4094.97 ,\n",
       "        4009.1  ,   47.148, 4081.57 , 4010.15 ,   47.148, 4072.15 ,\n",
       "        3982.27 ,   47.148, 4084.66 , 3926.17 ,   47.148, 4027.   ,\n",
       "        3950.76 ,   47.148, 3993.33 ,   47.148, 4022.31 ], dtype=float32),\n",
       " array([4018.86 ,   47.148, 4094.97 , 4009.1  ,   47.148, 4081.57 ,\n",
       "        4010.15 ,   47.148, 4072.15 , 3982.27 ,   47.148, 4084.66 ,\n",
       "        3926.17 ,   47.148, 4027.   , 3950.76 ,   47.148, 3993.33 ,\n",
       "        3953.55 ,   47.148, 4022.31 ,   47.148, 4024.13 ], dtype=float32),\n",
       " array([4009.1  ,   47.148, 4081.57 , 4010.15 ,   47.148, 4072.15 ,\n",
       "        3982.27 ,   47.148, 4084.66 , 3926.17 ,   47.148, 4027.   ,\n",
       "        3950.76 ,   47.148, 3993.33 , 3953.55 ,   47.148, 4022.31 ,\n",
       "        3970.64 ,   47.148, 4024.13 ,   47.148, 4043.52 ], dtype=float32),\n",
       " array([4010.15 ,   47.148, 4072.15 , 3982.27 ,   47.148, 4084.66 ,\n",
       "        3926.17 ,   47.148, 4027.   , 3950.76 ,   47.148, 3993.33 ,\n",
       "        3953.55 ,   47.148, 4022.31 , 3970.64 ,   47.148, 4024.13 ,\n",
       "        3968.3  ,   47.148, 4043.52 ,   47.148, 4021.09 ], dtype=float32),\n",
       " array([3982.27 ,   47.148, 4084.66 , 3926.17 ,   47.148, 4027.   ,\n",
       "        3950.76 ,   47.148, 3993.33 , 3953.55 ,   47.148, 4022.31 ,\n",
       "        3970.64 ,   47.148, 4024.13 , 3968.3  ,   47.148, 4043.52 ,\n",
       "        3944.13 ,   47.148, 4021.09 ,   47.148, 4020.75 ], dtype=float32),\n",
       " array([3926.17 ,   47.148, 4027.   , 3950.76 ,   47.148, 3993.33 ,\n",
       "        3953.55 ,   47.148, 4022.31 , 3970.64 ,   47.148, 4024.13 ,\n",
       "        3968.3  ,   47.148, 4043.52 , 3944.13 ,   47.148, 4021.09 ,\n",
       "        3961.09 ,   47.148, 4020.75 ,   47.618, 4013.8  ], dtype=float32),\n",
       " array([3950.76 ,   47.148, 3993.33 , 3953.55 ,   47.148, 4022.31 ,\n",
       "        3970.64 ,   47.148, 4024.13 , 3968.3  ,   47.148, 4043.52 ,\n",
       "        3944.13 ,   47.148, 4021.09 , 3961.09 ,   47.148, 4020.75 ,\n",
       "        3934.63 ,   47.618, 4013.8  ,   48.277, 4013.32 ], dtype=float32),\n",
       " array([3953.55 ,   47.148, 4022.31 , 3970.64 ,   47.148, 4024.13 ,\n",
       "        3968.3  ,   47.148, 4043.52 , 3944.13 ,   47.148, 4021.09 ,\n",
       "        3961.09 ,   47.148, 4020.75 , 3934.63 ,   47.618, 4013.8  ,\n",
       "        3941.8  ,   48.277, 4013.32 ,   48.528, 4007.63 ], dtype=float32),\n",
       " array([3970.64 ,   47.148, 4024.13 , 3968.3  ,   47.148, 4043.52 ,\n",
       "        3944.13 ,   47.148, 4021.09 , 3961.09 ,   47.148, 4020.75 ,\n",
       "        3934.63 ,   47.618, 4013.8  , 3941.8  ,   48.277, 4013.32 ,\n",
       "        4085.85 ,   48.528, 4007.63 ,   49.137, 4247.16 ], dtype=float32),\n",
       " array([3968.3  ,   47.148, 4043.52 , 3944.13 ,   47.148, 4021.09 ,\n",
       "        3961.09 ,   47.148, 4020.75 , 3934.63 ,   47.618, 4013.8  ,\n",
       "        3941.8  ,   48.277, 4013.32 , 4085.85 ,   48.528, 4007.63 ,\n",
       "        4181.09 ,   49.137, 4247.16 ,   49.287, 4262.76 ], dtype=float32),\n",
       " array([3944.13 ,   47.148, 4021.09 , 3961.09 ,   47.148, 4020.75 ,\n",
       "        3934.63 ,   47.618, 4013.8  , 3941.8  ,   48.277, 4013.32 ,\n",
       "        4085.85 ,   48.528, 4007.63 , 4181.09 ,   49.137, 4247.16 ,\n",
       "        4166.03 ,   49.287, 4262.76 ,   49.287, 4233.15 ], dtype=float32),\n",
       " array([3961.09 ,   47.148, 4020.75 , 3934.63 ,   47.618, 4013.8  ,\n",
       "        3941.8  ,   48.277, 4013.32 , 4085.85 ,   48.528, 4007.63 ,\n",
       "        4181.09 ,   49.137, 4247.16 , 4166.03 ,   49.287, 4262.76 ,\n",
       "        4173.2  ,   49.287, 4233.15 ,   49.287, 4250.2  ], dtype=float32),\n",
       " array([3934.63 ,   47.618, 4013.8  , 3941.8  ,   48.277, 4013.32 ,\n",
       "        4085.85 ,   48.528, 4007.63 , 4181.09 ,   49.137, 4247.16 ,\n",
       "        4166.03 ,   49.287, 4262.76 , 4173.2  ,   49.287, 4233.15 ,\n",
       "        4166.36 ,   49.287, 4250.2  ,   49.287, 4234.44 ], dtype=float32),\n",
       " array([3941.8  ,   48.277, 4013.32 , 4085.85 ,   48.528, 4007.63 ,\n",
       "        4181.09 ,   49.137, 4247.16 , 4166.03 ,   49.287, 4262.76 ,\n",
       "        4173.2  ,   49.287, 4233.15 , 4166.36 ,   49.287, 4250.2  ,\n",
       "        4204.17 ,   49.287, 4234.44 ,   49.287, 4280.14 ], dtype=float32),\n",
       " array([4085.85 ,   48.528, 4007.63 , 4181.09 ,   49.137, 4247.16 ,\n",
       "        4166.03 ,   49.287, 4262.76 , 4173.2  ,   49.287, 4233.15 ,\n",
       "        4166.36 ,   49.287, 4250.2  , 4204.17 ,   49.287, 4234.44 ,\n",
       "        4194.94 ,   49.287, 4280.14 ,   49.287, 4250.64 ], dtype=float32),\n",
       " array([4181.09 ,   49.137, 4247.16 , 4166.03 ,   49.287, 4262.76 ,\n",
       "        4173.2  ,   49.287, 4233.15 , 4166.36 ,   49.287, 4250.2  ,\n",
       "        4204.17 ,   49.287, 4234.44 , 4194.94 ,   49.287, 4280.14 ,\n",
       "        4193.22 ,   49.287, 4250.64 ,   49.287, 4251.27 ], dtype=float32),\n",
       " array([4166.03 ,   49.287, 4262.76 , 4173.2  ,   49.287, 4233.15 ,\n",
       "        4166.36 ,   49.287, 4250.2  , 4204.17 ,   49.287, 4234.44 ,\n",
       "        4194.94 ,   49.287, 4280.14 , 4193.22 ,   49.287, 4250.64 ,\n",
       "        4202.54 ,   49.287, 4251.27 ,   49.287, 4252.98 ], dtype=float32),\n",
       " array([4173.2  ,   49.287, 4233.15 , 4166.36 ,   49.287, 4250.2  ,\n",
       "        4204.17 ,   49.287, 4234.44 , 4194.94 ,   49.287, 4280.14 ,\n",
       "        4193.22 ,   49.287, 4250.64 , 4202.54 ,   49.287, 4251.27 ,\n",
       "        4203.09 ,   49.287, 4252.98 ,   49.287, 4257.22 ], dtype=float32),\n",
       " array([4166.36 ,   49.287, 4250.2  , 4204.17 ,   49.287, 4234.44 ,\n",
       "        4194.94 ,   49.287, 4280.14 , 4193.22 ,   49.287, 4250.64 ,\n",
       "        4202.54 ,   49.287, 4251.27 , 4203.09 ,   49.287, 4252.98 ,\n",
       "        4132.8  ,   49.287, 4257.22 ,   49.287, 4181.44 ], dtype=float32),\n",
       " array([4204.17 ,   49.287, 4234.44 , 4194.94 ,   49.287, 4280.14 ,\n",
       "        4193.22 ,   49.287, 4250.64 , 4202.54 ,   49.287, 4251.27 ,\n",
       "        4203.09 ,   49.287, 4252.98 , 4132.8  ,   49.287, 4257.22 ,\n",
       "        4131.8  ,   49.287, 4181.44 ,   49.287, 4188.94 ], dtype=float32),\n",
       " array([4194.94 ,   49.287, 4280.14 , 4193.22 ,   49.287, 4250.64 ,\n",
       "        4202.54 ,   49.287, 4251.27 , 4203.09 ,   49.287, 4252.98 ,\n",
       "        4132.8  ,   49.287, 4257.22 , 4131.8  ,   49.287, 4181.44 ,\n",
       "        4127.95 ,   49.287, 4188.94 ,   49.287, 4173.36 ], dtype=float32),\n",
       " array([4193.22 ,   49.287, 4250.64 , 4202.54 ,   49.287, 4251.27 ,\n",
       "        4203.09 ,   49.287, 4252.98 , 4132.8  ,   49.287, 4257.22 ,\n",
       "        4131.8  ,   49.287, 4181.44 , 4127.95 ,   49.287, 4188.94 ,\n",
       "        4140.27 ,   49.287, 4173.36 ,   49.287, 4207.19 ], dtype=float32),\n",
       " array([4202.54 ,   49.287, 4251.27 , 4203.09 ,   49.287, 4252.98 ,\n",
       "        4132.8  ,   49.287, 4257.22 , 4131.8  ,   49.287, 4181.44 ,\n",
       "        4127.95 ,   49.287, 4188.94 , 4140.27 ,   49.287, 4173.36 ,\n",
       "        4171.37 ,   49.287, 4207.19 ,   49.287, 4259.26 ], dtype=float32),\n",
       " array([4203.09 ,   49.287, 4252.98 , 4132.8  ,   49.287, 4257.22 ,\n",
       "        4131.8  ,   49.287, 4181.44 , 4127.95 ,   49.287, 4188.94 ,\n",
       "        4140.27 ,   49.287, 4173.36 , 4171.37 ,   49.287, 4207.19 ,\n",
       "        4173.23 ,   49.287, 4259.26 ,   49.287, 4106.58 ], dtype=float32),\n",
       " array([4132.8  ,   49.287, 4257.22 , 4131.8  ,   49.287, 4181.44 ,\n",
       "        4127.95 ,   49.287, 4188.94 , 4140.27 ,   49.287, 4173.36 ,\n",
       "        4171.37 ,   49.287, 4207.19 , 4173.23 ,   49.287, 4259.26 ,\n",
       "        4070.58 ,   49.287, 4106.58 ,   49.287, 4132.39 ], dtype=float32),\n",
       " array([4131.8  ,   49.287, 4181.44 , 4127.95 ,   49.287, 4188.94 ,\n",
       "        4140.27 ,   49.287, 4173.36 , 4171.37 ,   49.287, 4207.19 ,\n",
       "        4173.23 ,   49.287, 4259.26 , 4070.58 ,   49.287, 4106.58 ,\n",
       "        4084.11 ,   49.287, 4132.39 ,   49.287, 4130.7  ], dtype=float32),\n",
       " array([4127.95 ,   49.287, 4188.94 , 4140.27 ,   49.287, 4173.36 ,\n",
       "        4171.37 ,   49.287, 4207.19 , 4173.23 ,   49.287, 4259.26 ,\n",
       "        4070.58 ,   49.287, 4106.58 , 4084.11 ,   49.287, 4132.39 ,\n",
       "        4063.83 ,   49.287, 4130.7  ,   49.287, 4102.79 ], dtype=float32),\n",
       " array([4140.27 ,   49.287, 4173.36 , 4171.37 ,   49.287, 4207.19 ,\n",
       "        4173.23 ,   49.287, 4259.26 , 4070.58 ,   49.287, 4106.58 ,\n",
       "        4084.11 ,   49.287, 4132.39 , 4063.83 ,   49.287, 4130.7  ,\n",
       "        4014.58 ,   49.287, 4102.79 ,   49.287, 4075.97 ], dtype=float32),\n",
       " array([4171.37 ,   49.287, 4207.19 , 4173.23 ,   49.287, 4259.26 ,\n",
       "        4070.58 ,   49.287, 4106.58 , 4084.11 ,   49.287, 4132.39 ,\n",
       "        4063.83 ,   49.287, 4130.7  , 4014.58 ,   49.287, 4102.79 ,\n",
       "        4031.37 ,   49.287, 4075.97 ,   49.287, 4075.02 ], dtype=float32),\n",
       " array([4173.23 ,   49.287, 4259.26 , 4070.58 ,   49.287, 4106.58 ,\n",
       "        4084.11 ,   49.287, 4132.39 , 4063.83 ,   49.287, 4130.7  ,\n",
       "        4014.58 ,   49.287, 4102.79 , 4031.37 ,   49.287, 4075.97 ,\n",
       "        4042.5  ,   49.287, 4075.02 ,   49.287, 4113.97 ], dtype=float32),\n",
       " array([4070.58 ,   49.287, 4106.58 , 4084.11 ,   49.287, 4132.39 ,\n",
       "        4063.83 ,   49.287, 4130.7  , 4014.58 ,   49.287, 4102.79 ,\n",
       "        4031.37 ,   49.287, 4075.97 , 4042.5  ,   49.287, 4075.02 ,\n",
       "        4058.45 ,   49.287, 4113.97 ,   49.287, 4107.12 ], dtype=float32),\n",
       " array([4084.11 ,   49.287, 4132.39 , 4063.83 ,   49.287, 4130.7  ,\n",
       "        4014.58 ,   49.287, 4102.79 , 4031.37 ,   49.287, 4075.97 ,\n",
       "        4042.5  ,   49.287, 4075.02 , 4058.45 ,   49.287, 4113.97 ,\n",
       "        4045.69 ,   49.287, 4107.12 ,   49.287, 4081.95 ], dtype=float32),\n",
       " array([4063.83 ,   49.287, 4130.7  , 4014.58 ,   49.287, 4102.79 ,\n",
       "        4031.37 ,   49.287, 4075.97 , 4042.5  ,   49.287, 4075.02 ,\n",
       "        4058.45 ,   49.287, 4113.97 , 4045.69 ,   49.287, 4107.12 ,\n",
       "        3862.36 ,   49.287, 4081.95 ,   49.287, 3867.28 ], dtype=float32),\n",
       " array([4014.58 ,   49.287, 4102.79 , 4031.37 ,   49.287, 4075.97 ,\n",
       "        4042.5  ,   49.287, 4075.02 , 4058.45 ,   49.287, 4113.97 ,\n",
       "        4045.69 ,   49.287, 4107.12 , 3862.36 ,   49.287, 4081.95 ,\n",
       "        3824.19 ,   49.287, 3867.28 ,   49.287, 3873.04 ], dtype=float32),\n",
       " array([4031.37 ,   49.287, 4075.97 , 4042.5  ,   49.287, 4075.02 ,\n",
       "        4058.45 ,   49.287, 4113.97 , 4045.69 ,   49.287, 4107.12 ,\n",
       "        3862.36 ,   49.287, 4081.95 , 3824.19 ,   49.287, 3867.28 ,\n",
       "        3826.85 ,   49.287, 3873.04 ,   49.287, 3871.72 ], dtype=float32),\n",
       " array([4042.5  ,   49.287, 4075.02 , 4058.45 ,   49.287, 4113.97 ,\n",
       "        4045.69 ,   49.287, 4107.12 , 3862.36 ,   49.287, 4081.95 ,\n",
       "        3824.19 ,   49.287, 3867.28 , 3826.85 ,   49.287, 3873.04 ,\n",
       "        3778.62 ,   49.287, 3871.72 ,   49.287, 3785.52 ], dtype=float32),\n",
       " array([4058.45 ,   49.287, 4113.97 , 4045.69 ,   49.287, 4107.12 ,\n",
       "        3862.36 ,   49.287, 4081.95 , 3824.19 ,   49.287, 3867.28 ,\n",
       "        3826.85 ,   49.287, 3873.04 , 3778.62 ,   49.287, 3871.72 ,\n",
       "        3756.6  ,   49.287, 3785.52 ,   49.287, 3815.04 ], dtype=float32),\n",
       " array([4045.69 ,   49.287, 4107.12 , 3862.36 ,   49.287, 4081.95 ,\n",
       "        3824.19 ,   49.287, 3867.28 , 3826.85 ,   49.287, 3873.04 ,\n",
       "        3778.62 ,   49.287, 3871.72 , 3756.6  ,   49.287, 3785.52 ,\n",
       "        3788.71 ,   49.287, 3815.04 ,   49.287, 3841.24 ], dtype=float32),\n",
       " array([3862.36 ,   49.287, 4081.95 , 3824.19 ,   49.287, 3867.28 ,\n",
       "        3826.85 ,   49.287, 3873.04 , 3778.62 ,   49.287, 3871.72 ,\n",
       "        3756.6  ,   49.287, 3785.52 , 3788.71 ,   49.287, 3815.04 ,\n",
       "        3776.78 ,   49.287, 3841.24 ,   49.287, 3828.64 ], dtype=float32),\n",
       " array([3824.19 ,   49.287, 3867.28 , 3826.85 ,   49.287, 3873.04 ,\n",
       "        3778.62 ,   49.287, 3871.72 , 3756.6  ,   49.287, 3785.52 ,\n",
       "        3788.71 ,   49.287, 3815.04 , 3776.78 ,   49.287, 3841.24 ,\n",
       "        3791.59 ,   49.287, 3828.64 ,   49.287, 3818.45 ], dtype=float32),\n",
       " array([3826.85 ,   49.287, 3873.04 , 3778.62 ,   49.287, 3871.72 ,\n",
       "        3756.6  ,   49.287, 3785.52 , 3788.71 ,   49.287, 3815.04 ,\n",
       "        3776.78 ,   49.287, 3841.24 , 3791.59 ,   49.287, 3828.64 ,\n",
       "        3758.32 ,   49.287, 3818.45 ,   49.287, 3801.27 ], dtype=float32),\n",
       " array([3778.62 ,   49.287, 3871.72 , 3756.6  ,   49.287, 3785.52 ,\n",
       "        3788.71 ,   49.287, 3815.04 , 3776.78 ,   49.287, 3841.24 ,\n",
       "        3791.59 ,   49.287, 3828.64 , 3758.32 ,   49.287, 3818.45 ,\n",
       "        3766.68 ,   49.287, 3801.27 ,   49.287, 3836.45 ], dtype=float32),\n",
       " array([3756.6  ,   49.287, 3785.52 , 3788.71 ,   49.287, 3815.04 ,\n",
       "        3776.78 ,   49.287, 3841.24 , 3791.59 ,   49.287, 3828.64 ,\n",
       "        3758.32 ,   49.287, 3818.45 , 3766.68 ,   49.287, 3801.27 ,\n",
       "        3765.08 ,   49.287, 3836.45 ,   49.287, 3801.34 ], dtype=float32),\n",
       " array([3788.71 ,   49.287, 3815.04 , 3776.78 ,   49.287, 3841.24 ,\n",
       "        3791.59 ,   49.287, 3828.64 , 3758.32 ,   49.287, 3818.45 ,\n",
       "        3766.68 ,   49.287, 3801.27 , 3765.08 ,   49.287, 3836.45 ,\n",
       "        3741.31 ,   49.287, 3801.34 ,   49.287, 3813.65 ], dtype=float32),\n",
       " array([3776.78 ,   49.287, 3841.24 , 3791.59 ,   49.287, 3828.64 ,\n",
       "        3758.32 ,   49.287, 3818.45 , 3766.68 ,   49.287, 3801.27 ,\n",
       "        3765.08 ,   49.287, 3836.45 , 3741.31 ,   49.287, 3801.34 ,\n",
       "        3757.9  ,   49.287, 3813.65 ,   49.287, 3825.84 ], dtype=float32),\n",
       " array([3791.59 ,   49.287, 3828.64 , 3758.32 ,   49.287, 3818.45 ,\n",
       "        3766.68 ,   49.287, 3801.27 , 3765.08 ,   49.287, 3836.45 ,\n",
       "        3741.31 ,   49.287, 3801.34 , 3757.9  ,   49.287, 3813.65 ,\n",
       "        3765.91 ,   49.287, 3825.84 ,   49.287, 3810.12 ], dtype=float32),\n",
       " array([3758.32 ,   49.287, 3818.45 , 3766.68 ,   49.287, 3801.27 ,\n",
       "        3765.08 ,   49.287, 3836.45 , 3741.31 ,   49.287, 3801.34 ,\n",
       "        3757.9  ,   49.287, 3813.65 , 3765.91 ,   49.287, 3825.84 ,\n",
       "        3752.91 ,   49.287, 3810.12 ,   49.287, 3802.24 ], dtype=float32),\n",
       " array([3766.68 ,   49.287, 3801.27 , 3765.08 ,   49.287, 3836.45 ,\n",
       "        3741.31 ,   49.287, 3801.34 , 3757.9  ,   49.287, 3813.65 ,\n",
       "        3765.91 ,   49.287, 3825.84 , 3752.91 ,   49.287, 3810.12 ,\n",
       "        3720.79 ,   49.287, 3802.24 ,   49.287, 3778.48 ], dtype=float32),\n",
       " array([3765.08 ,   49.287, 3836.45 , 3741.31 ,   49.287, 3801.34 ,\n",
       "        3757.9  ,   49.287, 3813.65 , 3765.91 ,   49.287, 3825.84 ,\n",
       "        3752.91 ,   49.287, 3810.12 , 3720.79 ,   49.287, 3802.24 ,\n",
       "        3746.32 ,   49.287, 3778.48 ,   49.287, 3796.39 ], dtype=float32),\n",
       " array([3741.31 ,   49.287, 3801.34 , 3757.9  ,   49.287, 3813.65 ,\n",
       "        3765.91 ,   49.287, 3825.84 , 3752.91 ,   49.287, 3810.12 ,\n",
       "        3720.79 ,   49.287, 3802.24 , 3746.32 ,   49.287, 3778.48 ,\n",
       "        3710.05 ,   49.287, 3796.39 ,   49.287, 3759.43 ], dtype=float32),\n",
       " array([3757.9  ,   49.287, 3813.65 , 3765.91 ,   49.287, 3825.84 ,\n",
       "        3752.91 ,   49.287, 3810.12 , 3720.79 ,   49.287, 3802.24 ,\n",
       "        3746.32 ,   49.287, 3778.48 , 3710.05 ,   49.287, 3796.39 ,\n",
       "        3718.85 ,   49.287, 3759.43 ,   49.287, 3789.27 ], dtype=float32),\n",
       " array([3765.91 ,   49.287, 3825.84 , 3752.91 ,   49.287, 3810.12 ,\n",
       "        3720.79 ,   49.287, 3802.24 , 3746.32 ,   49.287, 3778.48 ,\n",
       "        3710.05 ,   49.287, 3796.39 , 3718.85 ,   49.287, 3759.43 ,\n",
       "        3736.96 ,   49.287, 3789.27 ,   49.287, 3795.64 ], dtype=float32),\n",
       " array([3752.91 ,   49.287, 3810.12 , 3720.79 ,   49.287, 3802.24 ,\n",
       "        3746.32 ,   49.287, 3778.48 , 3710.05 ,   49.287, 3796.39 ,\n",
       "        3718.85 ,   49.287, 3759.43 , 3736.96 ,   49.287, 3789.27 ,\n",
       "        3782.46 ,   49.287, 3795.64 ,   49.287, 3836.63 ], dtype=float32),\n",
       " array([3720.79 ,   49.287, 3802.24 , 3746.32 ,   49.287, 3778.48 ,\n",
       "        3710.05 ,   49.287, 3796.39 , 3718.85 ,   49.287, 3759.43 ,\n",
       "        3736.96 ,   49.287, 3789.27 , 3782.46 ,   49.287, 3795.64 ,\n",
       "        3779.57 ,   49.287, 3836.63 ,   49.078, 3846.32 ], dtype=float32),\n",
       " array([3746.32 ,   49.287, 3778.48 , 3710.05 ,   49.287, 3796.39 ,\n",
       "        3718.85 ,   49.287, 3759.43 , 3736.96 ,   49.287, 3789.27 ,\n",
       "        3782.46 ,   49.287, 3795.64 , 3779.57 ,   49.287, 3836.63 ,\n",
       "        3799.8  ,   49.078, 3846.32 ,   50.078, 3823.3  ], dtype=float32),\n",
       " array([3710.05 ,   49.287, 3796.39 , 3718.85 ,   49.287, 3759.43 ,\n",
       "        3736.96 ,   49.287, 3789.27 , 3782.46 ,   49.287, 3795.64 ,\n",
       "        3779.57 ,   49.287, 3836.63 , 3799.8  ,   49.078, 3846.32 ,\n",
       "        3755.62 ,   50.078, 3823.3  ,   50.258, 3834.73 ], dtype=float32),\n",
       " array([3718.85 ,   49.287, 3759.43 , 3736.96 ,   49.287, 3789.27 ,\n",
       "        3782.46 ,   49.287, 3795.64 , 3779.57 ,   49.287, 3836.63 ,\n",
       "        3799.8  ,   49.078, 3846.32 , 3755.62 ,   50.078, 3823.3  ,\n",
       "        3771.58 ,   50.258, 3834.73 ,   48.868, 3827.09 ], dtype=float32),\n",
       " array([3736.96 ,   49.287, 3789.27 , 3782.46 ,   49.287, 3795.64 ,\n",
       "        3779.57 ,   49.287, 3836.63 , 3799.8  ,   49.078, 3846.32 ,\n",
       "        3755.62 ,   50.078, 3823.3  , 3771.58 ,   50.258, 3834.73 ,\n",
       "        3821.71 ,   48.868, 3827.09 ,   47.508, 3899.78 ], dtype=float32),\n",
       " array([3782.46 ,   49.287, 3795.64 , 3779.57 ,   49.287, 3836.63 ,\n",
       "        3799.8  ,   49.078, 3846.32 , 3755.62 ,   50.078, 3823.3  ,\n",
       "        3771.58 ,   50.258, 3834.73 , 3821.71 ,   48.868, 3827.09 ,\n",
       "        3840.14 ,   47.508, 3899.78 ,   47.568, 3908.52 ], dtype=float32),\n",
       " array([3779.57 ,   49.287, 3836.63 , 3799.8  ,   49.078, 3846.32 ,\n",
       "        3755.62 ,   50.078, 3823.3  , 3771.58 ,   50.258, 3834.73 ,\n",
       "        3821.71 ,   48.868, 3827.09 , 3840.14 ,   47.508, 3899.78 ,\n",
       "        3838.65 ,   47.568, 3908.52 ,   47.187, 3893.13 ], dtype=float32),\n",
       " array([3799.8  ,   49.078, 3846.32 , 3755.62 ,   50.078, 3823.3  ,\n",
       "        3771.58 ,   50.258, 3834.73 , 3821.71 ,   48.868, 3827.09 ,\n",
       "        3840.14 ,   47.508, 3899.78 , 3838.65 ,   47.568, 3908.52 ,\n",
       "        3820.08 ,   47.187, 3893.13 ,   46.857, 3878.99 ], dtype=float32),\n",
       " array([3755.62 ,   50.078, 3823.3  , 3771.58 ,   50.258, 3834.73 ,\n",
       "        3821.71 ,   48.868, 3827.09 , 3840.14 ,   47.508, 3899.78 ,\n",
       "        3838.65 ,   47.568, 3908.52 , 3820.08 ,   47.187, 3893.13 ,\n",
       "        3828.31 ,   46.857, 3878.99 ,   46.857, 3886.92 ], dtype=float32),\n",
       " array([3771.58 ,   50.258, 3834.73 , 3821.71 ,   48.868, 3827.09 ,\n",
       "        3840.14 ,   47.508, 3899.78 , 3838.65 ,   47.568, 3908.52 ,\n",
       "        3820.08 ,   47.187, 3893.13 , 3828.31 ,   46.857, 3878.99 ,\n",
       "        3815.26 ,   46.857, 3886.92 ,   46.857, 3854.5  ], dtype=float32),\n",
       " array([3821.71 ,   48.868, 3827.09 , 3840.14 ,   47.508, 3899.78 ,\n",
       "        3838.65 ,   47.568, 3908.52 , 3820.08 ,   47.187, 3893.13 ,\n",
       "        3828.31 ,   46.857, 3878.99 , 3815.26 ,   46.857, 3886.92 ,\n",
       "        3778.39 ,   46.857, 3854.5  ,   46.857, 3833.02 ], dtype=float32),\n",
       " array([3840.14 ,   47.508, 3899.78 , 3838.65 ,   47.568, 3908.52 ,\n",
       "        3820.08 ,   47.187, 3893.13 , 3828.31 ,   46.857, 3878.99 ,\n",
       "        3815.26 ,   46.857, 3886.92 , 3778.39 ,   46.857, 3854.5  ,\n",
       "        3758.1  ,   46.857, 3833.02 ,   46.857, 3809.08 ], dtype=float32),\n",
       " array([3838.65 ,   47.568, 3908.52 , 3820.08 ,   47.187, 3893.13 ,\n",
       "        3828.31 ,   46.857, 3878.99 , 3815.26 ,   46.857, 3886.92 ,\n",
       "        3778.39 ,   46.857, 3854.5  , 3758.1  ,   46.857, 3833.02 ,\n",
       "        3749.7  ,   46.857, 3809.08 ,   46.857, 3789.27 ], dtype=float32),\n",
       " array([3820.08 ,   47.187, 3893.13 , 3828.31 ,   46.857, 3878.99 ,\n",
       "        3815.26 ,   46.857, 3886.92 , 3778.39 ,   46.857, 3854.5  ,\n",
       "        3758.1  ,   46.857, 3833.02 , 3749.7  ,   46.857, 3809.08 ,\n",
       "        3738.6  ,   46.857, 3789.27 ,   46.857, 3803.93 ], dtype=float32),\n",
       " array([3828.31 ,   46.857, 3878.99 , 3815.26 ,   46.857, 3886.92 ,\n",
       "        3778.39 ,   46.857, 3854.5  , 3758.1  ,   46.857, 3833.02 ,\n",
       "        3749.7  ,   46.857, 3809.08 , 3738.6  ,   46.857, 3789.27 ,\n",
       "        3747.92 ,   46.857, 3803.93 ,   46.857, 3811.26 ], dtype=float32),\n",
       " array([3815.26 ,   46.857, 3886.92 , 3778.39 ,   46.857, 3854.5  ,\n",
       "        3758.1  ,   46.857, 3833.02 , 3749.7  ,   46.857, 3809.08 ,\n",
       "        3738.6  ,   46.857, 3789.27 , 3747.92 ,   46.857, 3803.93 ,\n",
       "        3753.24 ,   46.857, 3811.26 ,   46.857, 3821.75 ], dtype=float32),\n",
       " array([3778.39 ,   46.857, 3854.5  , 3758.1  ,   46.857, 3833.02 ,\n",
       "        3749.7  ,   46.857, 3809.08 , 3738.6  ,   46.857, 3789.27 ,\n",
       "        3747.92 ,   46.857, 3803.93 , 3753.24 ,   46.857, 3811.26 ,\n",
       "        3771.63 ,   46.857, 3821.75 ,   46.857, 3822.23 ], dtype=float32),\n",
       " array([3758.1  ,   46.857, 3833.02 , 3749.7  ,   46.857, 3809.08 ,\n",
       "        3738.6  ,   46.857, 3789.27 , 3747.92 ,   46.857, 3803.93 ,\n",
       "        3753.24 ,   46.857, 3811.26 , 3771.63 ,   46.857, 3821.75 ,\n",
       "        3718.34 ,   46.857, 3822.23 ,   46.857, 3768.5  ], dtype=float32),\n",
       " array([3749.7  ,   46.857, 3809.08 , 3738.6  ,   46.857, 3789.27 ,\n",
       "        3747.92 ,   46.857, 3803.93 , 3753.24 ,   46.857, 3811.26 ,\n",
       "        3771.63 ,   46.857, 3821.75 , 3718.34 ,   46.857, 3822.23 ,\n",
       "        3729.69 ,   46.857, 3768.5  ,   46.857, 3789.99 ], dtype=float32),\n",
       " array([3738.6  ,   46.857, 3789.27 , 3747.92 ,   46.857, 3803.93 ,\n",
       "        3753.24 ,   46.857, 3811.26 , 3771.63 ,   46.857, 3821.75 ,\n",
       "        3718.34 ,   46.857, 3822.23 , 3729.69 ,   46.857, 3768.5  ,\n",
       "        3743.7  ,   46.857, 3789.99 ,   46.857, 3812.13 ], dtype=float32),\n",
       " array([3747.92 ,   46.857, 3803.93 , 3753.24 ,   46.857, 3811.26 ,\n",
       "        3771.63 ,   46.857, 3821.75 , 3718.34 ,   46.857, 3822.23 ,\n",
       "        3729.69 ,   46.857, 3768.5  , 3743.7  ,   46.857, 3789.99 ,\n",
       "        3752.87 ,   46.857, 3812.13 ,   46.857, 3816.66 ], dtype=float32),\n",
       " array([3753.24 ,   46.857, 3811.26 , 3771.63 ,   46.857, 3821.75 ,\n",
       "        3718.34 ,   46.857, 3822.23 , 3729.69 ,   46.857, 3768.5  ,\n",
       "        3743.7  ,   46.857, 3789.99 , 3752.87 ,   46.857, 3812.13 ,\n",
       "        3753.21 ,   46.857, 3816.66 ,   46.857, 3811.65 ], dtype=float32),\n",
       " array([3771.63 ,   46.857, 3821.75 , 3718.34 ,   46.857, 3822.23 ,\n",
       "        3729.69 ,   46.857, 3768.5  , 3743.7  ,   46.857, 3789.99 ,\n",
       "        3752.87 ,   46.857, 3812.13 , 3753.21 ,   46.857, 3816.66 ,\n",
       "        3796.47 ,   46.857, 3811.65 ,   46.857, 3851.29 ], dtype=float32),\n",
       " array([3718.34 ,   46.857, 3822.23 , 3729.69 ,   46.857, 3768.5  ,\n",
       "        3743.7  ,   46.857, 3789.99 , 3752.87 ,   46.857, 3812.13 ,\n",
       "        3753.21 ,   46.857, 3816.66 , 3796.47 ,   46.857, 3811.65 ,\n",
       "        3788.33 ,   46.857, 3851.29 ,   46.857, 3811.06 ], dtype=float32),\n",
       " array([3729.69 ,   46.857, 3768.5  , 3743.7  ,   46.857, 3789.99 ,\n",
       "        3752.87 ,   46.857, 3812.13 , 3753.21 ,   46.857, 3816.66 ,\n",
       "        3796.47 ,   46.857, 3811.65 , 3788.33 ,   46.857, 3851.29 ,\n",
       "        3766.51 ,   46.857, 3811.06 ,   49.627, 3816.89 ], dtype=float32),\n",
       " array([3743.7  ,   46.857, 3789.99 , 3752.87 ,   46.857, 3812.13 ,\n",
       "        3753.21 ,   46.857, 3816.66 , 3796.47 ,   46.857, 3811.65 ,\n",
       "        3788.33 ,   46.857, 3851.29 , 3766.51 ,   46.857, 3811.06 ,\n",
       "        3808.6  ,   49.627, 3816.89 ,   49.557, 3851.5  ], dtype=float32),\n",
       " array([3752.87 ,   46.857, 3812.13 , 3753.21 ,   46.857, 3816.66 ,\n",
       "        3796.47 ,   46.857, 3811.65 , 3788.33 ,   46.857, 3851.29 ,\n",
       "        3766.51 ,   46.857, 3811.06 , 3808.6  ,   49.627, 3816.89 ,\n",
       "        3812.61 ,   49.557, 3851.5  ,   50.768, 3848.78 ], dtype=float32),\n",
       " array([3753.21 ,   46.857, 3816.66 , 3796.47 ,   46.857, 3811.65 ,\n",
       "        3788.33 ,   46.857, 3851.29 , 3766.51 ,   46.857, 3811.06 ,\n",
       "        3808.6  ,   49.627, 3816.89 , 3812.61 ,   49.557, 3851.5  ,\n",
       "        3797.04 ,   50.768, 3848.78 ,   50.268, 3862.73 ], dtype=float32),\n",
       " array([3796.47 ,   46.857, 3811.65 , 3788.33 ,   46.857, 3851.29 ,\n",
       "        3766.51 ,   46.857, 3811.06 , 3808.6  ,   49.627, 3816.89 ,\n",
       "        3812.61 ,   49.557, 3851.5  , 3797.04 ,   50.768, 3848.78 ,\n",
       "        3787.4  ,   50.268, 3862.73 ,   50.637, 3812.24 ], dtype=float32),\n",
       " array([3788.33 ,   46.857, 3851.29 , 3766.51 ,   46.857, 3811.06 ,\n",
       "        3808.6  ,   49.627, 3816.89 , 3812.61 ,   49.557, 3851.5  ,\n",
       "        3797.04 ,   50.768, 3848.78 , 3787.4  ,   50.268, 3862.73 ,\n",
       "        3744.   ,   50.637, 3812.24 ,   51.118, 3812.76 ], dtype=float32),\n",
       " array([3766.51 ,   46.857, 3811.06 , 3808.6  ,   49.627, 3816.89 ,\n",
       "        3812.61 ,   49.557, 3851.5  , 3797.04 ,   50.768, 3848.78 ,\n",
       "        3787.4  ,   50.268, 3862.73 , 3744.   ,   50.637, 3812.24 ,\n",
       "        3753.25 ,   51.118, 3812.76 ,   50.757, 3789.04 ], dtype=float32),\n",
       " array([3808.6  ,   49.627, 3816.89 , 3812.61 ,   49.557, 3851.5  ,\n",
       "        3797.04 ,   50.768, 3848.78 , 3787.4  ,   50.268, 3862.73 ,\n",
       "        3744.   ,   50.637, 3812.24 , 3753.25 ,   51.118, 3812.76 ,\n",
       "        3745.8  ,   50.757, 3789.04 ,   48.748, 3795.75 ], dtype=float32),\n",
       " array([3812.61 ,   49.557, 3851.5  , 3797.04 ,   50.768, 3848.78 ,\n",
       "        3787.4  ,   50.268, 3862.73 , 3744.   ,   50.637, 3812.24 ,\n",
       "        3753.25 ,   51.118, 3812.76 , 3745.8  ,   50.757, 3789.04 ,\n",
       "        3720.64 ,   48.748, 3795.75 ,   48.748, 3769.35 ], dtype=float32),\n",
       " array([3797.04 ,   50.768, 3848.78 , 3787.4  ,   50.268, 3862.73 ,\n",
       "        3744.   ,   50.637, 3812.24 , 3753.25 ,   51.118, 3812.76 ,\n",
       "        3745.8  ,   50.757, 3789.04 , 3720.64 ,   48.748, 3795.75 ,\n",
       "        3739.52 ,   48.748, 3769.35 ,   48.748, 3787.77 ], dtype=float32),\n",
       " array([3787.4  ,   50.268, 3862.73 , 3744.   ,   50.637, 3812.24 ,\n",
       "        3753.25 ,   51.118, 3812.76 , 3745.8  ,   50.757, 3789.04 ,\n",
       "        3720.64 ,   48.748, 3795.75 , 3739.52 ,   48.748, 3769.35 ,\n",
       "        3741.53 ,   48.748, 3787.77 ,   48.748, 3794.85 ], dtype=float32),\n",
       " array([3744.   ,   50.637, 3812.24 , 3753.25 ,   51.118, 3812.76 ,\n",
       "        3745.8  ,   50.757, 3789.04 , 3720.64 ,   48.748, 3795.75 ,\n",
       "        3739.52 ,   48.748, 3769.35 , 3741.53 ,   48.748, 3787.77 ,\n",
       "        3748.79 ,   48.748, 3794.85 ,   48.748, 3777.82 ], dtype=float32),\n",
       " array([3753.25 ,   51.118, 3812.76 , 3745.8  ,   50.757, 3789.04 ,\n",
       "        3720.64 ,   48.748, 3795.75 , 3739.52 ,   48.748, 3769.35 ,\n",
       "        3741.53 ,   48.748, 3787.77 , 3748.79 ,   48.748, 3794.85 ,\n",
       "        3742.73 ,   48.748, 3777.82 ,   48.748, 3790.23 ], dtype=float32),\n",
       " array([3745.8  ,   50.757, 3789.04 , 3720.64 ,   48.748, 3795.75 ,\n",
       "        3739.52 ,   48.748, 3769.35 , 3741.53 ,   48.748, 3787.77 ,\n",
       "        3748.79 ,   48.748, 3794.85 , 3742.73 ,   48.748, 3777.82 ,\n",
       "        3742.43 ,   48.748, 3790.23 ,   48.748, 3773.94 ], dtype=float32),\n",
       " array([3720.64 ,   48.748, 3795.75 , 3739.52 ,   48.748, 3769.35 ,\n",
       "        3741.53 ,   48.748, 3787.77 , 3748.79 ,   48.748, 3794.85 ,\n",
       "        3742.73 ,   48.748, 3777.82 , 3742.43 ,   48.748, 3790.23 ,\n",
       "        3738.12 ,   48.748, 3773.94 ,   48.748, 3787.67 ], dtype=float32),\n",
       " array([3739.52 ,   48.748, 3769.35 , 3741.53 ,   48.748, 3787.77 ,\n",
       "        3748.79 ,   48.748, 3794.85 , 3742.73 ,   48.748, 3777.82 ,\n",
       "        3742.43 ,   48.748, 3790.23 , 3738.12 ,   48.748, 3773.94 ,\n",
       "        3750.76 ,   48.748, 3787.67 ,   48.748, 3793.31 ], dtype=float32),\n",
       " array([3741.53 ,   48.748, 3787.77 , 3748.79 ,   48.748, 3794.85 ,\n",
       "        3742.73 ,   48.748, 3777.82 , 3742.43 ,   48.748, 3790.23 ,\n",
       "        3738.12 ,   48.748, 3773.94 , 3750.76 ,   48.748, 3787.67 ,\n",
       "        3759.62 ,   48.748, 3793.31 ,   48.748, 3802.46 ], dtype=float32),\n",
       " array([3748.79 ,   48.748, 3794.85 , 3742.73 ,   48.748, 3777.82 ,\n",
       "        3742.43 ,   48.748, 3790.23 , 3738.12 ,   48.748, 3773.94 ,\n",
       "        3750.76 ,   48.748, 3787.67 , 3759.62 ,   48.748, 3793.31 ,\n",
       "        3746.64 ,   48.748, 3802.46 ,   48.748, 3795.13 ], dtype=float32),\n",
       " array([3742.73 ,   48.748, 3777.82 , 3742.43 ,   48.748, 3790.23 ,\n",
       "        3738.12 ,   48.748, 3773.94 , 3750.76 ,   48.748, 3787.67 ,\n",
       "        3759.62 ,   48.748, 3793.31 , 3746.64 ,   48.748, 3802.46 ,\n",
       "        3731.39 ,   48.748, 3795.13 ,   48.748, 3787.1  ], dtype=float32),\n",
       " array([3742.43 ,   48.748, 3790.23 , 3738.12 ,   48.748, 3773.94 ,\n",
       "        3750.76 ,   48.748, 3787.67 , 3759.62 ,   48.748, 3793.31 ,\n",
       "        3746.64 ,   48.748, 3802.46 , 3731.39 ,   48.748, 3795.13 ,\n",
       "        3750.09 ,   48.748, 3787.1  ,   48.748, 3794.72 ], dtype=float32),\n",
       " array([3738.12 ,   48.748, 3773.94 , 3750.76 ,   48.748, 3787.67 ,\n",
       "        3759.62 ,   48.748, 3793.31 , 3746.64 ,   48.748, 3802.46 ,\n",
       "        3731.39 ,   48.748, 3795.13 , 3750.09 ,   48.748, 3787.1  ,\n",
       "        3745.52 ,   48.748, 3794.72 ,   48.748, 3795.77 ], dtype=float32),\n",
       " array([3750.76 ,   48.748, 3787.67 , 3759.62 ,   48.748, 3793.31 ,\n",
       "        3746.64 ,   48.748, 3802.46 , 3731.39 ,   48.748, 3795.13 ,\n",
       "        3750.09 ,   48.748, 3787.1  , 3745.52 ,   48.748, 3794.72 ,\n",
       "        3741.19 ,   48.748, 3795.77 ,   48.748, 3785.34 ], dtype=float32),\n",
       " array([3759.62 ,   48.748, 3793.31 , 3746.64 ,   48.748, 3802.46 ,\n",
       "        3731.39 ,   48.748, 3795.13 , 3750.09 ,   48.748, 3787.1  ,\n",
       "        3745.52 ,   48.748, 3794.72 , 3741.19 ,   48.748, 3795.77 ,\n",
       "        3622.1  ,   48.748, 3785.34 ,   48.748, 3662.62 ], dtype=float32),\n",
       " array([3746.64 ,   48.748, 3802.46 , 3731.39 ,   48.748, 3795.13 ,\n",
       "        3750.09 ,   48.748, 3787.1  , 3745.52 ,   48.748, 3794.72 ,\n",
       "        3741.19 ,   48.748, 3795.77 , 3622.1  ,   48.748, 3785.34 ,\n",
       "        3590.54 ,   48.748, 3662.62 ,   48.748, 3637.03 ], dtype=float32),\n",
       " array([3731.39 ,   48.748, 3795.13 , 3750.09 ,   48.748, 3787.1  ,\n",
       "        3745.52 ,   48.748, 3794.72 , 3741.19 ,   48.748, 3795.77 ,\n",
       "        3622.1  ,   48.748, 3785.34 , 3590.54 ,   48.748, 3662.62 ,\n",
       "        3593.3  ,   48.748, 3637.03 ,   48.827, 3641.38 ], dtype=float32),\n",
       " array([3750.09 ,   48.748, 3787.1  , 3745.52 ,   48.748, 3794.72 ,\n",
       "        3741.19 ,   48.748, 3795.77 , 3622.1  ,   48.748, 3785.34 ,\n",
       "        3590.54 ,   48.748, 3662.62 , 3593.3  ,   48.748, 3637.03 ,\n",
       "        3586.69 ,   48.827, 3641.38 ,   48.678, 3645.03 ], dtype=float32),\n",
       " array([3745.52 ,   48.748, 3794.72 , 3741.19 ,   48.748, 3795.77 ,\n",
       "        3622.1  ,   48.748, 3785.34 , 3590.54 ,   48.748, 3662.62 ,\n",
       "        3593.3  ,   48.748, 3637.03 , 3586.69 ,   48.827, 3641.38 ,\n",
       "        3582.29 ,   48.678, 3645.03 ,   50.058, 3646.09 ], dtype=float32),\n",
       " array([3741.19 ,   48.748, 3795.77 , 3622.1  ,   48.748, 3785.34 ,\n",
       "        3590.54 ,   48.748, 3662.62 , 3593.3  ,   48.748, 3637.03 ,\n",
       "        3586.69 ,   48.827, 3641.38 , 3582.29 ,   48.678, 3645.03 ,\n",
       "        3604.44 ,   50.058, 3646.09 ,   49.608, 3661.36 ], dtype=float32),\n",
       " array([3622.1  ,   48.748, 3785.34 , 3590.54 ,   48.748, 3662.62 ,\n",
       "        3593.3  ,   48.748, 3637.03 , 3586.69 ,   48.827, 3641.38 ,\n",
       "        3582.29 ,   48.678, 3645.03 , 3604.44 ,   50.058, 3646.09 ,\n",
       "        3599.77 ,   49.608, 3661.36 ,   49.117, 3644.85 ], dtype=float32),\n",
       " array([3590.54 ,   48.748, 3662.62 , 3593.3  ,   48.748, 3637.03 ,\n",
       "        3586.69 ,   48.827, 3641.38 , 3582.29 ,   48.678, 3645.03 ,\n",
       "        3604.44 ,   50.058, 3646.09 , 3599.77 ,   49.608, 3661.36 ,\n",
       "        3599.16 ,   49.117, 3644.85 ,   48.928, 3662.11 ], dtype=float32),\n",
       " array([3593.3  ,   48.748, 3637.03 , 3586.69 ,   48.827, 3641.38 ,\n",
       "        3582.29 ,   48.678, 3645.03 , 3604.44 ,   50.058, 3646.09 ,\n",
       "        3599.77 ,   49.608, 3661.36 , 3599.16 ,   49.117, 3644.85 ,\n",
       "        3600.39 ,   48.928, 3662.11 ,   47.778, 3653.85 ], dtype=float32),\n",
       " array([3586.69 ,   48.827, 3641.38 , 3582.29 ,   48.678, 3645.03 ,\n",
       "        3604.44 ,   50.058, 3646.09 , 3599.77 ,   49.608, 3661.36 ,\n",
       "        3599.16 ,   49.117, 3644.85 , 3600.39 ,   48.928, 3662.11 ,\n",
       "        3592.69 ,   47.778, 3653.85 ,   48.767, 3646.2  ], dtype=float32),\n",
       " array([3582.29 ,   48.678, 3645.03 , 3604.44 ,   50.058, 3646.09 ,\n",
       "        3599.77 ,   49.608, 3661.36 , 3599.16 ,   49.117, 3644.85 ,\n",
       "        3600.39 ,   48.928, 3662.11 , 3592.69 ,   47.778, 3653.85 ,\n",
       "        3597.75 ,   48.767, 3646.2  ,   48.767, 3646.06 ], dtype=float32),\n",
       " array([3604.44 ,   50.058, 3646.09 , 3599.77 ,   49.608, 3661.36 ,\n",
       "        3599.16 ,   49.117, 3644.85 , 3600.39 ,   48.928, 3662.11 ,\n",
       "        3592.69 ,   47.778, 3653.85 , 3597.75 ,   48.767, 3646.2  ,\n",
       "        3587.61 ,   48.767, 3646.06 ,   48.767, 3643.48 ], dtype=float32),\n",
       " array([3599.77 ,   49.608, 3661.36 , 3599.16 ,   49.117, 3644.85 ,\n",
       "        3600.39 ,   48.928, 3662.11 , 3592.69 ,   47.778, 3653.85 ,\n",
       "        3597.75 ,   48.767, 3646.2  , 3587.61 ,   48.767, 3646.06 ,\n",
       "        3604.75 ,   48.767, 3643.48 ,   48.767, 3662.7  ], dtype=float32),\n",
       " array([3599.16 ,   49.117, 3644.85 , 3600.39 ,   48.928, 3662.11 ,\n",
       "        3592.69 ,   47.778, 3653.85 , 3597.75 ,   48.767, 3646.2  ,\n",
       "        3587.61 ,   48.767, 3646.06 , 3604.75 ,   48.767, 3643.48 ,\n",
       "        3609.93 ,   48.767, 3662.7  ,   48.767, 3671.31 ], dtype=float32),\n",
       " array([3600.39 ,   48.928, 3662.11 , 3592.69 ,   47.778, 3653.85 ,\n",
       "        3597.75 ,   48.767, 3646.2  , 3587.61 ,   48.767, 3646.06 ,\n",
       "        3604.75 ,   48.767, 3643.48 , 3609.93 ,   48.767, 3662.7  ,\n",
       "        3617.22 ,   48.767, 3671.31 ,   48.767, 3680.64 ], dtype=float32),\n",
       " array([3592.69 ,   47.778, 3653.85 , 3597.75 ,   48.767, 3646.2  ,\n",
       "        3587.61 ,   48.767, 3646.06 , 3604.75 ,   48.767, 3643.48 ,\n",
       "        3609.93 ,   48.767, 3662.7  , 3617.22 ,   48.767, 3671.31 ,\n",
       "        3612.36 ,   48.767, 3680.64 ,   48.767, 3674.74 ], dtype=float32),\n",
       " array([3597.75 ,   48.767, 3646.2  , 3587.61 ,   48.767, 3646.06 ,\n",
       "        3604.75 ,   48.767, 3643.48 , 3609.93 ,   48.767, 3662.7  ,\n",
       "        3617.22 ,   48.767, 3671.31 , 3612.36 ,   48.767, 3680.64 ,\n",
       "        3627.61 ,   48.767, 3674.74 ,   48.767, 3692.02 ], dtype=float32),\n",
       " array([3587.61 ,   48.767, 3646.06 , 3604.75 ,   48.767, 3643.48 ,\n",
       "        3609.93 ,   48.767, 3662.7  , 3617.22 ,   48.767, 3671.31 ,\n",
       "        3612.36 ,   48.767, 3680.64 , 3627.61 ,   48.767, 3674.74 ,\n",
       "        3613.51 ,   48.767, 3692.02 ,   48.767, 3671.96 ], dtype=float32),\n",
       " array([3604.75 ,   48.767, 3643.48 , 3609.93 ,   48.767, 3662.7  ,\n",
       "        3617.22 ,   48.767, 3671.31 , 3612.36 ,   48.767, 3680.64 ,\n",
       "        3627.61 ,   48.767, 3674.74 , 3613.51 ,   48.767, 3692.02 ,\n",
       "        3607.59 ,   48.767, 3671.96 ,   48.767, 3668.24 ], dtype=float32),\n",
       " array([3609.93 ,   48.767, 3662.7  , 3617.22 ,   48.767, 3671.31 ,\n",
       "        3612.36 ,   48.767, 3680.64 , 3627.61 ,   48.767, 3674.74 ,\n",
       "        3613.51 ,   48.767, 3692.02 , 3607.59 ,   48.767, 3671.96 ,\n",
       "        3607.93 ,   48.767, 3668.24 ,   48.767, 3669.94 ], dtype=float32),\n",
       " array([3617.22 ,   48.767, 3671.31 , 3612.36 ,   48.767, 3680.64 ,\n",
       "        3627.61 ,   48.767, 3674.74 , 3613.51 ,   48.767, 3692.02 ,\n",
       "        3607.59 ,   48.767, 3671.96 , 3607.93 ,   48.767, 3668.24 ,\n",
       "        3734.51 ,   48.767, 3669.94 ,   48.767, 3893.92 ], dtype=float32),\n",
       " array([3612.36 ,   48.767, 3680.64 , 3627.61 ,   48.767, 3674.74 ,\n",
       "        3613.51 ,   48.767, 3692.02 , 3607.59 ,   48.767, 3671.96 ,\n",
       "        3607.93 ,   48.767, 3668.24 , 3734.51 ,   48.767, 3669.94 ,\n",
       "        3860.51 ,   48.767, 3893.92 ,   48.767, 3920.38 ], dtype=float32),\n",
       " array([3627.61 ,   48.767, 3674.74 , 3613.51 ,   48.767, 3692.02 ,\n",
       "        3607.59 ,   48.767, 3671.96 , 3607.93 ,   48.767, 3668.24 ,\n",
       "        3734.51 ,   48.767, 3669.94 , 3860.51 ,   48.767, 3893.92 ,\n",
       "        3847.1  ,   48.767, 3920.38 ,   48.767, 3920.23 ], dtype=float32),\n",
       " array([3613.51 ,   48.767, 3692.02 , 3607.59 ,   48.767, 3671.96 ,\n",
       "        3607.93 ,   48.767, 3668.24 , 3734.51 ,   48.767, 3669.94 ,\n",
       "        3860.51 ,   48.767, 3893.92 , 3847.1  ,   48.767, 3920.38 ,\n",
       "        3859.98 ,   48.767, 3920.23 ,   48.767, 3933.33 ], dtype=float32),\n",
       " array([3607.59 ,   48.767, 3671.96 , 3607.93 ,   48.767, 3668.24 ,\n",
       "        3734.51 ,   48.767, 3669.94 , 3860.51 ,   48.767, 3893.92 ,\n",
       "        3847.1  ,   48.767, 3920.38 , 3859.98 ,   48.767, 3920.23 ,\n",
       "        3902.92 ,   48.767, 3933.33 ,   48.767, 3942.33 ], dtype=float32),\n",
       " array([3607.93 ,   48.767, 3668.24 , 3734.51 ,   48.767, 3669.94 ,\n",
       "        3860.51 ,   48.767, 3893.92 , 3847.1  ,   48.767, 3920.38 ,\n",
       "        3859.98 ,   48.767, 3920.23 , 3902.92 ,   48.767, 3933.33 ,\n",
       "        3883.78 ,   48.767, 3942.33 ,   48.767, 3965.26 ], dtype=float32),\n",
       " array([3734.51 ,   48.767, 3669.94 , 3860.51 ,   48.767, 3893.92 ,\n",
       "        3847.1  ,   48.767, 3920.38 , 3859.98 ,   48.767, 3920.23 ,\n",
       "        3902.92 ,   48.767, 3933.33 , 3883.78 ,   48.767, 3942.33 ,\n",
       "        3899.95 ,   48.767, 3965.26 ,   48.767, 3950.28 ], dtype=float32),\n",
       " array([3860.51 ,   48.767, 3893.92 , 3847.1  ,   48.767, 3920.38 ,\n",
       "        3859.98 ,   48.767, 3920.23 , 3902.92 ,   48.767, 3933.33 ,\n",
       "        3883.78 ,   48.767, 3942.33 , 3899.95 ,   48.767, 3965.26 ,\n",
       "        3884.08 ,   48.767, 3950.28 ,   48.767, 3929.05 ], dtype=float32),\n",
       " array([3847.1  ,   48.767, 3920.38 , 3859.98 ,   48.767, 3920.23 ,\n",
       "        3902.92 ,   48.767, 3933.33 , 3883.78 ,   48.767, 3942.33 ,\n",
       "        3899.95 ,   48.767, 3965.26 , 3884.08 ,   48.767, 3950.28 ,\n",
       "        3875.49 ,   48.767, 3929.05 ,   48.767, 3947.86 ], dtype=float32),\n",
       " array([3859.98 ,   48.767, 3920.23 , 3902.92 ,   48.767, 3933.33 ,\n",
       "        3883.78 ,   48.767, 3942.33 , 3899.95 ,   48.767, 3965.26 ,\n",
       "        3884.08 ,   48.767, 3950.28 , 3875.49 ,   48.767, 3929.05 ,\n",
       "        3899.21 ,   48.767, 3947.86 ,   48.767, 3953.73 ], dtype=float32),\n",
       " array([3902.92 ,   48.767, 3933.33 , 3883.78 ,   48.767, 3942.33 ,\n",
       "        3899.95 ,   48.767, 3965.26 , 3884.08 ,   48.767, 3950.28 ,\n",
       "        3875.49 ,   48.767, 3929.05 , 3899.21 ,   48.767, 3947.86 ,\n",
       "        3895.   ,   48.767, 3953.73 ,   48.767, 3958.96 ], dtype=float32),\n",
       " array([3883.78 ,   48.767, 3942.33 , 3899.95 ,   48.767, 3965.26 ,\n",
       "        3884.08 ,   48.767, 3950.28 , 3875.49 ,   48.767, 3929.05 ,\n",
       "        3899.21 ,   48.767, 3947.86 , 3895.   ,   48.767, 3953.73 ,\n",
       "        3881.97 ,   48.767, 3958.96 ,   48.767, 3947.21 ], dtype=float32),\n",
       " array([3899.95 ,   48.767, 3965.26 , 3884.08 ,   48.767, 3950.28 ,\n",
       "        3875.49 ,   48.767, 3929.05 , 3899.21 ,   48.767, 3947.86 ,\n",
       "        3895.   ,   48.767, 3953.73 , 3881.97 ,   48.767, 3958.96 ,\n",
       "        3885.74 ,   48.767, 3947.21 ,   48.767, 3944.91 ], dtype=float32),\n",
       " array([3884.08 ,   48.767, 3950.28 , 3875.49 ,   48.767, 3929.05 ,\n",
       "        3899.21 ,   48.767, 3947.86 , 3895.   ,   48.767, 3953.73 ,\n",
       "        3881.97 ,   48.767, 3958.96 , 3885.74 ,   48.767, 3947.21 ,\n",
       "        3878.8  ,   48.767, 3944.91 ,   48.767, 3945.61 ], dtype=float32),\n",
       " array([3875.49 ,   48.767, 3929.05 , 3899.21 ,   48.767, 3947.86 ,\n",
       "        3895.   ,   48.767, 3953.73 , 3881.97 ,   48.767, 3958.96 ,\n",
       "        3885.74 ,   48.767, 3947.21 , 3878.8  ,   48.767, 3944.91 ,\n",
       "        3880.6  ,   48.767, 3945.61 ,   48.767, 3932.61 ], dtype=float32),\n",
       " array([3899.21 ,   48.767, 3947.86 , 3895.   ,   48.767, 3953.73 ,\n",
       "        3881.97 ,   48.767, 3958.96 , 3885.74 ,   48.767, 3947.21 ,\n",
       "        3878.8  ,   48.767, 3944.91 , 3880.6  ,   48.767, 3945.61 ,\n",
       "        3883.47 ,   48.767, 3932.61 ,   48.767, 3946.85 ], dtype=float32),\n",
       " array([3895.   ,   48.767, 3953.73 , 3881.97 ,   48.767, 3958.96 ,\n",
       "        3885.74 ,   48.767, 3947.21 , 3878.8  ,   48.767, 3944.91 ,\n",
       "        3880.6  ,   48.767, 3945.61 , 3883.47 ,   48.767, 3932.61 ,\n",
       "        3895.01 ,   48.767, 3946.85 ,   48.767, 3966.57 ], dtype=float32),\n",
       " array([3881.97 ,   48.767, 3958.96 , 3885.74 ,   48.767, 3947.21 ,\n",
       "        3878.8  ,   48.767, 3944.91 , 3880.6  ,   48.767, 3945.61 ,\n",
       "        3883.47 ,   48.767, 3932.61 , 3895.01 ,   48.767, 3946.85 ,\n",
       "        3896.71 ,   48.767, 3966.57 ,   48.767, 3957.95 ], dtype=float32),\n",
       " array([3885.74 ,   48.767, 3947.21 , 3878.8  ,   48.767, 3944.91 ,\n",
       "        3880.6  ,   48.767, 3945.61 , 3883.47 ,   48.767, 3932.61 ,\n",
       "        3895.01 ,   48.767, 3946.85 , 3896.71 ,   48.767, 3966.57 ,\n",
       "        3921.32 ,   48.767, 3957.95 ,   48.767, 3949.38 ], dtype=float32),\n",
       " array([3878.8  ,   48.767, 3944.91 , 3880.6  ,   48.767, 3945.61 ,\n",
       "        3883.47 ,   48.767, 3932.61 , 3895.01 ,   48.767, 3946.85 ,\n",
       "        3896.71 ,   48.767, 3966.57 , 3921.32 ,   48.767, 3957.95 ,\n",
       "        3855.24 ,   48.767, 3949.38 ,   48.767, 3905.28 ], dtype=float32),\n",
       " array([3880.6  ,   48.767, 3945.61 , 3883.47 ,   48.767, 3932.61 ,\n",
       "        3895.01 ,   48.767, 3946.85 , 3896.71 ,   48.767, 3966.57 ,\n",
       "        3921.32 ,   48.767, 3957.95 , 3855.24 ,   48.767, 3949.38 ,\n",
       "        3862.46 ,   48.767, 3905.28 ,   48.767, 3922.22 ], dtype=float32),\n",
       " array([3883.47 ,   48.767, 3932.61 , 3895.01 ,   48.767, 3946.85 ,\n",
       "        3896.71 ,   48.767, 3966.57 , 3921.32 ,   48.767, 3957.95 ,\n",
       "        3855.24 ,   48.767, 3949.38 , 3862.46 ,   48.767, 3905.28 ,\n",
       "        3877.96 ,   48.767, 3922.22 ,   48.767, 3927.72 ], dtype=float32),\n",
       " array([3895.01 ,   48.767, 3946.85 , 3896.71 ,   48.767, 3966.57 ,\n",
       "        3921.32 ,   48.767, 3957.95 , 3855.24 ,   48.767, 3949.38 ,\n",
       "        3862.46 ,   48.767, 3905.28 , 3877.96 ,   48.767, 3922.22 ,\n",
       "        3874.19 ,   48.767, 3927.72 ,   48.767, 3937.72 ], dtype=float32),\n",
       " array([3896.71 ,   48.767, 3966.57 , 3921.32 ,   48.767, 3957.95 ,\n",
       "        3855.24 ,   48.767, 3949.38 , 3862.46 ,   48.767, 3905.28 ,\n",
       "        3877.96 ,   48.767, 3922.22 , 3874.19 ,   48.767, 3927.72 ,\n",
       "        3869.35 ,   48.767, 3937.72 ,   48.767, 3922.53 ], dtype=float32),\n",
       " array([3921.32 ,   48.767, 3957.95 , 3855.24 ,   48.767, 3949.38 ,\n",
       "        3862.46 ,   48.767, 3905.28 , 3877.96 ,   48.767, 3922.22 ,\n",
       "        3874.19 ,   48.767, 3927.72 , 3869.35 ,   48.767, 3937.72 ,\n",
       "        3873.55 ,   48.767, 3922.53 ,   48.767, 3926.27 ], dtype=float32),\n",
       " array([3855.24 ,   48.767, 3949.38 , 3862.46 ,   48.767, 3905.28 ,\n",
       "        3877.96 ,   48.767, 3922.22 , 3874.19 ,   48.767, 3927.72 ,\n",
       "        3869.35 ,   48.767, 3937.72 , 3873.55 ,   48.767, 3922.53 ,\n",
       "        3831.89 ,   48.767, 3926.27 ,   48.767, 3890.93 ], dtype=float32),\n",
       " array([3862.46 ,   48.767, 3905.28 , 3877.96 ,   48.767, 3922.22 ,\n",
       "        3874.19 ,   48.767, 3927.72 , 3869.35 ,   48.767, 3937.72 ,\n",
       "        3873.55 ,   48.767, 3922.53 , 3831.89 ,   48.767, 3926.27 ,\n",
       "        3834.05 ,   48.767, 3890.93 ,   48.767, 3897.59 ], dtype=float32),\n",
       " array([3877.96 ,   48.767, 3922.22 , 3874.19 ,   48.767, 3927.72 ,\n",
       "        3869.35 ,   48.767, 3937.72 , 3873.55 ,   48.767, 3922.53 ,\n",
       "        3831.89 ,   48.767, 3926.27 , 3834.05 ,   48.767, 3890.93 ,\n",
       "        3832.62 ,   48.767, 3897.59 ,   48.767, 3893.13 ], dtype=float32),\n",
       " array([3874.19 ,   48.767, 3927.72 , 3869.35 ,   48.767, 3937.72 ,\n",
       "        3873.55 ,   48.767, 3922.53 , 3831.89 ,   48.767, 3926.27 ,\n",
       "        3834.05 ,   48.767, 3890.93 , 3832.62 ,   48.767, 3897.59 ,\n",
       "        3831.43 ,   48.767, 3893.13 ,   48.767, 3896.65 ], dtype=float32),\n",
       " array([3869.35 ,   48.767, 3937.72 , 3873.55 ,   48.767, 3922.53 ,\n",
       "        3831.89 ,   48.767, 3926.27 , 3834.05 ,   48.767, 3890.93 ,\n",
       "        3832.62 ,   48.767, 3897.59 , 3831.43 ,   48.767, 3893.13 ,\n",
       "        3855.96 ,   48.767, 3896.65 ,   48.767, 3914.04 ], dtype=float32),\n",
       " array([3873.55 ,   48.767, 3922.53 , 3831.89 ,   48.767, 3926.27 ,\n",
       "        3834.05 ,   48.767, 3890.93 , 3832.62 ,   48.767, 3897.59 ,\n",
       "        3831.43 ,   48.767, 3893.13 , 3855.96 ,   48.767, 3896.65 ,\n",
       "        3898.91 ,   48.767, 3914.04 ,   48.767, 3975.07 ], dtype=float32),\n",
       " array([3831.89 ,   48.767, 3926.27 , 3834.05 ,   48.767, 3890.93 ,\n",
       "        3832.62 ,   48.767, 3897.59 , 3831.43 ,   48.767, 3893.13 ,\n",
       "        3855.96 ,   48.767, 3896.65 , 3898.91 ,   48.767, 3914.04 ,\n",
       "        3904.95 ,   48.767, 3975.07 ,   48.767, 3958.2  ], dtype=float32),\n",
       " array([3834.05 ,   48.767, 3890.93 , 3832.62 ,   48.767, 3897.59 ,\n",
       "        3831.43 ,   48.767, 3893.13 , 3855.96 ,   48.767, 3896.65 ,\n",
       "        3898.91 ,   48.767, 3914.04 , 3904.95 ,   48.767, 3975.07 ,\n",
       "        3814.62 ,   48.767, 3958.2  ,   48.767, 3797.06 ], dtype=float32),\n",
       " array([3832.62 ,   48.767, 3897.59 , 3831.43 ,   48.767, 3893.13 ,\n",
       "        3855.96 ,   48.767, 3896.65 , 3898.91 ,   48.767, 3914.04 ,\n",
       "        3904.95 ,   48.767, 3975.07 , 3814.62 ,   48.767, 3958.2  ,\n",
       "        3724.61 ,   48.767, 3797.06 ,   48.767, 3790.38 ], dtype=float32),\n",
       " array([3831.43 ,   48.767, 3893.13 , 3855.96 ,   48.767, 3896.65 ,\n",
       "        3898.91 ,   48.767, 3914.04 , 3904.95 ,   48.767, 3975.07 ,\n",
       "        3814.62 ,   48.767, 3958.2  , 3724.61 ,   48.767, 3797.06 ,\n",
       "        3729.51 ,   48.767, 3790.38 ,   48.767, 3801.62 ], dtype=float32),\n",
       " array([3855.96 ,   48.767, 3896.65 , 3898.91 ,   48.767, 3914.04 ,\n",
       "        3904.95 ,   48.767, 3975.07 , 3814.62 ,   48.767, 3958.2  ,\n",
       "        3724.61 ,   48.767, 3797.06 , 3729.51 ,   48.767, 3790.38 ,\n",
       "        3738.56 ,   48.767, 3801.62 ,   48.767, 3805.08 ], dtype=float32),\n",
       " array([3898.91 ,   48.767, 3914.04 , 3904.95 ,   48.767, 3975.07 ,\n",
       "        3814.62 ,   48.767, 3958.2  , 3724.61 ,   48.767, 3797.06 ,\n",
       "        3729.51 ,   48.767, 3790.38 , 3738.56 ,   48.767, 3801.62 ,\n",
       "        3733.08 ,   48.767, 3805.08 ,   48.767, 3802.57 ], dtype=float32),\n",
       " array([3904.95 ,   48.767, 3975.07 , 3814.62 ,   48.767, 3958.2  ,\n",
       "        3724.61 ,   48.767, 3797.06 , 3729.51 ,   48.767, 3790.38 ,\n",
       "        3738.56 ,   48.767, 3801.62 , 3733.08 ,   48.767, 3805.08 ,\n",
       "        3742.78 ,   48.767, 3802.57 ,   48.767, 3831.17 ], dtype=float32),\n",
       " array([3814.62 ,   48.767, 3958.2  , 3724.61 ,   48.767, 3797.06 ,\n",
       "        3729.51 ,   48.767, 3790.38 , 3738.56 ,   48.767, 3801.62 ,\n",
       "        3733.08 ,   48.767, 3805.08 , 3742.78 ,   48.767, 3802.57 ,\n",
       "        3753.34 ,   48.767, 3831.17 ,   48.767, 3820.96 ], dtype=float32),\n",
       " array([3724.61 ,   48.767, 3797.06 , 3729.51 ,   48.767, 3790.38 ,\n",
       "        3738.56 ,   48.767, 3801.62 , 3733.08 ,   48.767, 3805.08 ,\n",
       "        3742.78 ,   48.767, 3802.57 , 3753.34 ,   48.767, 3831.17 ,\n",
       "        3755.52 ,   48.767, 3820.96 ,   48.767, 3824.81 ], dtype=float32),\n",
       " array([3729.51 ,   48.767, 3790.38 , 3738.56 ,   48.767, 3801.62 ,\n",
       "        3733.08 ,   48.767, 3805.08 , 3742.78 ,   48.767, 3802.57 ,\n",
       "        3753.34 ,   48.767, 3831.17 , 3755.52 ,   48.767, 3820.96 ,\n",
       "        3756.01 ,   48.767, 3824.81 ,   48.767, 3821.98 ], dtype=float32),\n",
       " array([3738.56 ,   48.767, 3801.62 , 3733.08 ,   48.767, 3805.08 ,\n",
       "        3742.78 ,   48.767, 3802.57 , 3753.34 ,   48.767, 3831.17 ,\n",
       "        3755.52 ,   48.767, 3820.96 , 3756.01 ,   48.767, 3824.81 ,\n",
       "        3768.37 ,   48.767, 3821.98 ,   48.767, 3854.23 ], dtype=float32),\n",
       " array([3733.08 ,   48.767, 3805.08 , 3742.78 ,   48.767, 3802.57 ,\n",
       "        3753.34 ,   48.767, 3831.17 , 3755.52 ,   48.767, 3820.96 ,\n",
       "        3756.01 ,   48.767, 3824.81 , 3768.37 ,   48.767, 3821.98 ,\n",
       "        3822.51 ,   48.767, 3854.23 ,   48.767, 3901.89 ], dtype=float32),\n",
       " array([3742.78 ,   48.767, 3802.57 , 3753.34 ,   48.767, 3831.17 ,\n",
       "        3755.52 ,   48.767, 3820.96 , 3756.01 ,   48.767, 3824.81 ,\n",
       "        3768.37 ,   48.767, 3821.98 , 3822.51 ,   48.767, 3854.23 ,\n",
       "        3828.34 ,   48.767, 3901.89 ,   48.767, 3900.92 ], dtype=float32),\n",
       " array([3753.34 ,   48.767, 3831.17 , 3755.52 ,   48.767, 3820.96 ,\n",
       "        3756.01 ,   48.767, 3824.81 , 3768.37 ,   48.767, 3821.98 ,\n",
       "        3822.51 ,   48.767, 3854.23 , 3828.34 ,   48.767, 3901.89 ,\n",
       "        3830.21 ,   48.767, 3900.92 ,   48.767, 3889.94 ], dtype=float32),\n",
       " array([3755.52 ,   48.767, 3820.96 , 3756.01 ,   48.767, 3824.81 ,\n",
       "        3768.37 ,   48.767, 3821.98 , 3822.51 ,   48.767, 3854.23 ,\n",
       "        3828.34 ,   48.767, 3901.89 , 3830.21 ,   48.767, 3900.92 ,\n",
       "        3796.14 ,   48.767, 3889.94 ,   48.767, 3864.11 ], dtype=float32),\n",
       " array([3756.01 ,   48.767, 3824.81 , 3768.37 ,   48.767, 3821.98 ,\n",
       "        3822.51 ,   48.767, 3854.23 , 3828.34 ,   48.767, 3901.89 ,\n",
       "        3830.21 ,   48.767, 3900.92 , 3796.14 ,   48.767, 3889.94 ,\n",
       "        3803.22 ,   48.767, 3864.11 ,   48.767, 3855.86 ], dtype=float32),\n",
       " array([3768.37 ,   48.767, 3821.98 , 3822.51 ,   48.767, 3854.23 ,\n",
       "        3828.34 ,   48.767, 3901.89 , 3830.21 ,   48.767, 3900.92 ,\n",
       "        3796.14 ,   48.767, 3889.94 , 3803.22 ,   48.767, 3864.11 ,\n",
       "        3800.31 ,   48.767, 3855.86 ,   48.767, 3867.41 ], dtype=float32),\n",
       " array([3822.51 ,   48.767, 3854.23 , 3828.34 ,   48.767, 3901.89 ,\n",
       "        3830.21 ,   48.767, 3900.92 , 3796.14 ,   48.767, 3889.94 ,\n",
       "        3803.22 ,   48.767, 3864.11 , 3800.31 ,   48.767, 3855.86 ,\n",
       "        3811.57 ,   48.767, 3867.41 ,   48.767, 3861.16 ], dtype=float32),\n",
       " array([3828.34 ,   48.767, 3901.89 , 3830.21 ,   48.767, 3900.92 ,\n",
       "        3796.14 ,   48.767, 3889.94 , 3803.22 ,   48.767, 3864.11 ,\n",
       "        3800.31 ,   48.767, 3855.86 , 3811.57 ,   48.767, 3867.41 ,\n",
       "        3802.06 ,   48.767, 3861.16 ,   48.767, 3865.83 ], dtype=float32),\n",
       " array([3830.21 ,   48.767, 3900.92 , 3796.14 ,   48.767, 3889.94 ,\n",
       "        3803.22 ,   48.767, 3864.11 , 3800.31 ,   48.767, 3855.86 ,\n",
       "        3811.57 ,   48.767, 3867.41 , 3802.06 ,   48.767, 3861.16 ,\n",
       "        3814.59 ,   48.767, 3865.83 ,   48.767, 3878.52 ], dtype=float32),\n",
       " array([3796.14 ,   48.767, 3889.94 , 3803.22 ,   48.767, 3864.11 ,\n",
       "        3800.31 ,   48.767, 3855.86 , 3811.57 ,   48.767, 3867.41 ,\n",
       "        3802.06 ,   48.767, 3861.16 , 3814.59 ,   48.767, 3865.83 ,\n",
       "        3810.48 ,   48.767, 3878.52 ,   48.767, 3874.02 ], dtype=float32),\n",
       " array([3803.22 ,   48.767, 3864.11 , 3800.31 ,   48.767, 3855.86 ,\n",
       "        3811.57 ,   48.767, 3867.41 , 3802.06 ,   48.767, 3861.16 ,\n",
       "        3814.59 ,   48.767, 3865.83 , 3810.48 ,   48.767, 3878.52 ,\n",
       "        3799.68 ,   48.767, 3874.02 ,   48.767, 3870.82 ], dtype=float32),\n",
       " array([3800.31 ,   48.767, 3855.86 , 3811.57 ,   48.767, 3867.41 ,\n",
       "        3802.06 ,   48.767, 3861.16 , 3814.59 ,   48.767, 3865.83 ,\n",
       "        3810.48 ,   48.767, 3878.52 , 3799.68 ,   48.767, 3874.02 ,\n",
       "        3813.48 ,   48.767, 3870.82 ,   48.767, 3880.26 ], dtype=float32),\n",
       " array([3811.57 ,   48.767, 3867.41 , 3802.06 ,   48.767, 3861.16 ,\n",
       "        3814.59 ,   48.767, 3865.83 , 3810.48 ,   48.767, 3878.52 ,\n",
       "        3799.68 ,   48.767, 3874.02 , 3813.48 ,   48.767, 3870.82 ,\n",
       "        3822.23 ,   48.767, 3880.26 ,   47.438, 3884.74 ], dtype=float32),\n",
       " array([3802.06 ,   48.767, 3861.16 , 3814.59 ,   48.767, 3865.83 ,\n",
       "        3810.48 ,   48.767, 3878.52 , 3799.68 ,   48.767, 3874.02 ,\n",
       "        3813.48 ,   48.767, 3870.82 , 3822.23 ,   48.767, 3880.26 ,\n",
       "        3816.46 ,   47.438, 3884.74 ,   47.468, 3885.54 ], dtype=float32),\n",
       " array([3814.59 ,   48.767, 3865.83 , 3810.48 ,   48.767, 3878.52 ,\n",
       "        3799.68 ,   48.767, 3874.02 , 3813.48 ,   48.767, 3870.82 ,\n",
       "        3822.23 ,   48.767, 3880.26 , 3816.46 ,   47.438, 3884.74 ,\n",
       "        3823.07 ,   47.468, 3885.54 ,   47.737, 3896.21 ], dtype=float32),\n",
       " array([3810.48 ,   48.767, 3878.52 , 3799.68 ,   48.767, 3874.02 ,\n",
       "        3813.48 ,   48.767, 3870.82 , 3822.23 ,   48.767, 3880.26 ,\n",
       "        3816.46 ,   47.438, 3884.74 , 3823.07 ,   47.468, 3885.54 ,\n",
       "        3823.67 ,   47.737, 3896.21 ,   47.508, 3892.24 ], dtype=float32),\n",
       " array([3799.68 ,   48.767, 3874.02 , 3813.48 ,   48.767, 3870.82 ,\n",
       "        3822.23 ,   48.767, 3880.26 , 3816.46 ,   47.438, 3884.74 ,\n",
       "        3823.07 ,   47.468, 3885.54 , 3823.67 ,   47.737, 3896.21 ,\n",
       "        3800.84 ,   47.508, 3892.24 ,   47.148, 3858.9  ], dtype=float32),\n",
       " array([3813.48 ,   48.767, 3870.82 , 3822.23 ,   48.767, 3880.26 ,\n",
       "        3816.46 ,   47.438, 3884.74 , 3823.07 ,   47.468, 3885.54 ,\n",
       "        3823.67 ,   47.737, 3896.21 , 3800.84 ,   47.508, 3892.24 ,\n",
       "        3778.78 ,   47.148, 3858.9  ,   47.518, 3834.99 ], dtype=float32),\n",
       " array([3822.23 ,   48.767, 3880.26 , 3816.46 ,   47.438, 3884.74 ,\n",
       "        3823.07 ,   47.468, 3885.54 , 3823.67 ,   47.737, 3896.21 ,\n",
       "        3800.84 ,   47.508, 3892.24 , 3778.78 ,   47.148, 3858.9  ,\n",
       "        3777.78 ,   47.518, 3834.99 ,   47.207, 3844.65 ], dtype=float32),\n",
       " array([3816.46 ,   47.438, 3884.74 , 3823.07 ,   47.468, 3885.54 ,\n",
       "        3823.67 ,   47.737, 3896.21 , 3800.84 ,   47.508, 3892.24 ,\n",
       "        3778.78 ,   47.148, 3858.9  , 3777.78 ,   47.518, 3834.99 ,\n",
       "        3785.79 ,   47.207, 3844.65 ,   46.948, 3848.2  ], dtype=float32),\n",
       " array([3823.07 ,   47.468, 3885.54 , 3823.67 ,   47.737, 3896.21 ,\n",
       "        3800.84 ,   47.508, 3892.24 , 3778.78 ,   47.148, 3858.9  ,\n",
       "        3777.78 ,   47.518, 3834.99 , 3785.79 ,   47.207, 3844.65 ,\n",
       "        3778.32 ,   46.948, 3848.2  ,   46.948, 3842.66 ], dtype=float32),\n",
       " array([3823.67 ,   47.737, 3896.21 , 3800.84 ,   47.508, 3892.24 ,\n",
       "        3778.78 ,   47.148, 3858.9  , 3777.78 ,   47.518, 3834.99 ,\n",
       "        3785.79 ,   47.207, 3844.65 , 3778.32 ,   46.948, 3848.2  ,\n",
       "        3784.54 ,   46.948, 3842.66 ,   46.948, 3850.4  ], dtype=float32),\n",
       " array([3800.84 ,   47.508, 3892.24 , 3778.78 ,   47.148, 3858.9  ,\n",
       "        3777.78 ,   47.518, 3834.99 , 3785.79 ,   47.207, 3844.65 ,\n",
       "        3778.32 ,   46.948, 3848.2  , 3784.54 ,   46.948, 3842.66 ,\n",
       "        3784.58 ,   46.948, 3850.4  ,   46.948, 3847.84 ], dtype=float32),\n",
       " array([3778.78 ,   47.148, 3858.9  , 3777.78 ,   47.518, 3834.99 ,\n",
       "        3785.79 ,   47.207, 3844.65 , 3778.32 ,   46.948, 3848.2  ,\n",
       "        3784.54 ,   46.948, 3842.66 , 3784.58 ,   46.948, 3850.4  ,\n",
       "        3774.68 ,   46.948, 3847.84 ,   46.948, 3847.08 ], dtype=float32),\n",
       " array([3777.78 ,   47.518, 3834.99 , 3785.79 ,   47.207, 3844.65 ,\n",
       "        3778.32 ,   46.948, 3848.2  , 3784.54 ,   46.948, 3842.66 ,\n",
       "        3784.58 ,   46.948, 3850.4  , 3774.68 ,   46.948, 3847.84 ,\n",
       "        3790.85 ,   46.948, 3847.08 ,   46.948, 3859.16 ], dtype=float32),\n",
       " array([3785.79 ,   47.207, 3844.65 , 3778.32 ,   46.948, 3848.2  ,\n",
       "        3784.54 ,   46.948, 3842.66 , 3784.58 ,   46.948, 3850.4  ,\n",
       "        3774.68 ,   46.948, 3847.84 , 3790.85 ,   46.948, 3847.08 ,\n",
       "        3797.05 ,   46.948, 3859.16 ,   46.948, 3862.66 ], dtype=float32),\n",
       " array([3778.32 ,   46.948, 3848.2  , 3784.54 ,   46.948, 3842.66 ,\n",
       "        3784.58 ,   46.948, 3850.4  , 3774.68 ,   46.948, 3847.84 ,\n",
       "        3790.85 ,   46.948, 3847.08 , 3797.05 ,   46.948, 3859.16 ,\n",
       "        3740.31 ,   46.948, 3862.66 ,   46.948, 3804.3  ], dtype=float32),\n",
       " array([3784.54 ,   46.948, 3842.66 , 3784.58 ,   46.948, 3850.4  ,\n",
       "        3774.68 ,   46.948, 3847.84 , 3790.85 ,   46.948, 3847.08 ,\n",
       "        3797.05 ,   46.948, 3859.16 , 3740.31 ,   46.948, 3862.66 ,\n",
       "        3737.53 ,   46.948, 3804.3  ,   46.948, 3817.64 ], dtype=float32),\n",
       " array([3784.58 ,   46.948, 3850.4  , 3774.68 ,   46.948, 3847.84 ,\n",
       "        3790.85 ,   46.948, 3847.08 , 3797.05 ,   46.948, 3859.16 ,\n",
       "        3740.31 ,   46.948, 3862.66 , 3737.53 ,   46.948, 3804.3  ,\n",
       "        3755.94 ,   46.948, 3817.64 ,   46.948, 3813.73 ], dtype=float32),\n",
       " array([3774.68 ,   46.948, 3847.84 , 3790.85 ,   46.948, 3847.08 ,\n",
       "        3797.05 ,   46.948, 3859.16 , 3740.31 ,   46.948, 3862.66 ,\n",
       "        3737.53 ,   46.948, 3804.3  , 3755.94 ,   46.948, 3817.64 ,\n",
       "        3763.6  ,   46.948, 3813.73 ,   46.948, 3846.22 ], dtype=float32),\n",
       " array([3790.85 ,   46.948, 3847.08 , 3797.05 ,   46.948, 3859.16 ,\n",
       "        3740.31 ,   46.948, 3862.66 , 3737.53 ,   46.948, 3804.3  ,\n",
       "        3755.94 ,   46.948, 3817.64 , 3763.6  ,   46.948, 3813.73 ,\n",
       "        3763.61 ,   46.948, 3846.22 ,   46.948, 3827.13 ], dtype=float32),\n",
       " array([3797.05 ,   46.948, 3859.16 , 3740.31 ,   46.948, 3862.66 ,\n",
       "        3737.53 ,   46.948, 3804.3  , 3755.94 ,   46.948, 3817.64 ,\n",
       "        3763.6  ,   46.948, 3813.73 , 3763.61 ,   46.948, 3846.22 ,\n",
       "        3758.69 ,   46.948, 3827.13 ,   46.948, 3817.41 ], dtype=float32),\n",
       " array([3740.31 ,   46.948, 3862.66 , 3737.53 ,   46.948, 3804.3  ,\n",
       "        3755.94 ,   46.948, 3817.64 , 3763.6  ,   46.948, 3813.73 ,\n",
       "        3763.61 ,   46.948, 3846.22 , 3758.69 ,   46.948, 3827.13 ,\n",
       "        3749.88 ,   46.948, 3817.41 ,   46.948, 3808.05 ], dtype=float32),\n",
       " array([3737.53 ,   46.948, 3804.3  , 3755.94 ,   46.948, 3817.64 ,\n",
       "        3763.6  ,   46.948, 3813.73 , 3763.61 ,   46.948, 3846.22 ,\n",
       "        3758.69 ,   46.948, 3827.13 , 3749.88 ,   46.948, 3817.41 ,\n",
       "        3749.13 ,   46.948, 3808.05 ,   46.948, 3812.64 ], dtype=float32),\n",
       " array([3755.94 ,   46.948, 3817.64 , 3763.6  ,   46.948, 3813.73 ,\n",
       "        3763.61 ,   46.948, 3846.22 , 3758.69 ,   46.948, 3827.13 ,\n",
       "        3749.88 ,   46.948, 3817.41 , 3749.13 ,   46.948, 3808.05 ,\n",
       "        3691.78 ,   46.948, 3812.64 ,   46.948, 3751.99 ], dtype=float32),\n",
       " array([3763.6  ,   46.948, 3813.73 , 3763.61 ,   46.948, 3846.22 ,\n",
       "        3758.69 ,   46.948, 3827.13 , 3749.88 ,   46.948, 3817.41 ,\n",
       "        3749.13 ,   46.948, 3808.05 , 3691.78 ,   46.948, 3812.64 ,\n",
       "        3676.9  ,   46.948, 3751.99 ,   46.948, 3739.17 ], dtype=float32),\n",
       " array([3763.61 ,   46.948, 3846.22 , 3758.69 ,   46.948, 3827.13 ,\n",
       "        3749.88 ,   46.948, 3817.41 , 3749.13 ,   46.948, 3808.05 ,\n",
       "        3691.78 ,   46.948, 3812.64 , 3676.9  ,   46.948, 3751.99 ,\n",
       "        3673.36 ,   46.948, 3739.17 ,   46.948, 3734.3  ], dtype=float32),\n",
       " array([3758.69 ,   46.948, 3827.13 , 3749.88 ,   46.948, 3817.41 ,\n",
       "        3749.13 ,   46.948, 3808.05 , 3691.78 ,   46.948, 3812.64 ,\n",
       "        3676.9  ,   46.948, 3751.99 , 3673.36 ,   46.948, 3739.17 ,\n",
       "        3668.26 ,   46.948, 3734.3  ,   46.948, 3743.61 ], dtype=float32),\n",
       " array([3749.88 ,   46.948, 3817.41 , 3749.13 ,   46.948, 3808.05 ,\n",
       "        3691.78 ,   46.948, 3812.64 , 3676.9  ,   46.948, 3751.99 ,\n",
       "        3673.36 ,   46.948, 3739.17 , 3668.26 ,   46.948, 3734.3  ,\n",
       "        3688.22 ,   46.948, 3743.61 ,   46.948, 3747.39 ], dtype=float32),\n",
       " array([3749.13 ,   46.948, 3808.05 , 3691.78 ,   46.948, 3812.64 ,\n",
       "        3676.9  ,   46.948, 3751.99 , 3673.36 ,   46.948, 3739.17 ,\n",
       "        3668.26 ,   46.948, 3734.3  , 3688.22 ,   46.948, 3743.61 ,\n",
       "        3692.65 ,   46.948, 3747.39 ,   46.948, 3750.91 ], dtype=float32),\n",
       " array([3691.78 ,   46.948, 3812.64 , 3676.9  ,   46.948, 3751.99 ,\n",
       "        3673.36 ,   46.948, 3739.17 , 3668.26 ,   46.948, 3734.3  ,\n",
       "        3688.22 ,   46.948, 3743.61 , 3692.65 ,   46.948, 3747.39 ,\n",
       "        3689.44 ,   46.948, 3750.91 ,   46.948, 3736.1  ], dtype=float32),\n",
       " array([3676.9  ,   46.948, 3751.99 , 3673.36 ,   46.948, 3739.17 ,\n",
       "        3668.26 ,   46.948, 3734.3  , 3688.22 ,   46.948, 3743.61 ,\n",
       "        3692.65 ,   46.948, 3747.39 , 3689.44 ,   46.948, 3750.91 ,\n",
       "        3674.89 ,   46.948, 3736.1  ,   46.948, 3728.07 ], dtype=float32),\n",
       " array([3673.36 ,   46.948, 3739.17 , 3668.26 ,   46.948, 3734.3  ,\n",
       "        3688.22 ,   46.948, 3743.61 , 3692.65 ,   46.948, 3747.39 ,\n",
       "        3689.44 ,   46.948, 3750.91 , 3674.89 ,   46.948, 3736.1  ,\n",
       "        3681.66 ,   46.948, 3728.07 ,   46.948, 3734.43 ], dtype=float32),\n",
       " array([3668.26 ,   46.948, 3734.3  , 3688.22 ,   46.948, 3743.61 ,\n",
       "        3692.65 ,   46.948, 3747.39 , 3689.44 ,   46.948, 3750.91 ,\n",
       "        3674.89 ,   46.948, 3736.1  , 3681.66 ,   46.948, 3728.07 ,\n",
       "        3685.26 ,   46.948, 3734.43 ,   46.948, 3733.01 ], dtype=float32),\n",
       " array([3688.22 ,   46.948, 3743.61 , 3692.65 ,   46.948, 3747.39 ,\n",
       "        3689.44 ,   46.948, 3750.91 , 3674.89 ,   46.948, 3736.1  ,\n",
       "        3681.66 ,   46.948, 3728.07 , 3685.26 ,   46.948, 3734.43 ,\n",
       "        3686.13 ,   46.948, 3733.01 ,   46.948, 3739.42 ], dtype=float32),\n",
       " array([3692.65 ,   46.948, 3747.39 , 3689.44 ,   46.948, 3750.91 ,\n",
       "        3674.89 ,   46.948, 3736.1  , 3681.66 ,   46.948, 3728.07 ,\n",
       "        3685.26 ,   46.948, 3734.43 , 3686.13 ,   46.948, 3733.01 ,\n",
       "        3691.39 ,   46.948, 3739.42 ,   46.948, 3740.17 ], dtype=float32),\n",
       " array([3689.44 ,   46.948, 3750.91 , 3674.89 ,   46.948, 3736.1  ,\n",
       "        3681.66 ,   46.948, 3728.07 , 3685.26 ,   46.948, 3734.43 ,\n",
       "        3686.13 ,   46.948, 3733.01 , 3691.39 ,   46.948, 3739.42 ,\n",
       "        3705.49 ,   46.948, 3740.17 ,   46.948, 3753.23 ], dtype=float32),\n",
       " array([3674.89 ,   46.948, 3736.1  , 3681.66 ,   46.948, 3728.07 ,\n",
       "        3685.26 ,   46.948, 3734.43 , 3686.13 ,   46.948, 3733.01 ,\n",
       "        3691.39 ,   46.948, 3739.42 , 3705.49 ,   46.948, 3740.17 ,\n",
       "        3710.58 ,   46.948, 3753.23 ,   46.948, 3763.09 ], dtype=float32),\n",
       " array([3681.66 ,   46.948, 3728.07 , 3685.26 ,   46.948, 3734.43 ,\n",
       "        3686.13 ,   46.948, 3733.01 , 3691.39 ,   46.948, 3739.42 ,\n",
       "        3705.49 ,   46.948, 3740.17 , 3710.58 ,   46.948, 3753.23 ,\n",
       "        3712.35 ,   46.948, 3763.09 ,   46.948, 3744.34 ], dtype=float32),\n",
       " array([3685.26 ,   46.948, 3734.43 , 3686.13 ,   46.948, 3733.01 ,\n",
       "        3691.39 ,   46.948, 3739.42 , 3705.49 ,   46.948, 3740.17 ,\n",
       "        3710.58 ,   46.948, 3753.23 , 3712.35 ,   46.948, 3763.09 ,\n",
       "        3705.15 ,   46.948, 3744.34 ,   46.948, 3756.82 ], dtype=float32),\n",
       " array([3686.13 ,   46.948, 3733.01 , 3691.39 ,   46.948, 3739.42 ,\n",
       "        3705.49 ,   46.948, 3740.17 , 3710.58 ,   46.948, 3753.23 ,\n",
       "        3712.35 ,   46.948, 3763.09 , 3705.15 ,   46.948, 3744.34 ,\n",
       "        3711.83 ,   46.948, 3756.82 ,   46.948, 3751.84 ], dtype=float32),\n",
       " array([3691.39 ,   46.948, 3739.42 , 3705.49 ,   46.948, 3740.17 ,\n",
       "        3710.58 ,   46.948, 3753.23 , 3712.35 ,   46.948, 3763.09 ,\n",
       "        3705.15 ,   46.948, 3744.34 , 3711.83 ,   46.948, 3756.82 ,\n",
       "        3707.94 ,   46.948, 3751.84 ,   46.948, 3766.1  ], dtype=float32),\n",
       " array([3705.49 ,   46.948, 3740.17 , 3710.58 ,   46.948, 3753.23 ,\n",
       "        3712.35 ,   46.948, 3763.09 , 3705.15 ,   46.948, 3744.34 ,\n",
       "        3711.83 ,   46.948, 3756.82 , 3707.94 ,   46.948, 3751.84 ,\n",
       "        3721.98 ,   46.948, 3766.1  ,   46.948, 3772.77 ], dtype=float32),\n",
       " array([3710.58 ,   46.948, 3753.23 , 3712.35 ,   46.948, 3763.09 ,\n",
       "        3705.15 ,   46.948, 3744.34 , 3711.83 ,   46.948, 3756.82 ,\n",
       "        3707.94 ,   46.948, 3751.84 , 3721.98 ,   46.948, 3766.1  ,\n",
       "        3698.2  ,   46.948, 3772.77 ,   46.948, 3746.21 ], dtype=float32),\n",
       " array([3712.35 ,   46.948, 3763.09 , 3705.15 ,   46.948, 3744.34 ,\n",
       "        3711.83 ,   46.948, 3756.82 , 3707.94 ,   46.948, 3751.84 ,\n",
       "        3721.98 ,   46.948, 3766.1  , 3698.2  ,   46.948, 3772.77 ,\n",
       "        3681.05 ,   46.948, 3746.21 ,   46.948, 3730.17 ], dtype=float32),\n",
       " array([3705.15 ,   46.948, 3744.34 , 3711.83 ,   46.948, 3756.82 ,\n",
       "        3707.94 ,   46.948, 3751.84 , 3721.98 ,   46.948, 3766.1  ,\n",
       "        3698.2  ,   46.948, 3772.77 , 3681.05 ,   46.948, 3746.21 ,\n",
       "        3686.57 ,   46.948, 3730.17 ,   46.948, 3741.87 ], dtype=float32),\n",
       " array([3711.83 ,   46.948, 3756.82 , 3707.94 ,   46.948, 3751.84 ,\n",
       "        3721.98 ,   46.948, 3766.1  , 3698.2  ,   46.948, 3772.77 ,\n",
       "        3681.05 ,   46.948, 3746.21 , 3686.57 ,   46.948, 3730.17 ,\n",
       "        3684.37 ,   46.948, 3741.87 ,   46.948, 3739.56 ], dtype=float32),\n",
       " array([3707.94 ,   46.948, 3751.84 , 3721.98 ,   46.948, 3766.1  ,\n",
       "        3698.2  ,   46.948, 3772.77 , 3681.05 ,   46.948, 3746.21 ,\n",
       "        3686.57 ,   46.948, 3730.17 , 3684.37 ,   46.948, 3741.87 ,\n",
       "        3656.92 ,   46.948, 3739.56 ,   46.948, 3711.32 ], dtype=float32),\n",
       " array([3721.98 ,   46.948, 3766.1  , 3698.2  ,   46.948, 3772.77 ,\n",
       "        3681.05 ,   46.948, 3746.21 , 3686.57 ,   46.948, 3730.17 ,\n",
       "        3684.37 ,   46.948, 3741.87 , 3656.92 ,   46.948, 3739.56 ,\n",
       "        3679.79 ,   46.948, 3711.32 ,   46.948, 3749.07 ], dtype=float32),\n",
       " array([3698.2  ,   46.948, 3772.77 , 3681.05 ,   46.948, 3746.21 ,\n",
       "        3686.57 ,   46.948, 3730.17 , 3684.37 ,   46.948, 3741.87 ,\n",
       "        3656.92 ,   46.948, 3739.56 , 3679.79 ,   46.948, 3711.32 ,\n",
       "        3719.2  ,   46.948, 3749.07 ,   46.948, 3786.06 ], dtype=float32),\n",
       " array([3681.05 ,   46.948, 3746.21 , 3686.57 ,   46.948, 3730.17 ,\n",
       "        3684.37 ,   46.948, 3741.87 , 3656.92 ,   46.948, 3739.56 ,\n",
       "        3679.79 ,   46.948, 3711.32 , 3719.2  ,   46.948, 3749.07 ,\n",
       "        3722.25 ,   46.948, 3786.06 ,   48.528, 3779.73 ], dtype=float32),\n",
       " array([3686.57 ,   46.948, 3730.17 , 3684.37 ,   46.948, 3741.87 ,\n",
       "        3656.92 ,   46.948, 3739.56 , 3679.79 ,   46.948, 3711.32 ,\n",
       "        3719.2  ,   46.948, 3749.07 , 3722.25 ,   46.948, 3786.06 ,\n",
       "        3716.28 ,   48.528, 3779.73 ,   48.058, 3781.57 ], dtype=float32),\n",
       " array([3684.37 ,   46.948, 3741.87 , 3656.92 ,   46.948, 3739.56 ,\n",
       "        3679.79 ,   46.948, 3711.32 , 3719.2  ,   46.948, 3749.07 ,\n",
       "        3722.25 ,   46.948, 3786.06 , 3716.28 ,   48.528, 3779.73 ,\n",
       "        3799.81 ,   48.058, 3781.57 ,   39.267, 3880.15 ], dtype=float32),\n",
       " array([3656.92 ,   46.948, 3739.56 , 3679.79 ,   46.948, 3711.32 ,\n",
       "        3719.2  ,   46.948, 3749.07 , 3722.25 ,   46.948, 3786.06 ,\n",
       "        3716.28 ,   48.528, 3779.73 , 3799.81 ,   48.058, 3781.57 ,\n",
       "        3608.85 ,   39.267, 3880.15 ,   39.267, 3860.78 ], dtype=float32),\n",
       " array([3679.79 ,   46.948, 3711.32 , 3719.2  ,   46.948, 3749.07 ,\n",
       "        3722.25 ,   46.948, 3786.06 , 3716.28 ,   48.528, 3779.73 ,\n",
       "        3799.81 ,   48.058, 3781.57 , 3608.85 ,   39.267, 3880.15 ,\n",
       "        3601.73 ,   39.267, 3860.78 ,   39.267, 3851.23 ], dtype=float32),\n",
       " array([3719.2  ,   46.948, 3749.07 , 3722.25 ,   46.948, 3786.06 ,\n",
       "        3716.28 ,   48.528, 3779.73 , 3799.81 ,   48.058, 3781.57 ,\n",
       "        3608.85 ,   39.267, 3880.15 , 3601.73 ,   39.267, 3860.78 ,\n",
       "        3607.9  ,   39.267, 3851.23 ,   39.267, 3861.65 ], dtype=float32),\n",
       " array([3722.25 ,   46.948, 3786.06 , 3716.28 ,   48.528, 3779.73 ,\n",
       "        3799.81 ,   48.058, 3781.57 , 3608.85 ,   39.267, 3880.15 ,\n",
       "        3601.73 ,   39.267, 3860.78 , 3607.9  ,   39.267, 3851.23 ,\n",
       "        3621.95 ,   39.267, 3861.65 ,   39.267, 3860.09 ], dtype=float32),\n",
       " array([3716.28 ,   48.528, 3779.73 , 3799.81 ,   48.058, 3781.57 ,\n",
       "        3608.85 ,   39.267, 3880.15 , 3601.73 ,   39.267, 3860.78 ,\n",
       "        3607.9  ,   39.267, 3851.23 , 3621.95 ,   39.267, 3861.65 ,\n",
       "        3624.16 ,   39.267, 3860.09 ,   39.267, 3861.45 ], dtype=float32),\n",
       " array([3799.81 ,   48.058, 3781.57 , 3608.85 ,   39.267, 3880.15 ,\n",
       "        3601.73 ,   39.267, 3860.78 , 3607.9  ,   39.267, 3851.23 ,\n",
       "        3621.95 ,   39.267, 3861.65 , 3624.16 ,   39.267, 3860.09 ,\n",
       "        3617.73 ,   39.267, 3861.45 ,   39.267, 3858.66 ], dtype=float32),\n",
       " array([3608.85 ,   39.267, 3880.15 , 3601.73 ,   39.267, 3860.78 ,\n",
       "        3607.9  ,   39.267, 3851.23 , 3621.95 ,   39.267, 3861.65 ,\n",
       "        3624.16 ,   39.267, 3860.09 , 3617.73 ,   39.267, 3861.45 ,\n",
       "        3611.61 ,   39.267, 3858.66 ,   39.267, 3840.87 ], dtype=float32),\n",
       " array([3601.73 ,   39.267, 3860.78 , 3607.9  ,   39.267, 3851.23 ,\n",
       "        3621.95 ,   39.267, 3861.65 , 3624.16 ,   39.267, 3860.09 ,\n",
       "        3617.73 ,   39.267, 3861.45 , 3611.61 ,   39.267, 3858.66 ,\n",
       "        3610.88 ,   39.267, 3840.87 ,   39.267, 3853.27 ], dtype=float32),\n",
       " array([3607.9  ,   39.267, 3851.23 , 3621.95 ,   39.267, 3861.65 ,\n",
       "        3624.16 ,   39.267, 3860.09 , 3617.73 ,   39.267, 3861.45 ,\n",
       "        3611.61 ,   39.267, 3858.66 , 3610.88 ,   39.267, 3840.87 ,\n",
       "        3613.49 ,   39.267, 3853.27 ,   39.267, 3861.89 ], dtype=float32),\n",
       " array([3621.95 ,   39.267, 3861.65 , 3624.16 ,   39.267, 3860.09 ,\n",
       "        3617.73 ,   39.267, 3861.45 , 3611.61 ,   39.267, 3858.66 ,\n",
       "        3610.88 ,   39.267, 3840.87 , 3613.49 ,   39.267, 3853.27 ,\n",
       "        3611.17 ,   39.267, 3861.89 ,   39.267, 3852.69 ], dtype=float32),\n",
       " array([3624.16 ,   39.267, 3860.09 , 3617.73 ,   39.267, 3861.45 ,\n",
       "        3611.61 ,   39.267, 3858.66 , 3610.88 ,   39.267, 3840.87 ,\n",
       "        3613.49 ,   39.267, 3853.27 , 3611.17 ,   39.267, 3861.89 ,\n",
       "        3613.84 ,   39.267, 3852.69 ,   39.267, 3870.35 ], dtype=float32),\n",
       " array([3617.73 ,   39.267, 3861.45 , 3611.61 ,   39.267, 3858.66 ,\n",
       "        3610.88 ,   39.267, 3840.87 , 3613.49 ,   39.267, 3853.27 ,\n",
       "        3611.17 ,   39.267, 3861.89 , 3613.84 ,   39.267, 3852.69 ,\n",
       "        3615.69 ,   39.267, 3870.35 ,   39.267, 3872.64 ], dtype=float32),\n",
       " array([3611.61 ,   39.267, 3858.66 , 3610.88 ,   39.267, 3840.87 ,\n",
       "        3613.49 ,   39.267, 3853.27 , 3611.17 ,   39.267, 3861.89 ,\n",
       "        3613.84 ,   39.267, 3852.69 , 3615.69 ,   39.267, 3870.35 ,\n",
       "        3618.45 ,   39.267, 3872.64 ,   39.267, 3874.46 ], dtype=float32),\n",
       " array([3610.88 ,   39.267, 3840.87 , 3613.49 ,   39.267, 3853.27 ,\n",
       "        3611.17 ,   39.267, 3861.89 , 3613.84 ,   39.267, 3852.69 ,\n",
       "        3615.69 ,   39.267, 3870.35 , 3618.45 ,   39.267, 3872.64 ,\n",
       "        3619.13 ,   39.267, 3874.46 ,   39.267, 3910.71 ], dtype=float32)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
