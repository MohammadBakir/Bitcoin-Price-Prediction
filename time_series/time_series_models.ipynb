{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import math\n",
    "\n",
    "# import pyflux as pf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM\n",
    "#from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2019)\n",
    "set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('./processed_data/df_combined.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>vix</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08 00:00:00</td>\n",
       "      <td>1054.03</td>\n",
       "      <td>18.879</td>\n",
       "      <td>2732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-08 01:00:00</td>\n",
       "      <td>1060.48</td>\n",
       "      <td>18.915</td>\n",
       "      <td>2693.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds        y     vix    gold\n",
       "0 2017-02-08 00:00:00  1054.03  18.879  2732.0\n",
       "1 2017-02-08 01:00:00  1060.48  18.915  2693.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_chronological(df, ratio = 0.9, use_ratio = True, index = 1000):\n",
    "    \n",
    "    '''\n",
    "    Input is a dataframe, and a ratio. Splits dataframe into 2 dataframes chronologically.\n",
    "    Returns first dataframe up to the index of the length of the input dataframe times the input ratio, \n",
    "    and returns second dataframe of remaining elements.\n",
    "    use_ratio is a flag, wether ratio should be used or indicies instead.\n",
    "    \n",
    "    df = input dataframe\n",
    "    ratio = ratio to be used for splitting\n",
    "    use_ratio = if True, use ratio, \n",
    "    index = index to split input dataframe on\n",
    "    \n",
    "    '''\n",
    "    if use_ratio:\n",
    "        size = len(df) * ratio\n",
    "        size_round = round(size)\n",
    "\n",
    "        df_train = df[0:(size_round)]\n",
    "        df_test = df[size_round:]\n",
    "    else:\n",
    "        df_train = df[0:(index)]\n",
    "        df_test = df[index:]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(train_perc, stocks_to_trade, return_df_2):\n",
    "    train_len = int(return_df.shape[0] * (train_perc))\n",
    "    volume_to_trade = ['v_'+ticker for ticker in stocks_to_trade]\n",
    "    for i in volume_to_trade:\n",
    "        stocks_to_trade.append(i)\n",
    "\n",
    "    train = return_df_2[stocks_to_trade][1:train_len].copy()\n",
    "    train['diff'] = train[stocks_to_trade[0]] - train[stocks_to_trade[1]]\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    test = return_df_2[stocks_to_trade][train_len:-400].copy()\n",
    "    test['diff'] = test[stocks_to_trade[0]] - test[stocks_to_trade[1]]\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    true_test = return_df_2[stocks_to_trade][-400:].copy()\n",
    "    true_test['diff'] = true_test[stocks_to_trade[0]] - true_test[stocks_to_trade[1]]\n",
    "    true_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    feature_names = volume_to_trade\n",
    "    feature_names.append('diff')\n",
    "    \n",
    "    return train, test, true_test, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(df, lags, look_back, predict_window):\n",
    "    \n",
    "    '''\n",
    "    In order to use keras LSTM, we need to convert the input into a keras-friendly input.\n",
    "    \n",
    "    df = input dataframe\n",
    "    lags = number of lags\n",
    "    look_back = number of preceding elements to be considered\n",
    "    predict_window = size of window for predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    features = lags + 1\n",
    "    start = look_back\n",
    "    stop = len(df) - lags - predict_window\n",
    "\n",
    "    lstm_in_X = np.zeros(shape=(stop-start, look_back+1, features))\n",
    "    lstm_in_Y = np.zeros(shape=(stop-start, look_back+1))\n",
    "\n",
    "    iter_list = [num for num in range(look_back+1)][::-1]\n",
    "    for i in range(start, stop):\n",
    "        for index, j in enumerate(iter_list):\n",
    "            X = df[i - j : i - j + lags + 1, -1]\n",
    "            lstm_in_X[i - start, index] = np.ravel(X)\n",
    "            Y = df[i - j + lags + 1, -1]\n",
    "            lstm_in_Y[i-start, index] = Y\n",
    "            \n",
    "    return lstm_in_X, lstm_in_Y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(train_X, train_Y, lags, look_back, predict_window, lstm_nodes,\n",
    "               dense_layers, dropout = 0.1, loss_type = 'hinge', optimizer_type = 'adam',\n",
    "               number_epochs = 300, batch_size = 64, ):\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_nodes, input_shape=(look_back+1, features)))\n",
    "    model.add(Dropout(dropout))\n",
    "    for nodes in dense_layers:\n",
    "        model.add(Dense(nodes))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(look_back + 1))\n",
    "    model.compile(loss=loss_type, optimizer= optimizer_type)\n",
    "    model.fit(train_X, train_Y, epochs=number_epochs, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "    pred_Y_train = model.predict(train_X)\n",
    "    predictions = pred_Y_train[:,-1]\n",
    "    actuals = train_Y[:,-1]\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model, dataset, train_X, train_Y, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model, train_x, train_y, test_x, test_y):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(test_x[i].reshape(1,1,23), (test_y[i].reshape(1 ,))),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        train_x = np.concatenate((train_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "        le = len(test_y) + 1\n",
    "        train_y = np.concatenate((train_y, (test_y[i].reshape(1 ,))))\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_x[-1,-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     predictions_test = []\n",
    "#     actuals_test = []\n",
    "    \n",
    "#     for i in range(0, len(test_y) - 1):\n",
    "#         model.fit(train_x, train_y, \n",
    "#                     epochs= 2, \n",
    "#                     batch_size = 128, \n",
    "# #                     validation_data=(test_x[i], test_y[i]),\n",
    "#                     verbose=2,\n",
    "# #                     callbacks=[earlystopper],\n",
    "#                     shuffle=False)\n",
    "#         pred_Y_test = model.predict(test_x)\n",
    "#         test_x = np.concatenate((test_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "#         le = len(test_y) + 1\n",
    "#         test_y = np.concatenate((test_y, (test_y[i].reshape(1 ,))))\n",
    "#         predict_test = pred_Y_test[-1,-1]\n",
    "#         actual_test = train_x[-1,-1]\n",
    "#         predictions_test.append(predict_test)\n",
    "#         actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "#     return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = split_train_test_chronological(df_combined, ratio = .99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = split_train_test_chronological(df_combined, ratio = .95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var2(t-7)</th>\n",
       "      <th>var3(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var2(t-6)</th>\n",
       "      <th>var3(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var2(t-5)</th>\n",
       "      <th>var3(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>var3(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var2(t-2)</th>\n",
       "      <th>var3(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1054.030029</td>\n",
       "      <td>18.879000</td>\n",
       "      <td>2732.000000</td>\n",
       "      <td>1060.479980</td>\n",
       "      <td>18.915001</td>\n",
       "      <td>2693.000000</td>\n",
       "      <td>1062.939941</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>2712.120117</td>\n",
       "      <td>1065.819946</td>\n",
       "      <td>...</td>\n",
       "      <td>2742.100098</td>\n",
       "      <td>1066.310059</td>\n",
       "      <td>18.878000</td>\n",
       "      <td>2735.270020</td>\n",
       "      <td>1066.060059</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2732.120117</td>\n",
       "      <td>1066.260010</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2731.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1060.479980</td>\n",
       "      <td>18.915001</td>\n",
       "      <td>2693.000000</td>\n",
       "      <td>1062.939941</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>2712.120117</td>\n",
       "      <td>1065.819946</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>2743.989990</td>\n",
       "      <td>1068.150024</td>\n",
       "      <td>...</td>\n",
       "      <td>2735.270020</td>\n",
       "      <td>1066.060059</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2732.120117</td>\n",
       "      <td>1066.260010</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2731.959961</td>\n",
       "      <td>1033.900024</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2710.209961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1062.939941</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>2712.120117</td>\n",
       "      <td>1065.819946</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>2743.989990</td>\n",
       "      <td>1068.150024</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>2742.100098</td>\n",
       "      <td>1066.310059</td>\n",
       "      <td>...</td>\n",
       "      <td>2732.120117</td>\n",
       "      <td>1066.260010</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2731.959961</td>\n",
       "      <td>1033.900024</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2710.209961</td>\n",
       "      <td>1032.180054</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2719.310059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1065.819946</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>2743.989990</td>\n",
       "      <td>1068.150024</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>2742.100098</td>\n",
       "      <td>1066.310059</td>\n",
       "      <td>18.878000</td>\n",
       "      <td>2735.270020</td>\n",
       "      <td>1066.060059</td>\n",
       "      <td>...</td>\n",
       "      <td>2731.959961</td>\n",
       "      <td>1033.900024</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2710.209961</td>\n",
       "      <td>1032.180054</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2719.310059</td>\n",
       "      <td>1037.780029</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2732.989990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1068.150024</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>2742.100098</td>\n",
       "      <td>1066.310059</td>\n",
       "      <td>18.878000</td>\n",
       "      <td>2735.270020</td>\n",
       "      <td>1066.060059</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2732.120117</td>\n",
       "      <td>1066.260010</td>\n",
       "      <td>...</td>\n",
       "      <td>2710.209961</td>\n",
       "      <td>1032.180054</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2719.310059</td>\n",
       "      <td>1037.780029</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2732.989990</td>\n",
       "      <td>1045.300049</td>\n",
       "      <td>18.886999</td>\n",
       "      <td>2719.979980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var1(t-7)  var2(t-7)    var3(t-7)    var1(t-6)  var2(t-6)    var3(t-6)  \\\n",
       "7   1054.030029  18.879000  2732.000000  1060.479980  18.915001  2693.000000   \n",
       "8   1060.479980  18.915001  2693.000000  1062.939941  18.860001  2712.120117   \n",
       "9   1062.939941  18.860001  2712.120117  1065.819946  18.770000  2743.989990   \n",
       "10  1065.819946  18.770000  2743.989990  1068.150024  18.770000  2742.100098   \n",
       "11  1068.150024  18.770000  2742.100098  1066.310059  18.878000  2735.270020   \n",
       "\n",
       "      var1(t-5)  var2(t-5)    var3(t-5)    var1(t-4)  ...    var3(t-3)  \\\n",
       "7   1062.939941  18.860001  2712.120117  1065.819946  ...  2742.100098   \n",
       "8   1065.819946  18.770000  2743.989990  1068.150024  ...  2735.270020   \n",
       "9   1068.150024  18.770000  2742.100098  1066.310059  ...  2732.120117   \n",
       "10  1066.310059  18.878000  2735.270020  1066.060059  ...  2731.959961   \n",
       "11  1066.060059  18.886999  2732.120117  1066.260010  ...  2710.209961   \n",
       "\n",
       "      var1(t-2)  var2(t-2)    var3(t-2)    var1(t-1)  var2(t-1)    var3(t-1)  \\\n",
       "7   1066.310059  18.878000  2735.270020  1066.060059  18.886999  2732.120117   \n",
       "8   1066.060059  18.886999  2732.120117  1066.260010  18.886999  2731.959961   \n",
       "9   1066.260010  18.886999  2731.959961  1033.900024  18.886999  2710.209961   \n",
       "10  1033.900024  18.886999  2710.209961  1032.180054  18.886999  2719.310059   \n",
       "11  1032.180054  18.886999  2719.310059  1037.780029  18.886999  2732.989990   \n",
       "\n",
       "        var1(t)    var2(t)      var3(t)  \n",
       "7   1066.260010  18.886999  2731.959961  \n",
       "8   1033.900024  18.886999  2710.209961  \n",
       "9   1032.180054  18.886999  2719.310059  \n",
       "10  1037.780029  18.886999  2732.989990  \n",
       "11  1045.300049  18.886999  2719.979980  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "values = df_combined.drop('ds', axis = 1).values\n",
    "# integer encode direction\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 7, 1)\n",
    "\n",
    "reframed_2 = series_to_supervised(values, 7, 1)\n",
    "reframed_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed.iloc[:,-12].values\n",
    "X = reframed.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00600698, 0.00590796, 0.00589451, ..., 0.40667784, 0.4065094 ,\n",
       "       0.40857488], dtype=float32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed_2.iloc[:,-12].values\n",
    "X = reframed_2.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1068.15, 1066.31, 1066.06, ..., 8513.3 , 8510.17, 8548.55],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    y                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_3 = Sequential()\n",
    "LSTM_model_3.add(LSTM(64, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), dropout=0.05,recurrent_dropout=0.05))\n",
    "LSTM_model_3.add(Dense(64))\n",
    "# LSTM_model_3.add(LSTM(16, activation='relu'))\n",
    "LSTM_model_3.add(Dense(1))\n",
    "LSTM_model_3.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 341 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 2131161.6319 - val_loss: 6574.5435\n",
      "Epoch 2/100\n",
      " - 2s - loss: 377113.0051 - val_loss: 11044.5854\n",
      "Epoch 3/100\n",
      " - 2s - loss: 219567.1301 - val_loss: 26958.1064\n",
      "Epoch 4/100\n",
      " - 2s - loss: 177795.6453 - val_loss: 31925.8409\n",
      "Epoch 5/100\n",
      " - 2s - loss: 160762.0327 - val_loss: 30766.4993\n",
      "Epoch 6/100\n",
      " - 2s - loss: 149847.8289 - val_loss: 21392.1235\n",
      "Epoch 7/100\n",
      " - 2s - loss: 106559.2351 - val_loss: 33635.2081\n",
      "Epoch 8/100\n",
      " - 2s - loss: 107983.6157 - val_loss: 25020.7165\n",
      "Epoch 9/100\n",
      " - 2s - loss: 91090.8682 - val_loss: 23595.8687\n",
      "Epoch 10/100\n",
      " - 2s - loss: 96273.6178 - val_loss: 36549.5719\n",
      "Epoch 11/100\n",
      " - 2s - loss: 83795.3022 - val_loss: 29750.4288\n",
      "Epoch 12/100\n",
      " - 2s - loss: 86294.4862 - val_loss: 29835.7802\n",
      "Epoch 13/100\n",
      " - 2s - loss: 87315.1079 - val_loss: 31986.2374\n",
      "Epoch 14/100\n",
      " - 2s - loss: 84525.1099 - val_loss: 26247.5456\n",
      "Epoch 15/100\n",
      " - 2s - loss: 70029.3732 - val_loss: 27757.9982\n",
      "Epoch 16/100\n",
      " - 2s - loss: 69126.2824 - val_loss: 27284.5412\n",
      "Epoch 17/100\n",
      " - 2s - loss: 73281.9481 - val_loss: 29670.2926\n",
      "Epoch 18/100\n",
      " - 2s - loss: 63821.2307 - val_loss: 25237.3840\n",
      "Epoch 19/100\n",
      " - 2s - loss: 71456.5861 - val_loss: 24670.9599\n",
      "Epoch 20/100\n",
      " - 2s - loss: 71295.0631 - val_loss: 24426.7077\n",
      "Epoch 21/100\n",
      " - 2s - loss: 61582.9117 - val_loss: 26760.4049\n",
      "Epoch 22/100\n",
      " - 2s - loss: 63635.7663 - val_loss: 31046.7892\n",
      "Epoch 23/100\n",
      " - 2s - loss: 61297.0095 - val_loss: 29263.0412\n",
      "Epoch 24/100\n",
      " - 2s - loss: 56700.7204 - val_loss: 24890.3494\n",
      "Epoch 25/100\n",
      " - 2s - loss: 56216.3965 - val_loss: 29051.4959\n",
      "Epoch 26/100\n",
      " - 2s - loss: 59338.4614 - val_loss: 35485.3566\n",
      "Epoch 27/100\n",
      " - 2s - loss: 47590.7225 - val_loss: 28109.2302\n",
      "Epoch 28/100\n",
      " - 2s - loss: 45403.8872 - val_loss: 34452.5692\n",
      "Epoch 29/100\n",
      " - 2s - loss: 48167.9811 - val_loss: 26051.2803\n",
      "Epoch 30/100\n",
      " - 2s - loss: 45218.1399 - val_loss: 29302.6745\n",
      "Epoch 31/100\n",
      " - 2s - loss: 50480.9048 - val_loss: 32486.2221\n",
      "Epoch 32/100\n",
      " - 2s - loss: 45627.9270 - val_loss: 31462.3050\n",
      "Epoch 33/100\n",
      " - 2s - loss: 46087.3173 - val_loss: 34044.6056\n",
      "Epoch 34/100\n",
      " - 2s - loss: 39721.8943 - val_loss: 31370.0425\n",
      "Epoch 35/100\n",
      " - 2s - loss: 50808.2260 - val_loss: 28648.4026\n",
      "Epoch 36/100\n",
      " - 2s - loss: 39434.8151 - val_loss: 29372.7876\n",
      "Epoch 37/100\n",
      " - 2s - loss: 36930.0786 - val_loss: 33879.8900\n",
      "Epoch 38/100\n",
      " - 2s - loss: 35323.1216 - val_loss: 33623.3924\n",
      "Epoch 39/100\n",
      " - 2s - loss: 33825.8219 - val_loss: 39785.6016\n",
      "Epoch 40/100\n",
      " - 2s - loss: 31574.0273 - val_loss: 39115.3422\n",
      "Epoch 41/100\n",
      " - 2s - loss: 37131.7643 - val_loss: 43885.1779\n",
      "Epoch 42/100\n",
      " - 2s - loss: 37420.4605 - val_loss: 43391.4046\n",
      "Epoch 43/100\n",
      " - 2s - loss: 39519.4098 - val_loss: 39496.7256\n",
      "Epoch 44/100\n",
      " - 2s - loss: 30848.9535 - val_loss: 34412.1541\n",
      "Epoch 45/100\n",
      " - 2s - loss: 29556.4162 - val_loss: 37834.1370\n",
      "Epoch 46/100\n",
      " - 2s - loss: 28382.7884 - val_loss: 40658.5432\n",
      "Epoch 47/100\n",
      " - 2s - loss: 30369.1419 - val_loss: 36181.2650\n",
      "Epoch 48/100\n",
      " - 2s - loss: 34920.6952 - val_loss: 40951.3829\n",
      "Epoch 49/100\n",
      " - 2s - loss: 29520.5085 - val_loss: 36816.6010\n",
      "Epoch 50/100\n",
      " - 2s - loss: 28529.3518 - val_loss: 37071.9564\n",
      "Epoch 51/100\n",
      " - 2s - loss: 28429.2035 - val_loss: 40376.8757\n",
      "Epoch 52/100\n",
      " - 2s - loss: 24880.1794 - val_loss: 42100.2725\n",
      "Epoch 53/100\n",
      " - 2s - loss: 23212.8470 - val_loss: 41367.7561\n",
      "Epoch 54/100\n",
      " - 2s - loss: 21187.7031 - val_loss: 42671.5662\n",
      "Epoch 55/100\n",
      " - 2s - loss: 25635.2844 - val_loss: 39821.7163\n",
      "Epoch 56/100\n",
      " - 2s - loss: 21969.2835 - val_loss: 37314.5637\n",
      "Epoch 57/100\n",
      " - 2s - loss: 20104.3707 - val_loss: 40055.0708\n",
      "Epoch 58/100\n",
      " - 2s - loss: 26644.3124 - val_loss: 35232.3474\n",
      "Epoch 59/100\n",
      " - 2s - loss: 30866.4572 - val_loss: 44900.5298\n",
      "Epoch 60/100\n",
      " - 2s - loss: 24548.5912 - val_loss: 45640.2133\n",
      "Epoch 61/100\n",
      " - 2s - loss: 21385.8788 - val_loss: 43237.3693\n",
      "Epoch 62/100\n",
      " - 2s - loss: 20917.3958 - val_loss: 39309.6937\n",
      "Epoch 63/100\n",
      " - 2s - loss: 20776.3256 - val_loss: 39185.6562\n",
      "Epoch 64/100\n",
      " - 2s - loss: 22604.1160 - val_loss: 45007.8676\n",
      "Epoch 65/100\n",
      " - 2s - loss: 23983.7490 - val_loss: 41318.1867\n",
      "Epoch 66/100\n",
      " - 2s - loss: 18142.4890 - val_loss: 39693.8576\n",
      "Epoch 67/100\n",
      " - 2s - loss: 20369.7417 - val_loss: 41258.8144\n",
      "Epoch 68/100\n",
      " - 2s - loss: 17122.8597 - val_loss: 39030.5548\n",
      "Epoch 69/100\n",
      " - 2s - loss: 21476.4774 - val_loss: 40849.2367\n",
      "Epoch 70/100\n",
      " - 2s - loss: 18081.8976 - val_loss: 41308.5122\n",
      "Epoch 71/100\n",
      " - 2s - loss: 22914.3104 - val_loss: 45201.7017\n",
      "Epoch 72/100\n",
      " - 2s - loss: 20999.8791 - val_loss: 41364.8167\n",
      "Epoch 73/100\n",
      " - 2s - loss: 19672.2191 - val_loss: 37700.2972\n",
      "Epoch 74/100\n",
      " - 2s - loss: 16432.2596 - val_loss: 42255.4904\n",
      "Epoch 75/100\n",
      " - 2s - loss: 17525.7347 - val_loss: 41001.8334\n",
      "Epoch 76/100\n",
      " - 2s - loss: 20804.0716 - val_loss: 40047.4365\n",
      "Epoch 77/100\n",
      " - 2s - loss: 13186.8886 - val_loss: 42889.6219\n",
      "Epoch 78/100\n",
      " - 2s - loss: 17718.5968 - val_loss: 40523.5063\n",
      "Epoch 79/100\n",
      " - 2s - loss: 14848.8212 - val_loss: 38216.7345\n",
      "Epoch 80/100\n",
      " - 2s - loss: 19445.8051 - val_loss: 39765.4032\n",
      "Epoch 81/100\n",
      " - 2s - loss: 17993.6542 - val_loss: 41362.7852\n",
      "Epoch 82/100\n",
      " - 2s - loss: 13562.4038 - val_loss: 41942.8266\n",
      "Epoch 83/100\n",
      " - 2s - loss: 14967.7930 - val_loss: 37705.8024\n",
      "Epoch 84/100\n",
      " - 2s - loss: 21435.3707 - val_loss: 37177.4110\n",
      "Epoch 85/100\n",
      " - 2s - loss: 13179.7831 - val_loss: 40825.5503\n",
      "Epoch 86/100\n",
      " - 2s - loss: 13108.8391 - val_loss: 39156.8555\n",
      "Epoch 87/100\n",
      " - 2s - loss: 14398.7646 - val_loss: 38959.9914\n",
      "Epoch 88/100\n",
      " - 2s - loss: 14326.9843 - val_loss: 37966.6247\n",
      "Epoch 89/100\n",
      " - 2s - loss: 21745.4074 - val_loss: 42771.9813\n",
      "Epoch 90/100\n",
      " - 2s - loss: 12670.4117 - val_loss: 42266.2699\n",
      "Epoch 91/100\n",
      " - 2s - loss: 15039.9231 - val_loss: 39881.5351\n",
      "Epoch 92/100\n",
      " - 2s - loss: 15931.7008 - val_loss: 39442.3837\n",
      "Epoch 93/100\n",
      " - 2s - loss: 14700.0299 - val_loss: 38846.8958\n",
      "Epoch 94/100\n",
      " - 2s - loss: 13661.5305 - val_loss: 38230.5811\n",
      "Epoch 95/100\n",
      " - 2s - loss: 12762.7697 - val_loss: 37722.2118\n",
      "Epoch 96/100\n",
      " - 2s - loss: 15089.7516 - val_loss: 40056.5511\n",
      "Epoch 97/100\n",
      " - 2s - loss: 13343.5371 - val_loss: 35320.5689\n",
      "Epoch 98/100\n",
      " - 2s - loss: 12018.3785 - val_loss: 39802.6324\n",
      "Epoch 99/100\n",
      " - 2s - loss: 11691.2393 - val_loss: 37405.3811\n",
      "Epoch 100/100\n",
      " - 2s - loss: 10400.2424 - val_loss: 41092.5170\n"
     ]
    }
   ],
   "source": [
    "history_3 = LSTM_model_3.fit(train_X, train_y, \n",
    "                    epochs= 100, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(val_X, val_y),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model, train_x, train_y, test_x, test_y):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(test_x[i].reshape(1,1,23), (test_y[i].reshape(1 ,))),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        train_x = np.concatenate((train_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "        le = len(test_y) + 1\n",
    "        train_y = np.concatenate((train_y, (test_y[i].reshape(1 ,))))\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_y[-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 14370.6271 - val_loss: 30261.0488\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10952.5091 - val_loss: 27552.6777\n",
      "Train on 12001 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 11907.7510 - val_loss: 40678.8320\n",
      "Epoch 2/2\n",
      " - 2s - loss: 19194.2698 - val_loss: 37752.6992\n",
      "Train on 12002 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 12017.1417 - val_loss: 36747.5625\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10322.0354 - val_loss: 36797.5625\n",
      "Train on 12003 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 11332.9716 - val_loss: 44589.5391\n",
      "Epoch 2/2\n",
      " - 2s - loss: 11150.1828 - val_loss: 48188.5039\n",
      "Train on 12004 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9527.5860 - val_loss: 43571.3633\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10170.1304 - val_loss: 42747.1797\n",
      "Train on 12005 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 15908.2301 - val_loss: 71471.5078\n",
      "Epoch 2/2\n",
      " - 2s - loss: 9895.4899 - val_loss: 68366.2891\n",
      "Train on 12006 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 11670.1358 - val_loss: 41664.3242\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8945.1921 - val_loss: 41245.5820\n",
      "Train on 12007 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 8977.7143 - val_loss: 24794.5625\n",
      "Epoch 2/2\n",
      " - 2s - loss: 11563.9307 - val_loss: 24635.9688\n",
      "Train on 12008 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9778.3598 - val_loss: 32933.5664\n",
      "Epoch 2/2\n",
      " - 2s - loss: 50120.6987 - val_loss: 36349.4336\n",
      "Train on 12009 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7979.5287 - val_loss: 39856.8672\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6530.7952 - val_loss: 39910.0117\n",
      "Train on 12010 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7681.4023 - val_loss: 38616.0898\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8535.9571 - val_loss: 39758.9570\n",
      "Train on 12011 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7624.1096 - val_loss: 43369.6914\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7493.2579 - val_loss: 44492.6719\n",
      "Train on 12012 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 8317.3796 - val_loss: 30168.7891\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6845.9758 - val_loss: 29365.5410\n",
      "Train on 12013 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7557.9872 - val_loss: 21036.4004\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6408.2529 - val_loss: 23280.0098\n",
      "Train on 12014 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7292.2494 - val_loss: 31270.3438\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10188.3515 - val_loss: 31125.4531\n",
      "Train on 12015 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6990.2370 - val_loss: 34879.9414\n",
      "Epoch 2/2\n",
      " - 2s - loss: 11596.7157 - val_loss: 35846.7109\n",
      "Train on 12016 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 10189.7677 - val_loss: 32759.4980\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7828.7337 - val_loss: 33830.5781\n",
      "Train on 12017 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 12714.3486 - val_loss: 37131.4844\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7658.3809 - val_loss: 39225.6602\n",
      "Train on 12018 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 16568.7886 - val_loss: 40389.2227\n",
      "Epoch 2/2\n",
      " - 2s - loss: 9060.3628 - val_loss: 40017.2852\n",
      "Train on 12019 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6726.8614 - val_loss: 34354.8398\n",
      "Epoch 2/2\n",
      " - 2s - loss: 12873.4334 - val_loss: 31629.0078\n",
      "Train on 12020 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4744.2636 - val_loss: 36481.0938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6793.9696 - val_loss: 35177.4023\n",
      "Train on 12021 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4250.8139 - val_loss: 33729.3477\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8210.6694 - val_loss: 34514.5820\n",
      "Train on 12022 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 22000.7010 - val_loss: 31404.0625\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5042.7176 - val_loss: 30201.4512\n",
      "Train on 12023 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6167.2341 - val_loss: 19080.4707\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5406.9183 - val_loss: 19890.0898\n",
      "Train on 12024 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4062.9990 - val_loss: 71187.7344\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5816.6452 - val_loss: 68657.4297\n",
      "Train on 12025 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4619.5551 - val_loss: 77755.1953\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8147.7844 - val_loss: 76404.3281\n",
      "Train on 12026 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9039.9105 - val_loss: 55047.6289\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5400.2160 - val_loss: 50799.8320\n",
      "Train on 12027 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3969.0707 - val_loss: 39062.5898\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6457.2847 - val_loss: 39716.7148\n",
      "Train on 12028 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5884.0659 - val_loss: 45733.9531\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5964.7449 - val_loss: 47569.2500\n",
      "Train on 12029 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5670.3008 - val_loss: 37144.9414\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7276.3821 - val_loss: 34828.8906\n",
      "Train on 12030 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5779.6142 - val_loss: 38299.5234\n",
      "Epoch 2/2\n",
      " - 2s - loss: 14923.4979 - val_loss: 38448.8320\n",
      "Train on 12031 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6637.3609 - val_loss: 38135.5312\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6344.3885 - val_loss: 40602.7422\n",
      "Train on 12032 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4571.8846 - val_loss: 49988.0508\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7842.9823 - val_loss: 47001.7305\n",
      "Train on 12033 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4804.9550 - val_loss: 47382.5312\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5141.3091 - val_loss: 48442.5469\n",
      "Train on 12034 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3900.7411 - val_loss: 36890.5352\n",
      "Epoch 2/2\n",
      " - 2s - loss: 12589.7966 - val_loss: 35205.8906\n",
      "Train on 12035 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6041.0189 - val_loss: 45258.8242\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4511.5326 - val_loss: 47151.3242\n",
      "Train on 12036 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4992.4318 - val_loss: 22959.2773\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5093.3860 - val_loss: 22925.7754\n",
      "Train on 12037 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4952.1187 - val_loss: 24355.8848\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7384.5864 - val_loss: 26715.4883\n",
      "Train on 12038 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3950.5586 - val_loss: 54556.5742\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3762.6547 - val_loss: 54396.1094\n",
      "Train on 12039 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3118.1710 - val_loss: 53448.9023\n",
      "Epoch 2/2\n",
      " - 2s - loss: 13726.4764 - val_loss: 55990.0039\n",
      "Train on 12040 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3983.5351 - val_loss: 33136.6211\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8546.4139 - val_loss: 36149.3711\n",
      "Train on 12041 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4055.2798 - val_loss: 49319.4531\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5648.4257 - val_loss: 54000.5234\n",
      "Train on 12042 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3261.2003 - val_loss: 45274.0938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4035.7110 - val_loss: 41987.6719\n",
      "Train on 12043 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6242.9132 - val_loss: 33021.5273\n",
      "Epoch 2/2\n",
      " - 2s - loss: 13131.7830 - val_loss: 33606.2461\n",
      "Train on 12044 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3606.6029 - val_loss: 47615.6953\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5789.2796 - val_loss: 47180.2734\n",
      "Train on 12045 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3247.8537 - val_loss: 25180.0176\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5277.0002 - val_loss: 24561.6074\n",
      "Train on 12046 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6246.2348 - val_loss: 38611.0977\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10864.4097 - val_loss: 40295.6602\n",
      "Train on 12047 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4810.0579 - val_loss: 39884.1641\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3560.9523 - val_loss: 41827.6406\n",
      "Train on 12048 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3893.2523 - val_loss: 51415.2305\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7102.2487 - val_loss: 48122.5000\n",
      "Train on 12049 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3615.4527 - val_loss: 37412.5898\n",
      "Epoch 2/2\n",
      " - 2s - loss: 13854.2550 - val_loss: 39622.6719\n",
      "Train on 12050 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3135.5785 - val_loss: 32105.8848\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3836.8299 - val_loss: 32446.3340\n",
      "Train on 12051 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 2445.1336 - val_loss: 41264.4258\n",
      "Epoch 2/2\n",
      " - 2s - loss: 15787.8205 - val_loss: 41077.5703\n",
      "Train on 12052 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4092.9548 - val_loss: 44912.2266\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2875.4755 - val_loss: 43987.1758\n",
      "Train on 12053 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4033.9776 - val_loss: 41535.9531\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2569.1348 - val_loss: 44244.8008\n",
      "Train on 12054 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6610.1016 - val_loss: 42286.6602\n",
      "Epoch 2/2\n",
      " - 2s - loss: 20534.6023 - val_loss: 46212.6133\n",
      "Train on 12055 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3877.2916 - val_loss: 35532.2500\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2872.2407 - val_loss: 37100.1602\n",
      "Train on 12056 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2871.9995 - val_loss: 35244.9297\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3318.5837 - val_loss: 36547.6211\n",
      "Train on 12057 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7558.5625 - val_loss: 40490.6562\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2566.4580 - val_loss: 40004.8828\n",
      "Train on 12058 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2698.8348 - val_loss: 38748.7109\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2484.0597 - val_loss: 38117.0312\n",
      "Train on 12059 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2647.2010 - val_loss: 34052.8633\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4261.2110 - val_loss: 32060.4062\n",
      "Train on 12060 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6765.4289 - val_loss: 28403.3574\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3689.6965 - val_loss: 27074.7090\n",
      "Train on 12061 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3677.4652 - val_loss: 34570.4844\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5812.6511 - val_loss: 37336.0312\n",
      "Train on 12062 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3237.1080 - val_loss: 36371.5938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 35347.0823 - val_loss: 33870.9141\n",
      "Train on 12063 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3399.5508 - val_loss: 36121.4336\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2301.5477 - val_loss: 35910.1562\n",
      "Train on 12064 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3085.6232 - val_loss: 40865.9648\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3059.4583 - val_loss: 40719.3203\n",
      "Train on 12065 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2693.7956 - val_loss: 33755.4492\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2348.4634 - val_loss: 33856.1797\n",
      "Train on 12066 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2594.8964 - val_loss: 36936.6914\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3051.7881 - val_loss: 36762.9141\n",
      "Train on 12067 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3245.3624 - val_loss: 39629.9609\n",
      "Epoch 2/2\n",
      " - 2s - loss: 17565.4068 - val_loss: 38602.5586\n",
      "Train on 12068 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4043.7730 - val_loss: 34921.9922\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2711.7257 - val_loss: 35077.1016\n",
      "Train on 12069 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7339.5698 - val_loss: 32436.9238\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3413.1340 - val_loss: 32038.6406\n",
      "Train on 12070 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3880.2632 - val_loss: 40331.3477\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2673.6709 - val_loss: 40279.9805\n",
      "Train on 12071 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2449.6336 - val_loss: 36945.0430\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2816.7056 - val_loss: 40029.4961\n",
      "Train on 12072 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3662.1617 - val_loss: 35769.9297\n",
      "Epoch 2/2\n",
      " - 2s - loss: 9405.8834 - val_loss: 32419.9551\n",
      "Train on 12073 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2907.7540 - val_loss: 41884.8789\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3528.0703 - val_loss: 41145.3906\n",
      "Train on 12074 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 8529.6344 - val_loss: 43747.7656\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4951.3821 - val_loss: 42874.2734\n",
      "Train on 12075 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4153.5476 - val_loss: 36196.9219\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3592.6476 - val_loss: 35818.9805\n",
      "Train on 12076 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3072.0328 - val_loss: 34861.9766\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3140.0979 - val_loss: 34659.1484\n",
      "Train on 12077 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3629.5208 - val_loss: 36648.2227\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4440.3666 - val_loss: 35468.4023\n",
      "Train on 12078 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6595.5826 - val_loss: 33576.7148\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7142.4697 - val_loss: 33253.8750\n",
      "Train on 12079 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3615.3372 - val_loss: 36160.7930\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2643.0317 - val_loss: 37947.0586\n",
      "Train on 12080 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3781.4776 - val_loss: 38490.1094\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2763.7941 - val_loss: 38732.9492\n",
      "Train on 12081 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5269.8957 - val_loss: 33680.7617\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2589.4966 - val_loss: 36567.5977\n",
      "Train on 12082 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6107.8271 - val_loss: 38497.6797\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5758.5771 - val_loss: 37369.5352\n",
      "Train on 12083 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9420.3747 - val_loss: 39851.2109\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2601.1738 - val_loss: 37675.5117\n",
      "Train on 12084 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2615.3571 - val_loss: 39665.6445\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3650.7914 - val_loss: 39232.6211\n",
      "Train on 12085 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3351.6895 - val_loss: 42369.6406\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2733.4166 - val_loss: 45130.0039\n",
      "Train on 12086 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3093.1031 - val_loss: 27138.5410\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4358.7665 - val_loss: 27143.9297\n",
      "Train on 12087 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1997.4169 - val_loss: 48981.4922\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4634.0553 - val_loss: 50946.7500\n",
      "Train on 12088 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2098.8161 - val_loss: 43494.6484\n",
      "Epoch 2/2\n",
      " - 2s - loss: 19039.5962 - val_loss: 40152.0977\n",
      "Train on 12089 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2374.0585 - val_loss: 42845.3633\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3009.2562 - val_loss: 42093.6953\n",
      "Train on 12090 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2078.1879 - val_loss: 36188.2812\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2525.4276 - val_loss: 36006.6367\n",
      "Train on 12091 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2558.2384 - val_loss: 41531.6719\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5050.7728 - val_loss: 39868.6641\n",
      "Train on 12092 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2234.7645 - val_loss: 41426.3594\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5374.4856 - val_loss: 40511.0938\n",
      "Train on 12093 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2121.6823 - val_loss: 38484.0742\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1701.5192 - val_loss: 38282.5156\n",
      "Train on 12094 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4897.8832 - val_loss: 42168.5625\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4266.8044 - val_loss: 41594.9844\n",
      "Train on 12095 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4481.4040 - val_loss: 38114.3633\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2424.9304 - val_loss: 37017.0664\n",
      "Train on 12096 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2345.8273 - val_loss: 36769.1875\n",
      "Epoch 2/2\n",
      " - 2s - loss: 24059.4240 - val_loss: 40710.1562\n",
      "Train on 12097 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3451.5463 - val_loss: 39498.8438\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2249.3627 - val_loss: 40722.5703\n",
      "Train on 12098 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1944.0671 - val_loss: 43328.4141\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2467.4204 - val_loss: 43613.5703\n",
      "Train on 12099 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1781.9496 - val_loss: 41572.0820\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1884.9192 - val_loss: 40785.7539\n",
      "Train on 12100 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2358.9449 - val_loss: 38406.0469\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2386.2293 - val_loss: 40269.3984\n",
      "Train on 12101 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3559.7920 - val_loss: 39818.9570\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10980.2852 - val_loss: 41551.4766\n",
      "Train on 12102 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9247.5060 - val_loss: 38774.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      " - 2s - loss: 11820.3580 - val_loss: 39069.9219\n",
      "Train on 12103 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 15744.9515 - val_loss: 33829.1406\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6241.0620 - val_loss: 28928.6445\n",
      "Train on 12104 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 8947.4229 - val_loss: 38091.2969\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10059.2585 - val_loss: 38271.1484\n",
      "Train on 12105 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6959.9166 - val_loss: 38554.6055\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6035.5240 - val_loss: 38829.3945\n",
      "Train on 12106 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5695.5381 - val_loss: 40763.2734\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4938.5060 - val_loss: 40358.4180\n",
      "Train on 12107 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5307.5006 - val_loss: 42354.9648\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4371.6194 - val_loss: 43726.6289\n",
      "Train on 12108 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4675.3226 - val_loss: 35345.8359\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3393.7293 - val_loss: 36359.3008\n",
      "Train on 12109 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5184.9784 - val_loss: 42461.7539\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7654.5925 - val_loss: 42866.1836\n",
      "Train on 12110 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3296.5504 - val_loss: 36879.2812\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5038.6255 - val_loss: 37947.5352\n",
      "Train on 12111 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6438.1674 - val_loss: 30067.9492\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5519.2629 - val_loss: 29293.7090\n",
      "Train on 12112 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2839.5909 - val_loss: 51259.1250\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4171.9755 - val_loss: 51739.3672\n",
      "Train on 12113 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4008.3180 - val_loss: 61274.9844\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3405.7635 - val_loss: 61245.2539\n",
      "Train on 12114 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3474.2535 - val_loss: 45968.9922\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4479.3833 - val_loss: 43054.8320\n",
      "Train on 12115 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4641.1235 - val_loss: 45151.9961\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3065.7764 - val_loss: 44079.3906\n",
      "Train on 12116 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5321.9937 - val_loss: 39195.8789\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2637.4877 - val_loss: 40252.8398\n",
      "Train on 12117 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9338.3373 - val_loss: 50754.7227\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3095.8626 - val_loss: 51369.5156\n",
      "Train on 12118 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2554.2068 - val_loss: 45532.5391\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2572.7882 - val_loss: 45349.5547\n",
      "Train on 12119 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3795.9174 - val_loss: 42657.2734\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2437.8351 - val_loss: 43717.1328\n",
      "Train on 12120 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 10638.5471 - val_loss: 45824.6367\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2924.6077 - val_loss: 45870.3242\n",
      "Train on 12121 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3733.8585 - val_loss: 51864.7266\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2208.6953 - val_loss: 43508.0938\n",
      "Train on 12122 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 53739.6183 - val_loss: 32438.2441\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3188.3011 - val_loss: 35425.4727\n",
      "Train on 12123 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1869.4476 - val_loss: 41372.3164\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2481.9525 - val_loss: 42331.7578\n",
      "Train on 12124 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2839.5408 - val_loss: 44101.4375\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7600.3327 - val_loss: 43200.6484\n",
      "Train on 12125 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2069.6707 - val_loss: 42409.0469\n",
      "Epoch 2/2\n",
      " - 1s - loss: 3147.5009 - val_loss: 42414.8789\n",
      "Train on 12126 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2692.0120 - val_loss: 44952.8008\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2669.5269 - val_loss: 43526.4297\n",
      "Train on 12127 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1572.4522 - val_loss: 44862.2617\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1168.8391 - val_loss: 43412.3047\n",
      "Train on 12128 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1496.4825 - val_loss: 40897.7539\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2089.8092 - val_loss: 41060.1562\n",
      "Train on 12129 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7982.9206 - val_loss: 41848.1133\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3669.5295 - val_loss: 41209.4961\n",
      "Train on 12130 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4575.6102 - val_loss: 40985.1914\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1645.0004 - val_loss: 41280.6953\n",
      "Train on 12131 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2055.4110 - val_loss: 40206.8086\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1744.4885 - val_loss: 40481.8125\n",
      "Train on 12132 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2555.8623 - val_loss: 41133.2070\n",
      "Epoch 2/2\n",
      " - 2s - loss: 903.8106 - val_loss: 40174.5078\n",
      "Train on 12133 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1708.3743 - val_loss: 39947.7695\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1590.5544 - val_loss: 40369.2070\n",
      "Train on 12134 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2898.0207 - val_loss: 39450.8242\n",
      "Epoch 2/2\n",
      " - 2s - loss: 22940.1784 - val_loss: 41176.2969\n",
      "Train on 12135 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5225.0428 - val_loss: 39998.9258\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1480.7568 - val_loss: 40549.3359\n",
      "Train on 12136 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1869.4361 - val_loss: 37801.2891\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2465.8716 - val_loss: 37837.1836\n",
      "Train on 12137 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1318.8366 - val_loss: 37132.8945\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1057.0922 - val_loss: 36023.3164\n",
      "Train on 12138 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4315.0346 - val_loss: 36581.5117\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7256.2469 - val_loss: 35644.0742\n",
      "Train on 12139 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2310.1377 - val_loss: 34425.7422\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1688.0787 - val_loss: 34917.7031\n",
      "Train on 12140 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 5320.4419 - val_loss: 35540.3516\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5962.6400 - val_loss: 32783.8047\n",
      "Train on 12141 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2872.1673 - val_loss: 35555.0781\n",
      "Epoch 2/2\n",
      " - 1s - loss: 5401.8271 - val_loss: 34211.9023\n",
      "Train on 12142 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4661.7226 - val_loss: 34938.0547\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1934.8132 - val_loss: 35156.3398\n",
      "Train on 12143 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2235.2056 - val_loss: 35509.8867\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2928.6231 - val_loss: 34406.0820\n",
      "Train on 12144 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1259.4285 - val_loss: 35261.9805\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2303.3404 - val_loss: 35964.1211\n",
      "Train on 12145 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4995.1715 - val_loss: 34922.6289\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4178.5523 - val_loss: 35759.6797\n",
      "Train on 12146 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1854.9320 - val_loss: 34878.0234\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4510.6818 - val_loss: 35182.2539\n",
      "Train on 12147 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1037.6069 - val_loss: 34212.6250\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2041.2885 - val_loss: 34616.8906\n",
      "Train on 12148 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 23188.5229 - val_loss: 34867.2656\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1996.3375 - val_loss: 35531.0547\n",
      "Train on 12149 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1443.2222 - val_loss: 34829.1641\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1732.5784 - val_loss: 36401.4922\n",
      "Train on 12150 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1452.1903 - val_loss: 35304.7227\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1095.9401 - val_loss: 35374.5742\n",
      "Train on 12151 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2176.6280 - val_loss: 34394.8555\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2117.0696 - val_loss: 33172.7188\n",
      "Train on 12152 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1178.4750 - val_loss: 35349.3242\n",
      "Epoch 2/2\n",
      " - 2s - loss: 848.4879 - val_loss: 34940.4258\n",
      "Train on 12153 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3543.1752 - val_loss: 35013.2969\n",
      "Epoch 2/2\n",
      " - 2s - loss: 798.4113 - val_loss: 34443.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12154 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3018.9091 - val_loss: 33985.7695\n",
      "Epoch 2/2\n",
      " - 1s - loss: 3384.9646 - val_loss: 33589.4219\n",
      "Train on 12155 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1813.3490 - val_loss: 35050.0391\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1274.1789 - val_loss: 33535.1250\n",
      "Train on 12156 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 3214.4031 - val_loss: 34455.6445\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2997.9793 - val_loss: 35357.6797\n",
      "Train on 12157 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1054.0033 - val_loss: 34046.3750\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3158.0352 - val_loss: 35763.8320\n",
      "Train on 12158 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 2885.1504 - val_loss: 35135.4688\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3475.1056 - val_loss: 36992.6445\n",
      "Train on 12159 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 13025.9325 - val_loss: 33966.0586\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1299.6383 - val_loss: 35391.2930\n",
      "Train on 12160 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1371.0578 - val_loss: 36304.3906\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1895.9770 - val_loss: 35775.1914\n",
      "Train on 12161 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3354.2357 - val_loss: 35294.3555\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6112.8114 - val_loss: 34051.8711\n",
      "Train on 12162 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1279.6008 - val_loss: 36290.7148\n",
      "Epoch 2/2\n",
      " - 2s - loss: 988.8414 - val_loss: 36602.4336\n",
      "Train on 12163 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3991.3797 - val_loss: 35369.8906\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1552.6953 - val_loss: 35676.9023\n",
      "Train on 12164 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1574.4171 - val_loss: 35177.1289\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1586.4672 - val_loss: 35655.4141\n",
      "Train on 12165 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1636.9065 - val_loss: 35701.5312\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1525.9612 - val_loss: 36421.3359\n",
      "Train on 12166 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9951.3380 - val_loss: 35699.6836\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1398.1691 - val_loss: 35836.5430\n",
      "Train on 12167 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3798.5576 - val_loss: 34655.6953\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1752.6196 - val_loss: 36499.4688\n",
      "Train on 12168 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1785.9976 - val_loss: 35461.5039\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1998.2020 - val_loss: 34406.3555\n",
      "Train on 12169 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 867.0524 - val_loss: 34783.1602\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1713.9654 - val_loss: 34490.4570\n",
      "Train on 12170 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2013.6361 - val_loss: 35008.5469\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1795.7684 - val_loss: 33821.6875\n",
      "Train on 12171 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1705.2676 - val_loss: 35152.6797\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4658.7425 - val_loss: 36278.9023\n",
      "Train on 12172 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4137.9374 - val_loss: 34616.6211\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2961.7608 - val_loss: 35447.3477\n",
      "Train on 12173 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1500.7319 - val_loss: 35967.1797\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1147.9590 - val_loss: 36648.4102\n",
      "Train on 12174 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 12164.3372 - val_loss: 34746.1055\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1611.6147 - val_loss: 34676.7852\n",
      "Train on 12175 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1839.3784 - val_loss: 34751.5664\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1636.3112 - val_loss: 34855.5039\n",
      "Train on 12176 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1527.3486 - val_loss: 34757.9414\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2811.1393 - val_loss: 34125.0742\n",
      "Train on 12177 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2056.9058 - val_loss: 34799.0977\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1078.5900 - val_loss: 36044.0781\n",
      "Train on 12178 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1132.3845 - val_loss: 35429.8828\n",
      "Epoch 2/2\n",
      " - 2s - loss: 665.4685 - val_loss: 35159.0898\n",
      "Train on 12179 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2315.6356 - val_loss: 35451.4844\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5399.3802 - val_loss: 37453.4961\n",
      "Train on 12180 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 18303.6955 - val_loss: 33793.9453\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5154.9033 - val_loss: 35030.7500\n",
      "Train on 12181 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2745.4414 - val_loss: 35501.9766\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1258.5025 - val_loss: 35396.9883\n",
      "Train on 12182 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 803.4624 - val_loss: 36273.8789\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1119.0986 - val_loss: 36269.2305\n",
      "Train on 12183 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3973.5658 - val_loss: 36607.9453\n",
      "Epoch 2/2\n",
      " - 2s - loss: 955.0113 - val_loss: 36435.0352\n",
      "Train on 12184 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 795.4989 - val_loss: 35678.8398\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1400.4770 - val_loss: 36401.7695\n",
      "Train on 12185 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1625.6233 - val_loss: 36059.6562\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1303.6617 - val_loss: 35375.2188\n",
      "Train on 12186 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1612.1800 - val_loss: 34600.5430\n",
      "Epoch 2/2\n",
      " - 2s - loss: 10193.1814 - val_loss: 32816.0820\n",
      "Train on 12187 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7136.3603 - val_loss: 36537.8203\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1601.7251 - val_loss: 35010.8320\n",
      "Train on 12188 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1101.8371 - val_loss: 34953.1133\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1720.1258 - val_loss: 34826.1562\n",
      "Train on 12189 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2492.8048 - val_loss: 33745.8516\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1768.1451 - val_loss: 35052.0469\n",
      "Train on 12190 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 917.4444 - val_loss: 35520.6523\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1295.6808 - val_loss: 33907.5000\n",
      "Train on 12191 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1613.8356 - val_loss: 35239.9805\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1378.7170 - val_loss: 34781.4297\n",
      "Train on 12192 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 805.2108 - val_loss: 34362.1719\n",
      "Epoch 2/2\n",
      " - 2s - loss: 12939.6256 - val_loss: 33838.7539\n",
      "Train on 12193 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1428.9793 - val_loss: 34628.4297\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1217.6210 - val_loss: 35259.5039\n",
      "Train on 12194 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1066.0949 - val_loss: 34444.8594\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2040.1359 - val_loss: 34636.2461\n",
      "Train on 12195 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1698.9199 - val_loss: 34649.2422\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1979.6092 - val_loss: 34754.1172\n",
      "Train on 12196 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2604.4762 - val_loss: 34436.8828\n",
      "Epoch 2/2\n",
      " - 2s - loss: 767.8437 - val_loss: 34180.6602\n",
      "Train on 12197 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4961.6529 - val_loss: 34919.8945\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3716.5372 - val_loss: 36251.5664\n",
      "Train on 12198 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1449.4438 - val_loss: 34749.0195\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8106.5988 - val_loss: 34919.0703\n",
      "Train on 12199 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1472.4298 - val_loss: 34188.5156\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1081.4910 - val_loss: 34481.5703\n",
      "Train on 12200 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1972.7456 - val_loss: 35535.4727\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1796.1861 - val_loss: 34759.2148\n",
      "Train on 12201 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9537.2907 - val_loss: 34923.8164\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1097.2407 - val_loss: 34872.9180\n",
      "Train on 12202 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 935.6076 - val_loss: 35033.6758\n",
      "Epoch 2/2\n",
      " - 2s - loss: 6766.8953 - val_loss: 35679.9453\n",
      "Train on 12203 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1061.5206 - val_loss: 32540.6016\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2847.2761 - val_loss: 33097.3438\n",
      "Train on 12204 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2425.4401 - val_loss: 31616.3301\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1393.5105 - val_loss: 32281.8066\n",
      "Train on 12205 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 1522.7717 - val_loss: 31906.5410\n",
      "Epoch 2/2\n",
      " - 2s - loss: 963.6547 - val_loss: 31163.7988\n",
      "Train on 12206 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1709.4812 - val_loss: 31180.2656\n",
      "Epoch 2/2\n",
      " - 2s - loss: 7188.2541 - val_loss: 31232.1914\n",
      "Train on 12207 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1217.7004 - val_loss: 32539.8965\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1911.5238 - val_loss: 32414.4160\n",
      "Train on 12208 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1600.7728 - val_loss: 32917.5273\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1099.1366 - val_loss: 31930.0957\n",
      "Train on 12209 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 959.5872 - val_loss: 32668.6211\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2213.1657 - val_loss: 32323.0527\n",
      "Train on 12210 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4413.7603 - val_loss: 33016.0273\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3078.3046 - val_loss: 31730.5156\n",
      "Train on 12211 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1834.6915 - val_loss: 33005.2930\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1465.3813 - val_loss: 32352.5566\n",
      "Train on 12212 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1831.8454 - val_loss: 33159.9102\n",
      "Epoch 2/2\n",
      " - 2s - loss: 16279.7747 - val_loss: 31897.0352\n",
      "Train on 12213 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 993.2142 - val_loss: 32459.1777\n",
      "Epoch 2/2\n",
      " - 2s - loss: 853.4467 - val_loss: 32164.3555\n",
      "Train on 12214 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1694.3466 - val_loss: 32469.8223\n",
      "Epoch 2/2\n",
      " - 2s - loss: 900.2411 - val_loss: 32592.2383\n",
      "Train on 12215 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1535.0382 - val_loss: 32441.6738\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1147.3798 - val_loss: 32113.7598\n",
      "Train on 12216 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 861.8056 - val_loss: 32566.3262\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1149.2492 - val_loss: 32148.6816\n",
      "Train on 12217 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1642.3336 - val_loss: 32219.7246\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1889.1802 - val_loss: 33587.6328\n",
      "Train on 12218 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1485.2343 - val_loss: 32918.5938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1357.0982 - val_loss: 32719.2988\n",
      "Train on 12219 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 740.8279 - val_loss: 32425.4062\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1614.3597 - val_loss: 32578.0469\n",
      "Train on 12220 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2149.2450 - val_loss: 31824.6953\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1664.2761 - val_loss: 32395.3418\n",
      "Train on 12221 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9574.4176 - val_loss: 34343.6172\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3516.9610 - val_loss: 32414.5039\n",
      "Train on 12222 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1319.4873 - val_loss: 32829.4414\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1771.0571 - val_loss: 32425.6699\n",
      "Train on 12223 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2938.1598 - val_loss: 34888.2383\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1540.5505 - val_loss: 34358.4609\n",
      "Train on 12224 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 10572.4646 - val_loss: 35517.5234\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1933.6114 - val_loss: 37739.9883\n",
      "Train on 12225 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 625.2315 - val_loss: 36858.9375\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2142.0746 - val_loss: 37076.8359\n",
      "Train on 12226 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1465.7191 - val_loss: 37423.6406\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3285.6181 - val_loss: 37251.7344\n",
      "Train on 12227 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 975.7939 - val_loss: 38470.5703\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1234.3041 - val_loss: 38088.7266\n",
      "Train on 12228 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1492.5234 - val_loss: 37723.2930\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2937.1242 - val_loss: 37068.7539\n",
      "Train on 12229 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1761.0912 - val_loss: 38787.0703\n",
      "Epoch 2/2\n",
      " - 2s - loss: 15780.3891 - val_loss: 37024.0156\n",
      "Train on 12230 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2602.4821 - val_loss: 37518.8203\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1012.1609 - val_loss: 38388.6328\n",
      "Train on 12231 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1062.4735 - val_loss: 37562.1484\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1478.8212 - val_loss: 38050.4258\n",
      "Train on 12232 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3035.7875 - val_loss: 38261.1172\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1441.6808 - val_loss: 38290.7305\n",
      "Train on 12233 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1042.0489 - val_loss: 37725.5703\n",
      "Epoch 2/2\n",
      " - 2s - loss: 667.4606 - val_loss: 37673.4258\n",
      "Train on 12234 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1136.3465 - val_loss: 37706.9844\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1300.8950 - val_loss: 37930.4141\n",
      "Train on 12235 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1119.8015 - val_loss: 38498.2539\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1218.5091 - val_loss: 37228.2695\n",
      "Train on 12236 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 23980.7137 - val_loss: 34146.6367\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1186.8357 - val_loss: 36797.2812\n",
      "Train on 12237 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 683.8143 - val_loss: 37334.6172\n",
      "Epoch 2/2\n",
      " - 2s - loss: 673.4832 - val_loss: 37375.6680\n",
      "Train on 12238 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 596.9139 - val_loss: 37333.6758\n",
      "Epoch 2/2\n",
      " - 2s - loss: 615.0911 - val_loss: 37931.9375\n",
      "Train on 12239 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 599.7489 - val_loss: 37172.2344\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1731.9623 - val_loss: 37739.8906\n",
      "Train on 12240 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2600.2709 - val_loss: 37783.4453\n",
      "Epoch 2/2\n",
      " - 2s - loss: 8306.7549 - val_loss: 36250.1719\n",
      "Train on 12241 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1306.9648 - val_loss: 38787.4570\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2934.7400 - val_loss: 38365.8672\n",
      "Train on 12242 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1038.4303 - val_loss: 36978.8398\n",
      "Epoch 2/2\n",
      " - 2s - loss: 924.7983 - val_loss: 36685.8086\n",
      "Train on 12243 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 752.4870 - val_loss: 36746.6250\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2298.1670 - val_loss: 36822.4805\n",
      "Train on 12244 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2544.9441 - val_loss: 37407.9609\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1904.8944 - val_loss: 35953.2891\n",
      "Train on 12245 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 960.4950 - val_loss: 36048.6211\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1021.9474 - val_loss: 35649.9766\n",
      "Train on 12246 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1299.2662 - val_loss: 36401.1172\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1182.6545 - val_loss: 36097.7734\n",
      "Train on 12247 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 923.7023 - val_loss: 36990.9531\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2167.8862 - val_loss: 36615.3281\n",
      "Train on 12248 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3031.1235 - val_loss: 35757.0938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1335.3824 - val_loss: 35067.7734\n",
      "Train on 12249 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 663.9173 - val_loss: 36079.9648\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1932.8713 - val_loss: 35698.9453\n",
      "Train on 12250 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2442.1810 - val_loss: 35903.3125\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1006.0795 - val_loss: 36142.6875\n",
      "Train on 12251 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4523.6946 - val_loss: 35940.6992\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1271.6980 - val_loss: 35271.0586\n",
      "Train on 12252 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7494.5937 - val_loss: 37616.7734\n",
      "Epoch 2/2\n",
      " - 2s - loss: 638.6985 - val_loss: 36830.9141\n",
      "Train on 12253 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 867.8714 - val_loss: 37178.7305\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3582.2119 - val_loss: 37160.0000\n",
      "Train on 12254 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1077.0666 - val_loss: 37928.8945\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1070.5348 - val_loss: 37599.2539\n",
      "Train on 12255 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1032.5138 - val_loss: 35001.2383\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1353.3259 - val_loss: 34463.5312\n",
      "Train on 12256 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1460.2405 - val_loss: 33818.7266\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 18012.2219 - val_loss: 32151.0469\n",
      "Train on 12257 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1121.9767 - val_loss: 34306.2578\n",
      "Epoch 2/2\n",
      " - 2s - loss: 788.4323 - val_loss: 34356.1055\n",
      "Train on 12258 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 675.9902 - val_loss: 34047.0977\n",
      "Epoch 2/2\n",
      " - 2s - loss: 622.0551 - val_loss: 34920.0742\n",
      "Train on 12259 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1398.4084 - val_loss: 34320.0039\n",
      "Epoch 2/2\n",
      " - 2s - loss: 900.9606 - val_loss: 34514.9453\n",
      "Train on 12260 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3462.3762 - val_loss: 34114.8828\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2541.8084 - val_loss: 33825.1914\n",
      "Train on 12261 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 594.9176 - val_loss: 35206.2539\n",
      "Epoch 2/2\n",
      " - 1s - loss: 3537.2722 - val_loss: 34192.1289\n",
      "Train on 12262 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 1293.2198 - val_loss: 34942.6172\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2012.6399 - val_loss: 35039.1602\n",
      "Train on 12263 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1198.8162 - val_loss: 34529.1875\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1161.9731 - val_loss: 34247.6758\n",
      "Train on 12264 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 816.6007 - val_loss: 34703.1602\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1055.5965 - val_loss: 34941.8867\n",
      "Train on 12265 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1122.9110 - val_loss: 35632.4609\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1155.9642 - val_loss: 35481.4609\n",
      "Train on 12266 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1789.5333 - val_loss: 36198.5938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2131.6715 - val_loss: 35253.2695\n",
      "Train on 12267 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2961.5527 - val_loss: 36012.5664\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2734.2113 - val_loss: 35831.6445\n",
      "Train on 12268 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4058.8236 - val_loss: 35819.8125\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1485.9689 - val_loss: 35144.3477\n",
      "Train on 12269 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1170.0036 - val_loss: 35649.0547\n",
      "Epoch 2/2\n",
      " - 2s - loss: 970.8542 - val_loss: 36241.8984\n",
      "Train on 12270 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 807.2701 - val_loss: 35684.5586\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1037.5357 - val_loss: 36349.1523\n",
      "Train on 12271 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1673.4073 - val_loss: 35731.6133\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2174.5042 - val_loss: 36839.6289\n",
      "Train on 12272 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1186.8122 - val_loss: 36567.8789\n",
      "Epoch 2/2\n",
      " - 2s - loss: 743.6796 - val_loss: 35943.7539\n",
      "Train on 12273 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3505.4827 - val_loss: 34805.0195\n",
      "Epoch 2/2\n",
      " - 2s - loss: 927.6698 - val_loss: 35189.9492\n",
      "Train on 12274 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1136.7857 - val_loss: 35733.7383\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2728.3295 - val_loss: 34313.4023\n",
      "Train on 12275 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1003.6836 - val_loss: 34851.4023\n",
      "Epoch 2/2\n",
      " - 2s - loss: 733.9421 - val_loss: 34285.7305\n",
      "Train on 12276 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 857.2636 - val_loss: 35872.1367\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2373.9616 - val_loss: 36123.7539\n",
      "Train on 12277 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1290.4819 - val_loss: 35651.5430\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3670.6203 - val_loss: 36818.6406\n",
      "Train on 12278 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1817.2582 - val_loss: 36398.0430\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1434.8667 - val_loss: 35745.0000\n",
      "Train on 12279 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1428.4296 - val_loss: 35828.1328\n",
      "Epoch 2/2\n",
      " - 1s - loss: 5623.6339 - val_loss: 36546.6875\n",
      "Train on 12280 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 2026.2579 - val_loss: 36475.4062\n",
      "Epoch 2/2\n",
      " - 2s - loss: 957.3715 - val_loss: 35596.9844\n",
      "Train on 12281 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1584.9182 - val_loss: 34653.8750\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1605.1391 - val_loss: 34287.6289\n",
      "Train on 12282 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1697.2806 - val_loss: 34233.0391\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3873.6913 - val_loss: 34386.3438\n",
      "Train on 12283 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 946.6571 - val_loss: 34682.3320\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1046.0450 - val_loss: 33360.1836\n",
      "Train on 12284 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1466.2811 - val_loss: 35177.3086\n",
      "Epoch 2/2\n",
      " - 2s - loss: 9870.6434 - val_loss: 36084.8789\n",
      "Train on 12285 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1182.4660 - val_loss: 34911.5000\n",
      "Epoch 2/2\n",
      " - 1s - loss: 1981.4969 - val_loss: 34075.8438\n",
      "Train on 12286 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2331.5255 - val_loss: 35879.0742\n",
      "Epoch 2/2\n",
      " - 1s - loss: 2656.2860 - val_loss: 34819.7773\n",
      "Train on 12287 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 741.5022 - val_loss: 35028.4688\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1069.3155 - val_loss: 35406.2656\n",
      "Train on 12288 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 1273.6621 - val_loss: 35574.3242\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1000.9107 - val_loss: 35240.9883\n",
      "Train on 12289 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1647.3849 - val_loss: 34829.8008\n",
      "Epoch 2/2\n",
      " - 2s - loss: 592.6810 - val_loss: 35268.2148\n",
      "Train on 12290 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4080.5175 - val_loss: 33112.0938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1153.5284 - val_loss: 34155.3906\n",
      "Train on 12291 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 911.0544 - val_loss: 33160.3555\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3134.4991 - val_loss: 33009.1953\n",
      "Train on 12292 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 791.5993 - val_loss: 32483.8145\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5086.6230 - val_loss: 32298.9160\n",
      "Train on 12293 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1956.4747 - val_loss: 32545.3574\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1470.9086 - val_loss: 34422.4805\n",
      "Train on 12294 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2310.2649 - val_loss: 33677.7148\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1537.0590 - val_loss: 33860.4922\n",
      "Train on 12295 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 8372.7421 - val_loss: 35892.3945\n",
      "Epoch 2/2\n",
      " - 2s - loss: 957.7016 - val_loss: 34819.2305\n",
      "Train on 12296 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1195.5354 - val_loss: 34724.6289\n",
      "Epoch 2/2\n",
      " - 2s - loss: 795.5783 - val_loss: 34587.1914\n",
      "Train on 12297 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2235.3897 - val_loss: 33794.1250\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1217.1589 - val_loss: 34670.9688\n",
      "Train on 12298 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2987.4670 - val_loss: 33375.1680\n",
      "Epoch 2/2\n",
      " - 2s - loss: 785.8222 - val_loss: 34525.2891\n",
      "Train on 12299 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1211.8448 - val_loss: 33631.2266\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1008.9614 - val_loss: 33771.9570\n",
      "Train on 12300 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 6190.7528 - val_loss: 31949.5547\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1381.3026 - val_loss: 33672.2500\n",
      "Train on 12301 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1240.5239 - val_loss: 33690.7109\n",
      "Epoch 2/2\n",
      " - 2s - loss: 507.7409 - val_loss: 33636.6875\n",
      "Train on 12302 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9617.1600 - val_loss: 31357.0078\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2024.2703 - val_loss: 32772.8438\n",
      "Train on 12303 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1046.0713 - val_loss: 33643.5859\n",
      "Epoch 2/2\n",
      " - 2s - loss: 858.0628 - val_loss: 33594.1641\n",
      "Train on 12304 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1234.3609 - val_loss: 33723.6992\n",
      "Epoch 2/2\n",
      " - 2s - loss: 955.4186 - val_loss: 33992.5195\n",
      "Train on 12305 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 760.1421 - val_loss: 33699.1367\n",
      "Epoch 2/2\n",
      " - 2s - loss: 804.8249 - val_loss: 34113.6211\n",
      "Train on 12306 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 7328.5665 - val_loss: 33948.9609\n",
      "Epoch 2/2\n",
      " - 2s - loss: 945.8736 - val_loss: 32950.0508\n",
      "Train on 12307 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 635.8610 - val_loss: 33635.8828\n",
      "Epoch 2/2\n",
      " - 2s - loss: 738.1769 - val_loss: 33280.2344\n",
      "Train on 12308 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 890.4464 - val_loss: 33440.8555\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5250.1080 - val_loss: 32474.1348\n",
      "Train on 12309 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 612.5704 - val_loss: 33406.9336\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1228.9537 - val_loss: 33943.0234\n",
      "Train on 12310 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1091.9697 - val_loss: 33058.2695\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1548.6129 - val_loss: 32975.3164\n",
      "Train on 12311 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 832.1528 - val_loss: 33850.1602\n",
      "Epoch 2/2\n",
      " - 2s - loss: 939.5172 - val_loss: 34668.7852\n",
      "Train on 12312 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 625.0099 - val_loss: 34447.3945\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2158.1076 - val_loss: 33035.8125\n",
      "Train on 12313 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 4291.6884 - val_loss: 34129.4062\n",
      "Epoch 2/2\n",
      " - 2s - loss: 807.0260 - val_loss: 34387.2461\n",
      "Train on 12314 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1892.2667 - val_loss: 34209.4648\n",
      "Epoch 2/2\n",
      " - 2s - loss: 859.4695 - val_loss: 34484.2891\n",
      "Train on 12315 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1140.5901 - val_loss: 34461.8086\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1058.2585 - val_loss: 34200.5234\n",
      "Train on 12316 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1258.8482 - val_loss: 33668.5781\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2108.8307 - val_loss: 33700.3906\n",
      "Train on 12317 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 3163.3854 - val_loss: 35494.2461\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1744.1201 - val_loss: 34286.9961\n",
      "Train on 12318 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1555.3438 - val_loss: 34692.5195\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1246.5321 - val_loss: 34450.0234\n",
      "Train on 12319 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 9969.4428 - val_loss: 35162.8438\n",
      "Epoch 2/2\n",
      " - 2s - loss: 930.9812 - val_loss: 33620.2109\n",
      "Train on 12320 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 670.5086 - val_loss: 34040.7891\n",
      "Epoch 2/2\n",
      " - 2s - loss: 685.5551 - val_loss: 34507.5078\n",
      "Train on 12321 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 445.9040 - val_loss: 33717.8711\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1459.8264 - val_loss: 33373.2969\n",
      "Train on 12322 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1404.4601 - val_loss: 33954.8086\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2957.9107 - val_loss: 33715.8984\n",
      "Train on 12323 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1657.2469 - val_loss: 33797.0898\n",
      "Epoch 2/2\n",
      " - 2s - loss: 5924.4621 - val_loss: 34248.0391\n",
      "Train on 12324 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1054.7724 - val_loss: 33983.8789\n",
      "Epoch 2/2\n",
      " - 2s - loss: 4241.4460 - val_loss: 34349.5898\n",
      "Train on 12325 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 908.1514 - val_loss: 34390.3242\n",
      "Epoch 2/2\n",
      " - 2s - loss: 931.6156 - val_loss: 34329.7734\n",
      "Train on 12326 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 951.3294 - val_loss: 34800.2812\n",
      "Epoch 2/2\n",
      " - 2s - loss: 532.4747 - val_loss: 34094.0547\n",
      "Train on 12327 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 496.1731 - val_loss: 36058.5430\n",
      "Epoch 2/2\n",
      " - 2s - loss: 13465.9139 - val_loss: 35587.6797\n",
      "Train on 12328 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 979.3983 - val_loss: 32037.5039\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1079.7261 - val_loss: 32091.7129\n",
      "Train on 12329 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1488.7411 - val_loss: 31521.4199\n",
      "Epoch 2/2\n",
      " - 2s - loss: 726.1819 - val_loss: 31917.2715\n",
      "Train on 12330 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 764.3546 - val_loss: 31979.7598\n",
      "Epoch 2/2\n",
      " - 2s - loss: 852.6994 - val_loss: 31888.8379\n",
      "Train on 12331 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 968.3316 - val_loss: 31954.9668\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1217.8313 - val_loss: 31828.2676\n",
      "Train on 12332 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1859.7182 - val_loss: 32614.2793\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1039.7488 - val_loss: 31928.1758\n",
      "Train on 12333 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1465.2898 - val_loss: 30995.1680\n",
      "Epoch 2/2\n",
      " - 2s - loss: 3306.7806 - val_loss: 30713.4180\n",
      "Train on 12334 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1246.9711 - val_loss: 31180.0938\n",
      "Epoch 2/2\n",
      " - 2s - loss: 2240.5692 - val_loss: 31317.0742\n",
      "Train on 12335 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 1890.7380 - val_loss: 31132.7754\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1908.1244 - val_loss: 31552.2891\n",
      "Train on 12336 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 812.1456 - val_loss: 31519.6855\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1627.5240 - val_loss: 30468.8281\n",
      "Train on 12337 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 954.6409 - val_loss: 31263.6094\n",
      "Epoch 2/2\n",
      " - 2s - loss: 632.3857 - val_loss: 31533.9902\n",
      "Train on 12338 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 2017.0647 - val_loss: 31980.8086\n",
      "Epoch 2/2\n",
      " - 2s - loss: 867.9255 - val_loss: 31399.1309\n",
      "Train on 12339 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 929.8807 - val_loss: 31680.9590\n",
      "Epoch 2/2\n",
      " - 2s - loss: 1233.5263 - val_loss: 31957.5859\n"
     ]
    }
   ],
   "source": [
    "res = make_refitting_window(LSTM_model_3, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c3c651400>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "sns.lineplot(data=pd.DataFrame({'y' : res[1]}))\n",
    "sns.lineplot(data=pd.DataFrame({'yhat' : res[0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3434.413,\n",
       " 3433.0493,\n",
       " 3435.8845,\n",
       " 3426.1816,\n",
       " 3428.7656,\n",
       " 3435.9648,\n",
       " 3434.5532,\n",
       " 3431.7764,\n",
       " 3421.282,\n",
       " 3431.8796,\n",
       " 3430.3389,\n",
       " 3431.5254,\n",
       " 3435.0134,\n",
       " 3430.6624,\n",
       " 3436.0645,\n",
       " 3438.0942,\n",
       " 3430.3699,\n",
       " 3434.8362,\n",
       " 3436.8806,\n",
       " 3438.159,\n",
       " 3439.357,\n",
       " 3432.4304,\n",
       " 3431.5596,\n",
       " 3435.353,\n",
       " 3437.4746,\n",
       " 3440.3604,\n",
       " 3441.0386,\n",
       " 3436.6724,\n",
       " 3431.371,\n",
       " 3435.0383,\n",
       " 3434.1345,\n",
       " 3434.485,\n",
       " 3437.875,\n",
       " 3439.476,\n",
       " 3445.3562,\n",
       " 3433.4536,\n",
       " 3440.1616,\n",
       " 3435.413,\n",
       " 3434.9814,\n",
       " 3434.1685,\n",
       " 3432.4915,\n",
       " 3432.337,\n",
       " 3436.2932,\n",
       " 3436.204,\n",
       " 3435.0232,\n",
       " 3437.9033,\n",
       " 3434.1548,\n",
       " 3435.2903,\n",
       " 3441.5708,\n",
       " 3435.4792,\n",
       " 3440.5352,\n",
       " 3439.1016,\n",
       " 3437.6406,\n",
       " 3433.8948,\n",
       " 3431.202,\n",
       " 3433.7192,\n",
       " 3435.6763,\n",
       " 3434.161,\n",
       " 3436.2646,\n",
       " 3440.3242,\n",
       " 3437.748,\n",
       " 3431.1895,\n",
       " 3451.0125,\n",
       " 3436.9172,\n",
       " 3435.767,\n",
       " 3436.5454,\n",
       " 3434.8955,\n",
       " 3437.9607,\n",
       " 3435.514,\n",
       " 3436.2888,\n",
       " 3435.2102,\n",
       " 3432.8308,\n",
       " 3442.7935,\n",
       " 3438.7087,\n",
       " 3437.6128,\n",
       " 3438.3945,\n",
       " 3436.3496,\n",
       " 3437.7046,\n",
       " 3437.9727,\n",
       " 3433.5142,\n",
       " 3438.169,\n",
       " 3433.14,\n",
       " 3438.9004,\n",
       " 3438.0154,\n",
       " 3434.393,\n",
       " 3432.9314,\n",
       " 3433.6863,\n",
       " 3433.0654,\n",
       " 3446.1147,\n",
       " 3438.2205,\n",
       " 3438.9607,\n",
       " 3439.808,\n",
       " 3440.3557,\n",
       " 3436.9749,\n",
       " 3438.069,\n",
       " 3437.1309,\n",
       " 3433.3726,\n",
       " 3432.6643,\n",
       " 3436.0945,\n",
       " 3438.8828,\n",
       " 3433.8718,\n",
       " 3438.0396,\n",
       " 3442.1235,\n",
       " 3444.721,\n",
       " 3442.9827,\n",
       " 3436.9138,\n",
       " 3439.512,\n",
       " 3431.3718,\n",
       " 3434.081,\n",
       " 3435.0542,\n",
       " 3431.4832,\n",
       " 3435.3083,\n",
       " 3433.005,\n",
       " 3434.094,\n",
       " 3437.3792,\n",
       " 3437.082,\n",
       " 3435.8745,\n",
       " 3431.3245,\n",
       " 3434.0667,\n",
       " 3434.7417,\n",
       " 3434.7783,\n",
       " 3439.8403,\n",
       " 3438.3542,\n",
       " 3433.9216,\n",
       " 3436.1663,\n",
       " 3439.2146,\n",
       " 3438.7043,\n",
       " 3439.2551,\n",
       " 3439.3616,\n",
       " 3439.5889,\n",
       " 3438.6882,\n",
       " 3437.784,\n",
       " 3439.2905,\n",
       " 3439.7327,\n",
       " 3443.375,\n",
       " 3440.3584,\n",
       " 3437.9707,\n",
       " 3440.3186,\n",
       " 3442.5164,\n",
       " 3442.1348,\n",
       " 3447.8284,\n",
       " 3445.4792,\n",
       " 3440.8022,\n",
       " 3443.8167,\n",
       " 3438.1255,\n",
       " 3439.2725,\n",
       " 3440.9758,\n",
       " 3440.6807,\n",
       " 3440.4465,\n",
       " 3437.372,\n",
       " 3439.1638,\n",
       " 3442.8496,\n",
       " 3439.4697,\n",
       " 3439.5493,\n",
       " 3443.6777,\n",
       " 3443.5007,\n",
       " 3440.6487,\n",
       " 3439.4653,\n",
       " 3436.264,\n",
       " 3437.9412,\n",
       " 3437.9006,\n",
       " 3446.0286,\n",
       " 3439.2507,\n",
       " 3442.039,\n",
       " 3441.2683,\n",
       " 3439.466,\n",
       " 3439.5757,\n",
       " 3436.0833,\n",
       " 3440.8975,\n",
       " 3439.8604,\n",
       " 3441.2375,\n",
       " 3435.3755,\n",
       " 3437.0913,\n",
       " 3434.6409,\n",
       " 3437.4775,\n",
       " 3437.8281,\n",
       " 3440.6035,\n",
       " 3435.6292,\n",
       " 3438.1033,\n",
       " 3434.8071,\n",
       " 3441.27,\n",
       " 3438.4592,\n",
       " 3437.9685,\n",
       " 3438.3503,\n",
       " 3437.5708,\n",
       " 3440.3857,\n",
       " 3446.9392,\n",
       " 3440.0928,\n",
       " 3440.2056,\n",
       " 3437.8486,\n",
       " 3442.1628,\n",
       " 3439.7976,\n",
       " 3444.237,\n",
       " 3437.9702,\n",
       " 3439.9724,\n",
       " 3438.9524,\n",
       " 3441.5156,\n",
       " 3436.223,\n",
       " 3439.7107,\n",
       " 3439.5488,\n",
       " 3440.0059,\n",
       " 3439.2422,\n",
       " 3437.807,\n",
       " 3438.4685,\n",
       " 3438.8452,\n",
       " 3442.7812,\n",
       " 3441.4395,\n",
       " 3437.073,\n",
       " 3439.9092,\n",
       " 3438.0627,\n",
       " 3438.976,\n",
       " 3437.1301,\n",
       " 3440.4858,\n",
       " 3438.8516,\n",
       " 3436.973,\n",
       " 3439.6926,\n",
       " 3440.142,\n",
       " 3436.3318,\n",
       " 3437.2183,\n",
       " 3438.5725,\n",
       " 3438.669,\n",
       " 3438.2402,\n",
       " 3438.0884,\n",
       " 3439.2607,\n",
       " 3437.5688,\n",
       " 3438.0715,\n",
       " 3438.5256,\n",
       " 3438.289,\n",
       " 3439.3376,\n",
       " 3442.6018,\n",
       " 3436.7048,\n",
       " 3436.834,\n",
       " 3437.3457,\n",
       " 3438.6455,\n",
       " 3437.1702,\n",
       " 3439.5176,\n",
       " 3440.9502,\n",
       " 3438.952,\n",
       " 3437.535,\n",
       " 3438.4639,\n",
       " 3443.745,\n",
       " 3438.8154,\n",
       " 3439.4597,\n",
       " 3439.6782,\n",
       " 3443.146,\n",
       " 3443.7405,\n",
       " 3442.5037,\n",
       " 3441.9336,\n",
       " 3443.752,\n",
       " 3441.7876,\n",
       " 3441.6187,\n",
       " 3443.043,\n",
       " 3439.761,\n",
       " 3441.585,\n",
       " 3440.2136,\n",
       " 3444.406,\n",
       " 3447.4312,\n",
       " 3441.0903,\n",
       " 3439.6572,\n",
       " 3439.9763,\n",
       " 3442.4373,\n",
       " 3441.8545,\n",
       " 3439.7612,\n",
       " 3441.7017,\n",
       " 3440.4404,\n",
       " 3441.732,\n",
       " 3442.607,\n",
       " 3440.7258,\n",
       " 3440.135,\n",
       " 3437.8623,\n",
       " 3437.3772,\n",
       " 3436.5107,\n",
       " 3438.9272,\n",
       " 3442.2253,\n",
       " 3444.8137,\n",
       " 3444.296,\n",
       " 3440.2356,\n",
       " 3438.7078,\n",
       " 3439.9863,\n",
       " 3439.4897,\n",
       " 3441.5515,\n",
       " 3444.238,\n",
       " 3442.7437,\n",
       " 3445.1506,\n",
       " 3438.6624,\n",
       " 3442.8281,\n",
       " 3441.179,\n",
       " 3439.3767,\n",
       " 3439.176,\n",
       " 3440.2546,\n",
       " 3444.5188,\n",
       " 3445.761,\n",
       " 3447.8826,\n",
       " 3441.9448,\n",
       " 3443.6794,\n",
       " 3440.4412,\n",
       " 3440.5183,\n",
       " 3439.687,\n",
       " 3440.1118,\n",
       " 3439.6938,\n",
       " 3438.5898,\n",
       " 3438.55,\n",
       " 3441.354,\n",
       " 3439.536,\n",
       " 3438.6143,\n",
       " 3438.0798,\n",
       " 3440.8816,\n",
       " 3440.1104,\n",
       " 3443.245,\n",
       " 3438.9688,\n",
       " 3442.0706,\n",
       " 3438.1262,\n",
       " 3443.02,\n",
       " 3438.5535,\n",
       " 3437.455,\n",
       " 3438.7122,\n",
       " 3440.3643,\n",
       " 3438.9927,\n",
       " 3437.004,\n",
       " 3438.4707,\n",
       " 3436.1912,\n",
       " 3439.6355,\n",
       " 3436.791,\n",
       " 3437.268,\n",
       " 3439.3025,\n",
       " 3438.851,\n",
       " 3439.0461,\n",
       " 3441.066,\n",
       " 3439.4353,\n",
       " 3439.4404,\n",
       " 3439.7493,\n",
       " 3440.3816,\n",
       " 3440.0334,\n",
       " 3443.2344,\n",
       " 3441.1453,\n",
       " 3440.4585,\n",
       " 3443.6765,\n",
       " 3440.5347,\n",
       " 3441.0308,\n",
       " 3439.5605]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3712.18,\n",
       " 3742.8,\n",
       " 3758.81,\n",
       " 3805.71,\n",
       " 3813.19,\n",
       " 3883.33,\n",
       " 3818.0,\n",
       " 3782.54,\n",
       " 3782.21,\n",
       " 3796.2,\n",
       " 3789.11,\n",
       " 3792.66,\n",
       " 3730.29,\n",
       " 3683.78,\n",
       " 3693.86,\n",
       " 3702.3,\n",
       " 3697.13,\n",
       " 3727.7,\n",
       " 3737.75,\n",
       " 3719.7,\n",
       " 3730.11,\n",
       " 3710.16,\n",
       " 3699.47,\n",
       " 3706.75,\n",
       " 3928.4,\n",
       " 4031.7,\n",
       " 4051.15,\n",
       " 4039.42,\n",
       " 4051.51,\n",
       " 4009.81,\n",
       " 4019.83,\n",
       " 4032.44,\n",
       " 4074.37,\n",
       " 4090.16,\n",
       " 4056.07,\n",
       " 4048.05,\n",
       " 3940.39,\n",
       " 3943.7,\n",
       " 4042.4,\n",
       " 4070.96,\n",
       " 4040.78,\n",
       " 4097.71,\n",
       " 4056.8,\n",
       " 4018.45,\n",
       " 4025.08,\n",
       " 3930.05,\n",
       " 3973.58,\n",
       " 3990.4,\n",
       " 4034.15,\n",
       " 4000.04,\n",
       " 3984.61,\n",
       " 4016.76,\n",
       " 4038.09,\n",
       " 4043.79,\n",
       " 4035.2,\n",
       " 3993.59,\n",
       " 3971.87,\n",
       " 3956.28,\n",
       " 3920.44,\n",
       " 3862.93,\n",
       " 3806.06,\n",
       " 3819.92,\n",
       " 3835.76,\n",
       " 3832.9,\n",
       " 3846.68,\n",
       " 3820.43,\n",
       " 3825.94,\n",
       " 3830.71,\n",
       " 3808.98,\n",
       " 3799.09,\n",
       " 3835.21,\n",
       " 3842.58,\n",
       " 3844.54,\n",
       " 3890.3,\n",
       " 3905.13,\n",
       " 3876.19,\n",
       " 3850.44,\n",
       " 3835.15,\n",
       " 3817.91,\n",
       " 3830.85,\n",
       " 3845.49,\n",
       " 3838.27,\n",
       " 3856.92,\n",
       " 3870.11,\n",
       " 3884.84,\n",
       " 3900.95,\n",
       " 3849.91,\n",
       " 3962.31,\n",
       " 3982.13,\n",
       " 3994.63,\n",
       " 3978.92,\n",
       " 3998.2,\n",
       " 4006.3,\n",
       " 3994.42,\n",
       " 3999.43,\n",
       " 3974.9,\n",
       " 3976.51,\n",
       " 3986.94,\n",
       " 4016.62,\n",
       " 4018.86,\n",
       " 4009.1,\n",
       " 4010.15,\n",
       " 3982.27,\n",
       " 3926.17,\n",
       " 3950.76,\n",
       " 3953.55,\n",
       " 3970.64,\n",
       " 3968.3,\n",
       " 3944.13,\n",
       " 3961.09,\n",
       " 3934.63,\n",
       " 3941.8,\n",
       " 4085.85,\n",
       " 4181.09,\n",
       " 4166.03,\n",
       " 4173.2,\n",
       " 4166.36,\n",
       " 4204.17,\n",
       " 4194.94,\n",
       " 4193.22,\n",
       " 4202.54,\n",
       " 4203.09,\n",
       " 4132.8,\n",
       " 4131.8,\n",
       " 4127.95,\n",
       " 4140.27,\n",
       " 4171.37,\n",
       " 4173.23,\n",
       " 4070.58,\n",
       " 4084.11,\n",
       " 4063.83,\n",
       " 4014.58,\n",
       " 4031.37,\n",
       " 4042.5,\n",
       " 4058.45,\n",
       " 4045.69,\n",
       " 3862.36,\n",
       " 3824.19,\n",
       " 3826.85,\n",
       " 3778.62,\n",
       " 3756.6,\n",
       " 3788.71,\n",
       " 3776.78,\n",
       " 3791.59,\n",
       " 3758.32,\n",
       " 3766.68,\n",
       " 3765.08,\n",
       " 3741.31,\n",
       " 3757.9,\n",
       " 3765.91,\n",
       " 3752.91,\n",
       " 3720.79,\n",
       " 3746.32,\n",
       " 3710.05,\n",
       " 3718.85,\n",
       " 3736.96,\n",
       " 3782.46,\n",
       " 3779.57,\n",
       " 3799.8,\n",
       " 3755.62,\n",
       " 3771.58,\n",
       " 3821.71,\n",
       " 3840.14,\n",
       " 3838.65,\n",
       " 3820.08,\n",
       " 3828.31,\n",
       " 3815.26,\n",
       " 3778.39,\n",
       " 3758.1,\n",
       " 3749.7,\n",
       " 3738.6,\n",
       " 3747.92,\n",
       " 3753.24,\n",
       " 3771.63,\n",
       " 3718.34,\n",
       " 3729.69,\n",
       " 3743.7,\n",
       " 3752.87,\n",
       " 3753.21,\n",
       " 3796.47,\n",
       " 3788.33,\n",
       " 3766.51,\n",
       " 3808.6,\n",
       " 3812.61,\n",
       " 3797.04,\n",
       " 3787.4,\n",
       " 3744.0,\n",
       " 3753.25,\n",
       " 3745.8,\n",
       " 3720.64,\n",
       " 3739.52,\n",
       " 3741.53,\n",
       " 3748.79,\n",
       " 3742.73,\n",
       " 3742.43,\n",
       " 3738.12,\n",
       " 3750.76,\n",
       " 3759.62,\n",
       " 3746.64,\n",
       " 3731.39,\n",
       " 3750.09,\n",
       " 3745.52,\n",
       " 3741.19,\n",
       " 3622.1,\n",
       " 3590.54,\n",
       " 3593.3,\n",
       " 3586.69,\n",
       " 3582.29,\n",
       " 3604.44,\n",
       " 3599.77,\n",
       " 3599.16,\n",
       " 3600.39,\n",
       " 3592.69,\n",
       " 3597.75,\n",
       " 3587.61,\n",
       " 3604.75,\n",
       " 3609.93,\n",
       " 3617.22,\n",
       " 3612.36,\n",
       " 3627.61,\n",
       " 3613.51,\n",
       " 3607.59,\n",
       " 3607.93,\n",
       " 3734.51,\n",
       " 3860.51,\n",
       " 3847.1,\n",
       " 3859.98,\n",
       " 3902.92,\n",
       " 3883.78,\n",
       " 3899.95,\n",
       " 3884.08,\n",
       " 3875.49,\n",
       " 3899.21,\n",
       " 3895.0,\n",
       " 3881.97,\n",
       " 3885.74,\n",
       " 3878.8,\n",
       " 3880.6,\n",
       " 3883.47,\n",
       " 3895.01,\n",
       " 3896.71,\n",
       " 3921.32,\n",
       " 3855.24,\n",
       " 3862.46,\n",
       " 3877.96,\n",
       " 3874.19,\n",
       " 3869.35,\n",
       " 3873.55,\n",
       " 3831.89,\n",
       " 3834.05,\n",
       " 3832.62,\n",
       " 3831.43,\n",
       " 3855.96,\n",
       " 3898.91,\n",
       " 3904.95,\n",
       " 3814.62,\n",
       " 3724.61,\n",
       " 3729.51,\n",
       " 3738.56,\n",
       " 3733.08,\n",
       " 3742.78,\n",
       " 3753.34,\n",
       " 3755.52,\n",
       " 3756.01,\n",
       " 3768.37,\n",
       " 3822.51,\n",
       " 3828.34,\n",
       " 3830.21,\n",
       " 3796.14,\n",
       " 3803.22,\n",
       " 3800.31,\n",
       " 3811.57,\n",
       " 3802.06,\n",
       " 3814.59,\n",
       " 3810.48,\n",
       " 3799.68,\n",
       " 3813.48,\n",
       " 3822.23,\n",
       " 3816.46,\n",
       " 3823.07,\n",
       " 3823.67,\n",
       " 3800.84,\n",
       " 3778.78,\n",
       " 3777.78,\n",
       " 3785.79,\n",
       " 3778.32,\n",
       " 3784.54,\n",
       " 3784.58,\n",
       " 3774.68,\n",
       " 3790.85,\n",
       " 3797.05,\n",
       " 3740.31,\n",
       " 3737.53,\n",
       " 3755.94,\n",
       " 3763.6,\n",
       " 3763.61,\n",
       " 3758.69,\n",
       " 3749.88,\n",
       " 3749.13,\n",
       " 3691.78,\n",
       " 3676.9,\n",
       " 3673.36,\n",
       " 3668.26,\n",
       " 3688.22,\n",
       " 3692.65,\n",
       " 3689.44,\n",
       " 3674.89,\n",
       " 3681.66,\n",
       " 3685.26,\n",
       " 3686.13,\n",
       " 3691.39,\n",
       " 3705.49,\n",
       " 3710.58,\n",
       " 3712.35,\n",
       " 3705.15,\n",
       " 3711.83,\n",
       " 3707.94,\n",
       " 3721.98,\n",
       " 3698.2,\n",
       " 3681.05,\n",
       " 3686.57,\n",
       " 3684.37,\n",
       " 3656.92,\n",
       " 3679.79,\n",
       " 3719.2,\n",
       " 3722.25,\n",
       " 3716.28,\n",
       " 3799.81,\n",
       " 3608.85,\n",
       " 3601.73,\n",
       " 3607.9,\n",
       " 3621.95,\n",
       " 3624.16,\n",
       " 3617.73,\n",
       " 3611.61,\n",
       " 3610.88,\n",
       " 3613.49,\n",
       " 3611.17,\n",
       " 3613.84,\n",
       " 3615.69]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.447"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028],\n",
       "       [34.44028]], dtype=float32)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model_3.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.267, 39.267, 39.267, 39.267, 39.267, 39.267, 39.267, 39.267,\n",
       "       39.267, 40.298, 40.198, 39.728, 39.557, 39.488, 39.098, 38.947,\n",
       "       39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438,\n",
       "       39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438,\n",
       "       39.438, 39.198, 38.427, 37.898, 37.728, 37.688, 37.717, 37.908,\n",
       "       37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658,\n",
       "       37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658,\n",
       "       37.658, 37.277, 37.368, 37.807, 37.957, 37.728, 37.548, 37.747,\n",
       "       38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348,\n",
       "       38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348,\n",
       "       38.348, 38.648, 38.137, 38.097, 38.147, 38.067, 37.897, 37.917,\n",
       "       37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997,\n",
       "       37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997,\n",
       "       37.997, 36.867, 37.247, 36.857, 36.388, 36.657, 36.647, 37.018,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.438, 37.838, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 30.708, 30.892, 30.621, 30.43 , 30.048,\n",
       "       29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977,\n",
       "       29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977,\n",
       "       29.977, 29.977, 29.977, 29.117, 28.877, 28.778, 28.848, 28.848,\n",
       "       28.889, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707,\n",
       "       28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707,\n",
       "       28.707, 28.707, 28.847, 25.55 , 25.49 , 25.289, 25.489, 25.498,\n",
       "       25.497, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631,\n",
       "       26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631,\n",
       "       26.631, 26.631, 26.631, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>vix</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>2019-06-01 19:00:00</td>\n",
       "      <td>8510.17</td>\n",
       "      <td>30.447</td>\n",
       "      <td>3580.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12664</th>\n",
       "      <td>2019-06-01 20:00:00</td>\n",
       "      <td>8548.55</td>\n",
       "      <td>30.447</td>\n",
       "      <td>3578.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12665</th>\n",
       "      <td>2019-06-01 21:00:00</td>\n",
       "      <td>8582.48</td>\n",
       "      <td>30.447</td>\n",
       "      <td>3580.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12666</th>\n",
       "      <td>2019-06-01 22:00:00</td>\n",
       "      <td>8568.81</td>\n",
       "      <td>30.447</td>\n",
       "      <td>3561.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12667</th>\n",
       "      <td>2019-06-01 23:00:00</td>\n",
       "      <td>8560.63</td>\n",
       "      <td>30.447</td>\n",
       "      <td>3569.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds        y     vix     gold\n",
       "12663 2019-06-01 19:00:00  8510.17  30.447  3580.20\n",
       "12664 2019-06-01 20:00:00  8548.55  30.447  3578.11\n",
       "12665 2019-06-01 21:00:00  8582.48  30.447  3580.26\n",
       "12666 2019-06-01 22:00:00  8568.81  30.447  3561.58\n",
       "12667 2019-06-01 23:00:00  8560.63  30.447  3569.34"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
