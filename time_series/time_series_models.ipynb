{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import math\n",
    "\n",
    "# import pyflux as pf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM\n",
    "#from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2019)\n",
    "set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('./processed_data/df_combined.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>vix</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08 00:00:00</td>\n",
       "      <td>1054.03</td>\n",
       "      <td>18.879</td>\n",
       "      <td>2732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-08 01:00:00</td>\n",
       "      <td>1060.48</td>\n",
       "      <td>18.915</td>\n",
       "      <td>2693.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds        y     vix    gold\n",
       "0 2017-02-08 00:00:00  1054.03  18.879  2732.0\n",
       "1 2017-02-08 01:00:00  1060.48  18.915  2693.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_chronological(df, ratio = 0.9, use_ratio = True, index = 1000):\n",
    "    \n",
    "    '''\n",
    "    Input is a dataframe, and a ratio. Splits dataframe into 2 dataframes chronologically.\n",
    "    Returns first dataframe up to the index of the length of the input dataframe times the input ratio, \n",
    "    and returns second dataframe of remaining elements.\n",
    "    use_ratio is a flag, wether ratio should be used or indicies instead.\n",
    "    \n",
    "    df = input dataframe\n",
    "    ratio = ratio to be used for splitting\n",
    "    use_ratio = if True, use ratio, \n",
    "    index = index to split input dataframe on\n",
    "    \n",
    "    '''\n",
    "    if use_ratio:\n",
    "        size = len(df) * ratio\n",
    "        size_round = round(size)\n",
    "\n",
    "        df_train = df[0:(size_round)]\n",
    "        df_test = df[size_round:]\n",
    "    else:\n",
    "        df_train = df[0:(index)]\n",
    "        df_test = df[index:]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(train_perc, stocks_to_trade, return_df_2):\n",
    "    train_len = int(return_df.shape[0] * (train_perc))\n",
    "    volume_to_trade = ['v_'+ticker for ticker in stocks_to_trade]\n",
    "    for i in volume_to_trade:\n",
    "        stocks_to_trade.append(i)\n",
    "\n",
    "    train = return_df_2[stocks_to_trade][1:train_len].copy()\n",
    "    train['diff'] = train[stocks_to_trade[0]] - train[stocks_to_trade[1]]\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    test = return_df_2[stocks_to_trade][train_len:-400].copy()\n",
    "    test['diff'] = test[stocks_to_trade[0]] - test[stocks_to_trade[1]]\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    true_test = return_df_2[stocks_to_trade][-400:].copy()\n",
    "    true_test['diff'] = true_test[stocks_to_trade[0]] - true_test[stocks_to_trade[1]]\n",
    "    true_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    feature_names = volume_to_trade\n",
    "    feature_names.append('diff')\n",
    "    \n",
    "    return train, test, true_test, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(df, lags, look_back, predict_window):\n",
    "    \n",
    "    '''\n",
    "    In order to use keras LSTM, we need to convert the input into a keras-friendly input.\n",
    "    \n",
    "    df = input dataframe\n",
    "    lags = number of lags\n",
    "    look_back = number of preceding elements to be considered\n",
    "    predict_window = size of window for predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    features = lags + 1\n",
    "    start = look_back\n",
    "    stop = len(df) - lags - predict_window\n",
    "\n",
    "    lstm_in_X = np.zeros(shape=(stop-start, look_back+1, features))\n",
    "    lstm_in_Y = np.zeros(shape=(stop-start, look_back+1))\n",
    "\n",
    "    iter_list = [num for num in range(look_back+1)][::-1]\n",
    "    for i in range(start, stop):\n",
    "        for index, j in enumerate(iter_list):\n",
    "            X = df[i - j : i - j + lags + 1, -1]\n",
    "            lstm_in_X[i - start, index] = np.ravel(X)\n",
    "            Y = df[i - j + lags + 1, -1]\n",
    "            lstm_in_Y[i-start, index] = Y\n",
    "            \n",
    "    return lstm_in_X, lstm_in_Y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(train_X, train_Y, lags, look_back, predict_window, lstm_nodes,\n",
    "               dense_layers, dropout = 0.1, loss_type = 'hinge', optimizer_type = 'adam',\n",
    "               number_epochs = 300, batch_size = 64, ):\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_nodes, input_shape=(look_back+1, features)))\n",
    "    model.add(Dropout(dropout))\n",
    "    for nodes in dense_layers:\n",
    "        model.add(Dense(nodes))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(look_back + 1))\n",
    "    model.compile(loss=loss_type, optimizer= optimizer_type)\n",
    "    model.fit(train_X, train_Y, epochs=number_epochs, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "    pred_Y_train = model.predict(train_X)\n",
    "    predictions = pred_Y_train[:,-1]\n",
    "    actuals = train_Y[:,-1]\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model, dataset, train_X, train_Y, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "#                     validation_data=(test_x[i], test_y[i]),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        test_x = np.concatenate((test_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "        le = len(test_y) + 1\n",
    "        test_y = np.concatenate((test_y, (test_y[i].reshape(1 ,))))\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_x[-1,-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = split_train_test_chronological(df_combined, ratio = .99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = split_train_test_chronological(df_combined, ratio = .95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var2(t-7)</th>\n",
       "      <th>var3(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var2(t-6)</th>\n",
       "      <th>var3(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var2(t-5)</th>\n",
       "      <th>var3(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>var3(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var2(t-2)</th>\n",
       "      <th>var3(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.179461</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-7)  var2(t-7)  var3(t-7)  var1(t-6)  var2(t-6)  var3(t-6)  \\\n",
       "7    0.005247   0.179461   0.003901   0.005594   0.180275   0.001623   \n",
       "8    0.005594   0.180275   0.001623   0.005727   0.179032   0.002740   \n",
       "9    0.005727   0.179032   0.002740   0.005882   0.176997   0.004602   \n",
       "10   0.005882   0.176997   0.004602   0.006007   0.176997   0.004491   \n",
       "11   0.006007   0.176997   0.004491   0.005908   0.179438   0.004092   \n",
       "\n",
       "    var1(t-5)  var2(t-5)  var3(t-5)  var1(t-4)  ...  var3(t-3)  var1(t-2)  \\\n",
       "7    0.005727   0.179032   0.002740   0.005882  ...   0.004491   0.005908   \n",
       "8    0.005882   0.176997   0.004602   0.006007  ...   0.004092   0.005895   \n",
       "9    0.006007   0.176997   0.004491   0.005908  ...   0.003908   0.005905   \n",
       "10   0.005908   0.179438   0.004092   0.005895  ...   0.003899   0.004164   \n",
       "11   0.005895   0.179642   0.003908   0.005905  ...   0.002629   0.004071   \n",
       "\n",
       "    var2(t-2)  var3(t-2)  var1(t-1)  var2(t-1)  var3(t-1)   var1(t)   var2(t)  \\\n",
       "7    0.179438   0.004092   0.005895   0.179642   0.003908  0.005905  0.179642   \n",
       "8    0.179642   0.003908   0.005905   0.179642   0.003899  0.004164  0.179642   \n",
       "9    0.179642   0.003899   0.004164   0.179642   0.002629  0.004071  0.179642   \n",
       "10   0.179642   0.002629   0.004071   0.179642   0.003160  0.004373  0.179642   \n",
       "11   0.179642   0.003160   0.004373   0.179642   0.003959  0.004777  0.179642   \n",
       "\n",
       "     var3(t)  \n",
       "7   0.003899  \n",
       "8   0.002629  \n",
       "9   0.003160  \n",
       "10  0.003959  \n",
       "11  0.003199  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "values = df_combined.drop('ds', axis = 1).values\n",
    "# integer encode direction\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 7, 1)\n",
    "\n",
    "reframed_2 = series_to_supervised(values, 7, 1)\n",
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed.iloc[:,-11].values\n",
    "X = reframed.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed_2.iloc[:,-11].values\n",
    "X = reframed_2.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_perc = 0.40\n",
    "# lags = 50\n",
    "# look_back = 6       # 0 is a look_back of 1, 1 is a look_back of 2, etc.\n",
    "# predict_window = 1\n",
    "\n",
    "# # Neural net parameters\n",
    "# lstm_neurons = 16\n",
    "# dense_layers = []\n",
    "\n",
    "# model, predictions_test, actuals_test, predictions_train, actuals_train = main(df_train, df_val, train_perc,\n",
    "#                                                                                lags, look_back, \n",
    "#                                                                                predict_window, \n",
    "#                                                                                lstm_neurons, dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00524709, 0.17946108, 0.00390136, ..., 0.00390837,\n",
       "         0.17964192, 0.00389902]],\n",
       "\n",
       "       [[0.00559421, 0.18027489, 0.00162329, ..., 0.00389902,\n",
       "         0.17964192, 0.00262855]],\n",
       "\n",
       "       [[0.00572659, 0.1790316 , 0.00274013, ..., 0.00262855,\n",
       "         0.17964192, 0.0031601 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.14719364, 0.6875989 , 0.06145546, ..., 0.06514594,\n",
       "         0.71517694, 0.06405188]],\n",
       "\n",
       "       [[0.14818656, 0.6857679 , 0.06328142, ..., 0.06405188,\n",
       "         0.71517694, 0.06482349]],\n",
       "\n",
       "       [[0.1501584 , 0.67446536, 0.06471661, ..., 0.06482349,\n",
       "         0.71517694, 0.06527327]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14945447, 0.7185677 , 0.06664537, 0.14903203, 0.71517694,\n",
       "        0.06430598, 0.14952445, 0.71517694, 0.06589244, 0.14759782,\n",
       "        0.71517694, 0.06514594, 0.14829904, 0.71517694, 0.06405188,\n",
       "        0.1499469 , 0.71517694, 0.06482349, 0.15080851, 0.71517694,\n",
       "        0.06527327, 0.71517694, 0.06773242]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_3 = Sequential()\n",
    "LSTM_model_3.add(LSTM(64, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), dropout=0.05,recurrent_dropout=0.05))\n",
    "LSTM_model_3.add(Dense(64))\n",
    "# LSTM_model_3.add(LSTM(16, activation='relu'))\n",
    "LSTM_model_3.add(Dense(1))\n",
    "LSTM_model_3.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 341 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 79.1496 - val_loss: 178.8948\n",
      "Epoch 2/100\n",
      " - 2s - loss: 81.0312 - val_loss: 178.0877\n",
      "Epoch 3/100\n",
      " - 2s - loss: 81.0999 - val_loss: 177.9849\n",
      "Epoch 4/100\n",
      " - 2s - loss: 81.1355 - val_loss: 177.9613\n",
      "Epoch 5/100\n",
      " - 2s - loss: 81.1654 - val_loss: 177.9484\n",
      "Epoch 6/100\n",
      " - 2s - loss: 83.4045 - val_loss: 178.3477\n",
      "Epoch 7/100\n",
      " - 2s - loss: 81.1642 - val_loss: 177.9843\n",
      "Epoch 8/100\n",
      " - 2s - loss: 81.2057 - val_loss: 177.9368\n",
      "Epoch 9/100\n",
      " - 2s - loss: 81.2314 - val_loss: 177.9236\n",
      "Epoch 10/100\n",
      " - 2s - loss: 81.2536 - val_loss: 177.9150\n",
      "Epoch 11/100\n",
      " - 2s - loss: 81.2735 - val_loss: 177.9084\n",
      "Epoch 12/100\n",
      " - 2s - loss: 81.3353 - val_loss: 177.9114\n",
      "Epoch 13/100\n",
      " - 2s - loss: 81.3083 - val_loss: 177.8968\n",
      "Epoch 14/100\n",
      " - 2s - loss: 81.3240 - val_loss: 177.8909\n",
      "Epoch 15/100\n",
      " - 2s - loss: 81.3378 - val_loss: 177.8867\n",
      "Epoch 16/100\n",
      " - 2s - loss: 81.3503 - val_loss: 177.8829\n",
      "Epoch 17/100\n",
      " - 2s - loss: 81.3616 - val_loss: 177.8795\n",
      "Epoch 18/100\n",
      " - 2s - loss: 81.3717 - val_loss: 177.8766\n",
      "Epoch 19/100\n",
      " - 2s - loss: 81.3807 - val_loss: 177.8742\n",
      "Epoch 20/100\n",
      " - 2s - loss: 81.3888 - val_loss: 177.8720\n",
      "Epoch 21/100\n",
      " - 2s - loss: 81.3960 - val_loss: 177.8702\n",
      "Epoch 22/100\n",
      " - 2s - loss: 81.4023 - val_loss: 177.8686\n",
      "Epoch 23/100\n",
      " - 2s - loss: 81.4079 - val_loss: 177.8673\n",
      "Epoch 24/100\n",
      " - 2s - loss: 81.4128 - val_loss: 177.8661\n",
      "Epoch 25/100\n",
      " - 2s - loss: 81.4170 - val_loss: 177.8653\n",
      "Epoch 26/100\n",
      " - 2s - loss: 81.4207 - val_loss: 177.8645\n",
      "Epoch 27/100\n",
      " - 2s - loss: 81.5203 - val_loss: 178.0117\n",
      "Epoch 28/100\n",
      " - 2s - loss: 81.4202 - val_loss: 177.8790\n",
      "Epoch 29/100\n",
      " - 2s - loss: 81.4279 - val_loss: 177.8646\n",
      "Epoch 30/100\n",
      " - 2s - loss: 81.4302 - val_loss: 177.8633\n",
      "Epoch 31/100\n",
      " - 2s - loss: 81.4317 - val_loss: 177.8630\n",
      "Epoch 32/100\n",
      " - 2s - loss: 81.4327 - val_loss: 177.8631\n",
      "Epoch 33/100\n",
      " - 2s - loss: 81.4334 - val_loss: 177.8633\n",
      "Epoch 34/100\n",
      " - 2s - loss: 81.4371 - val_loss: 177.8644\n",
      "Epoch 35/100\n",
      " - 2s - loss: 81.4339 - val_loss: 177.8638\n",
      "Epoch 36/100\n",
      " - 2s - loss: 81.4339 - val_loss: 177.8639\n",
      "Epoch 37/100\n",
      " - 2s - loss: 81.4336 - val_loss: 177.8644\n",
      "Epoch 38/100\n",
      " - 2s - loss: 81.4331 - val_loss: 177.8648\n",
      "Epoch 39/100\n",
      " - 2s - loss: 81.4325 - val_loss: 177.8653\n",
      "Epoch 40/100\n",
      " - 2s - loss: 81.4316 - val_loss: 177.8659\n",
      "Epoch 41/100\n",
      " - 2s - loss: 81.4307 - val_loss: 177.8665\n",
      "Epoch 42/100\n",
      " - 2s - loss: 81.4296 - val_loss: 177.8671\n",
      "Epoch 43/100\n",
      " - 2s - loss: 81.4283 - val_loss: 177.8678\n",
      "Epoch 44/100\n",
      " - 2s - loss: 81.4270 - val_loss: 177.8685\n",
      "Epoch 45/100\n",
      " - 2s - loss: 81.4255 - val_loss: 177.8693\n",
      "Epoch 46/100\n",
      " - 2s - loss: 81.4240 - val_loss: 177.8700\n",
      "Epoch 47/100\n",
      " - 2s - loss: 81.4502 - val_loss: 177.8371\n",
      "Epoch 48/100\n",
      " - 2s - loss: 81.4220 - val_loss: 177.8679\n",
      "Epoch 49/100\n",
      " - 2s - loss: 81.4191 - val_loss: 177.8721\n",
      "Epoch 50/100\n",
      " - 2s - loss: 81.4171 - val_loss: 177.8732\n",
      "Epoch 51/100\n",
      " - 2s - loss: 81.4152 - val_loss: 177.8741\n",
      "Epoch 52/100\n",
      " - 2s - loss: 86.1017 - val_loss: 181.0606\n",
      "Epoch 53/100\n",
      " - 2s - loss: 81.0953 - val_loss: 178.3688\n",
      "Epoch 54/100\n",
      " - 2s - loss: 81.2014 - val_loss: 177.9861\n",
      "Epoch 55/100\n",
      " - 2s - loss: 81.2323 - val_loss: 177.9418\n",
      "Epoch 56/100\n",
      " - 2s - loss: 81.2469 - val_loss: 177.9330\n",
      "Epoch 57/100\n",
      " - 2s - loss: 81.3333 - val_loss: 177.9425\n",
      "Epoch 58/100\n",
      " - 2s - loss: 81.2685 - val_loss: 177.9261\n",
      "Epoch 59/100\n",
      " - 2s - loss: 81.2786 - val_loss: 177.9214\n",
      "Epoch 60/100\n",
      " - 2s - loss: 81.2872 - val_loss: 177.9184\n",
      "Epoch 61/100\n",
      " - 2s - loss: 81.2949 - val_loss: 177.9159\n",
      "Epoch 62/100\n",
      " - 2s - loss: 81.3017 - val_loss: 177.9137\n",
      "Epoch 63/100\n",
      " - 2s - loss: 81.3077 - val_loss: 177.9119\n",
      "Epoch 64/100\n",
      " - 2s - loss: 81.3130 - val_loss: 177.9104\n",
      "Epoch 65/100\n",
      " - 2s - loss: 81.3176 - val_loss: 177.9092\n",
      "Epoch 66/100\n",
      " - 2s - loss: 81.3217 - val_loss: 177.9081\n",
      "Epoch 67/100\n",
      " - 2s - loss: 81.3252 - val_loss: 177.9072\n",
      "Epoch 68/100\n",
      " - 2s - loss: 81.3281 - val_loss: 177.9064\n",
      "Epoch 69/100\n",
      " - 2s - loss: 81.3306 - val_loss: 177.9059\n",
      "Epoch 70/100\n",
      " - 2s - loss: 81.3327 - val_loss: 177.9056\n",
      "Epoch 71/100\n",
      " - 2s - loss: 81.3343 - val_loss: 177.9053\n",
      "Epoch 72/100\n",
      " - 2s - loss: 81.3356 - val_loss: 177.9052\n",
      "Epoch 73/100\n",
      " - 2s - loss: 81.3366 - val_loss: 177.9052\n",
      "Epoch 74/100\n",
      " - 2s - loss: 95.5954 - val_loss: 184.8038\n",
      "Epoch 75/100\n",
      " - 2s - loss: 80.7340 - val_loss: 178.8449\n",
      "Epoch 76/100\n",
      " - 2s - loss: 81.0097 - val_loss: 178.1839\n",
      "Epoch 77/100\n",
      " - 2s - loss: 81.0471 - val_loss: 178.0396\n",
      "Epoch 78/100\n",
      " - 2s - loss: 81.0748 - val_loss: 178.0136\n",
      "Epoch 79/100\n",
      " - 2s - loss: 81.0959 - val_loss: 178.0023\n",
      "Epoch 80/100\n",
      " - 2s - loss: 81.1148 - val_loss: 177.9936\n",
      "Epoch 81/100\n",
      " - 2s - loss: 81.1322 - val_loss: 177.9858\n",
      "Epoch 82/100\n",
      " - 2s - loss: 81.1483 - val_loss: 177.9790\n",
      "Epoch 83/100\n",
      " - 2s - loss: 81.1630 - val_loss: 177.9729\n",
      "Epoch 84/100\n",
      " - 2s - loss: 81.1765 - val_loss: 177.9673\n",
      "Epoch 85/100\n",
      " - 2s - loss: 81.1888 - val_loss: 177.9626\n",
      "Epoch 86/100\n",
      " - 2s - loss: 81.1999 - val_loss: 177.9583\n",
      "Epoch 87/100\n",
      " - 2s - loss: 81.2101 - val_loss: 177.9545\n",
      "Epoch 88/100\n",
      " - 2s - loss: 81.2192 - val_loss: 177.9510\n",
      "Epoch 89/100\n",
      " - 2s - loss: 81.2275 - val_loss: 177.9483\n",
      "Epoch 90/100\n",
      " - 2s - loss: 81.2348 - val_loss: 177.9457\n",
      "Epoch 91/100\n",
      " - 2s - loss: 81.2414 - val_loss: 177.9434\n",
      "Epoch 92/100\n",
      " - 2s - loss: 81.2473 - val_loss: 177.9415\n",
      "Epoch 93/100\n",
      " - 2s - loss: 81.2525 - val_loss: 177.9401\n",
      "Epoch 94/100\n",
      " - 2s - loss: 81.2570 - val_loss: 177.9386\n",
      "Epoch 95/100\n",
      " - 2s - loss: 81.2609 - val_loss: 177.9377\n",
      "Epoch 96/100\n",
      " - 2s - loss: 81.2643 - val_loss: 177.9368\n",
      "Epoch 97/100\n",
      " - 2s - loss: 81.2672 - val_loss: 177.9359\n",
      "Epoch 98/100\n",
      " - 2s - loss: 81.2696 - val_loss: 177.9354\n",
      "Epoch 99/100\n",
      " - 2s - loss: 81.2717 - val_loss: 177.9351\n",
      "Epoch 100/100\n",
      " - 2s - loss: 81.2733 - val_loss: 177.9347\n"
     ]
    }
   ],
   "source": [
    "history_3 = LSTM_model_3.fit(train_X, train_y, \n",
    "                    epochs= 100, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(val_X, val_y),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model, train_x, train_y, test_x, test_y):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(test_x[i].reshape(1,1,23), (test_y[i].reshape(1 ,))),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        train_x = np.concatenate((train_x, (test_x[i].reshape(1 , 1 , 23))))\n",
    "        le = len(test_y) + 1\n",
    "        train_y = np.concatenate((train_y, (test_y[i].reshape(1 ,))))\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_x[-1,-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.6118 - val_loss: 72.0893\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.6158 - val_loss: 72.1114\n",
      "Train on 12001 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.6120 - val_loss: 72.1152\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.6098 - val_loss: 72.1161\n",
      "Train on 12002 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.6073 - val_loss: 72.1169\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.6053 - val_loss: 72.1174\n",
      "Train on 12003 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.6028 - val_loss: 72.1181\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.6008 - val_loss: 72.1187\n",
      "Train on 12004 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5983 - val_loss: 72.1195\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5962 - val_loss: 72.1200\n",
      "Train on 12005 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5937 - val_loss: 72.1208\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5916 - val_loss: 72.1214\n",
      "Train on 12006 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5891 - val_loss: 72.1222\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5870 - val_loss: 72.1228\n",
      "Train on 12007 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5845 - val_loss: 72.1236\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5824 - val_loss: 72.1242\n",
      "Train on 12008 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5798 - val_loss: 72.1250\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5777 - val_loss: 72.1255\n",
      "Train on 12009 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5752 - val_loss: 72.1264\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5731 - val_loss: 72.1270\n",
      "Train on 12010 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5705 - val_loss: 72.1279\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5684 - val_loss: 72.1286\n",
      "Train on 12011 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5658 - val_loss: 72.1294\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5637 - val_loss: 72.1300\n",
      "Train on 12012 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5611 - val_loss: 72.1309\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5590 - val_loss: 72.1316\n",
      "Train on 12013 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5564 - val_loss: 72.1325\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5543 - val_loss: 72.1331\n",
      "Train on 12014 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5518 - val_loss: 92.2018\n",
      "Epoch 2/2\n",
      " - 1s - loss: 80.5497 - val_loss: 92.2025\n",
      "Train on 12015 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5488 - val_loss: 88.6093\n",
      "Epoch 2/2\n",
      " - 1s - loss: 80.5465 - val_loss: 88.6093\n",
      "Train on 12016 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 1s - loss: 80.5453 - val_loss: 84.8847\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5431 - val_loss: 84.8848\n",
      "Train on 12017 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5416 - val_loss: 120.8533\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5394 - val_loss: 120.8538\n",
      "Train on 12018 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5409 - val_loss: 116.0437\n",
      "Epoch 2/2\n",
      " - 1s - loss: 80.5384 - val_loss: 116.0428\n",
      "Train on 12019 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5395 - val_loss: 135.1018\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5371 - val_loss: 135.1010\n",
      "Train on 12020 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5398 - val_loss: 114.1327\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5373 - val_loss: 114.1314\n",
      "Train on 12021 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5383 - val_loss: 113.4914\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5359 - val_loss: 113.4909\n",
      "Train on 12022 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5368 - val_loss: 113.4913\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5344 - val_loss: 113.4907\n",
      "Train on 12023 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 80.5353 - val_loss: 113.4911\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5329 - val_loss: 113.4906\n",
      "Train on 12024 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.5338 - val_loss: 113.4910\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5314 - val_loss: 113.4905\n",
      "Train on 12025 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.5323 - val_loss: 113.4910\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5299 - val_loss: 113.4905\n",
      "Train on 12026 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.5308 - val_loss: 113.4910\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5285 - val_loss: 113.4905\n",
      "Train on 12027 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.5294 - val_loss: 113.4910\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5270 - val_loss: 113.4906\n",
      "Train on 12028 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      " - 4s - loss: 80.5279 - val_loss: 113.4911\n",
      "Epoch 2/2\n",
      " - 2s - loss: 80.5255 - val_loss: 113.4908\n",
      "Train on 12029 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-e08a8ef3b70e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_refitting_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_model_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-170-84a8d27b7c89>\u001b[0m in \u001b[0;36mmake_refitting_window\u001b[0;34m(model, train_x, train_y, test_x, test_y)\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                     callbacks=[earlystopper],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     shuffle=False)\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred_Y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/plaidml/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/plaidml/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/plaidml/__init__.py\u001b[0m in \u001b[0;36mas_ndarray\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                 dtype=_NP_TYPES[self.shape.dtype])\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m             \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/plaidml/__init__.py\u001b[0m in \u001b[0;36mmmap_current\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmmap_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         mapping = _lib().plaidml_map_buffer_current(self.buffer,\n\u001b[0;32m-> 1232\u001b[0;31m                                                     ctypes.cast(None, _MAP_BUFFER_FUNCTYPE), None)\n\u001b[0m\u001b[1;32m   1233\u001b[0m         yield _View(self.buffer._ctx, mapping, self.shape.dtype, self.shape.ctype,\n\u001b[1;32m   1234\u001b[0m                     _lib().plaidml_get_shape_element_count(self.shape), self.shape, None)\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/plaidml/__init__.py\u001b[0m in \u001b[0;36m_check_err\u001b[0;34m(self, result, func, args)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaidml_compute_grad_wrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_err\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = make_refitting_window(LSTM_model_3, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.267"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-ad7d0eaba52c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "test_y.append(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.267, 39.267, 39.267, 39.267, 39.267, 39.267, 39.267, 39.267,\n",
       "       39.267, 40.298, 40.198, 39.728, 39.557, 39.488, 39.098, 38.947,\n",
       "       39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438,\n",
       "       39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438,\n",
       "       39.438, 39.198, 38.427, 37.898, 37.728, 37.688, 37.717, 37.908,\n",
       "       37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658,\n",
       "       37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658,\n",
       "       37.658, 37.277, 37.368, 37.807, 37.957, 37.728, 37.548, 37.747,\n",
       "       38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348,\n",
       "       38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348,\n",
       "       38.348, 38.648, 38.137, 38.097, 38.147, 38.067, 37.897, 37.917,\n",
       "       37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997,\n",
       "       37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997,\n",
       "       37.997, 36.867, 37.247, 36.857, 36.388, 36.657, 36.647, 37.018,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.438, 37.838, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 30.708, 30.892, 30.621, 30.43 , 30.048,\n",
       "       29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977,\n",
       "       29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977,\n",
       "       29.977, 29.977, 29.977, 29.117, 28.877, 28.778, 28.848, 28.848,\n",
       "       28.889, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707,\n",
       "       28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707,\n",
       "       28.707, 28.707, 28.847, 25.55 , 25.49 , 25.289, 25.489, 25.498,\n",
       "       25.497, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631,\n",
       "       26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631,\n",
       "       26.631, 26.631, 26.631, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       39.267], dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((test_y, (test_y[0].reshape(1 ,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.267, 39.267, 39.267, 39.267, 39.267, 39.267, 39.267, 39.267,\n",
       "       39.267, 40.298, 40.198, 39.728, 39.557, 39.488, 39.098, 38.947,\n",
       "       39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438,\n",
       "       39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438, 39.438,\n",
       "       39.438, 39.198, 38.427, 37.898, 37.728, 37.688, 37.717, 37.908,\n",
       "       37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658,\n",
       "       37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658, 37.658,\n",
       "       37.658, 37.277, 37.368, 37.807, 37.957, 37.728, 37.548, 37.747,\n",
       "       38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348,\n",
       "       38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348, 38.348,\n",
       "       38.348, 38.648, 38.137, 38.097, 38.147, 38.067, 37.897, 37.917,\n",
       "       37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997,\n",
       "       37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997, 37.997,\n",
       "       37.997, 36.867, 37.247, 36.857, 36.388, 36.657, 36.647, 37.018,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038, 37.038,\n",
       "       37.038, 37.438, 37.838, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667, 38.667,\n",
       "       38.667, 38.667, 38.667, 30.708, 30.892, 30.621, 30.43 , 30.048,\n",
       "       29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977,\n",
       "       29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977, 29.977,\n",
       "       29.977, 29.977, 29.977, 29.117, 28.877, 28.778, 28.848, 28.848,\n",
       "       28.889, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707,\n",
       "       28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707, 28.707,\n",
       "       28.707, 28.707, 28.847, 25.55 , 25.49 , 25.289, 25.489, 25.498,\n",
       "       25.497, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631,\n",
       "       26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631, 26.631,\n",
       "       26.631, 26.631, 26.631, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447,\n",
       "       30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447, 30.447],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 39.267,\n",
       " 40.298,\n",
       " 40.198,\n",
       " 39.728,\n",
       " 39.557,\n",
       " 39.488,\n",
       " 39.098,\n",
       " 38.947,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.438,\n",
       " 39.198,\n",
       " 38.427,\n",
       " 37.898,\n",
       " 37.728,\n",
       " 37.688,\n",
       " 37.717,\n",
       " 37.908,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.658,\n",
       " 37.277,\n",
       " 37.368,\n",
       " 37.807,\n",
       " 37.957,\n",
       " 37.728,\n",
       " 37.548,\n",
       " 37.747,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.348,\n",
       " 38.648,\n",
       " 38.137,\n",
       " 38.097,\n",
       " 38.147,\n",
       " 38.067,\n",
       " 37.897,\n",
       " 37.917,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 37.997,\n",
       " 36.867,\n",
       " 37.247,\n",
       " 36.857,\n",
       " 36.388,\n",
       " 36.657,\n",
       " 36.647,\n",
       " 37.018,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.038,\n",
       " 37.438,\n",
       " 37.838,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 38.667,\n",
       " 30.708,\n",
       " 30.892,\n",
       " 30.621,\n",
       " 30.43,\n",
       " 30.048,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.977,\n",
       " 29.117,\n",
       " 28.877,\n",
       " 28.778,\n",
       " 28.848,\n",
       " 28.848,\n",
       " 28.889,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.707,\n",
       " 28.847,\n",
       " 25.55,\n",
       " 25.49,\n",
       " 25.289,\n",
       " 25.489,\n",
       " 25.498,\n",
       " 25.497,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 26.631,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447,\n",
       " 30.447]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
