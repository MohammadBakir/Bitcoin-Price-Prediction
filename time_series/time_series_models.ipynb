{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import math\n",
    "\n",
    "# import pyflux as pf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM\n",
    "#from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2019)\n",
    "set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('./processed_data/df_combined.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>vix</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08 00:00:00</td>\n",
       "      <td>1054.03</td>\n",
       "      <td>18.879</td>\n",
       "      <td>2732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-08 01:00:00</td>\n",
       "      <td>1060.48</td>\n",
       "      <td>18.915</td>\n",
       "      <td>2693.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds        y     vix    gold\n",
       "0 2017-02-08 00:00:00  1054.03  18.879  2732.0\n",
       "1 2017-02-08 01:00:00  1060.48  18.915  2693.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_chronological(df, ratio = 0.9, use_ratio = True, index = 1000):\n",
    "    \n",
    "    '''\n",
    "    Input is a dataframe, and a ratio. Splits dataframe into 2 dataframes chronologically.\n",
    "    Returns first dataframe up to the index of the length of the input dataframe times the input ratio, \n",
    "    and returns second dataframe of remaining elements.\n",
    "    use_ratio is a flag, wether ratio should be used or indicies instead.\n",
    "    \n",
    "    df = input dataframe\n",
    "    ratio = ratio to be used for splitting\n",
    "    use_ratio = if True, use ratio, \n",
    "    index = index to split input dataframe on\n",
    "    \n",
    "    '''\n",
    "    if use_ratio:\n",
    "        size = len(df) * ratio\n",
    "        size_round = round(size)\n",
    "\n",
    "        df_train = df[0:(size_round)]\n",
    "        df_test = df[size_round:]\n",
    "    else:\n",
    "        df_train = df[0:(index)]\n",
    "        df_test = df[index:]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(train_perc, stocks_to_trade, return_df_2):\n",
    "    train_len = int(return_df.shape[0] * (train_perc))\n",
    "    volume_to_trade = ['v_'+ticker for ticker in stocks_to_trade]\n",
    "    for i in volume_to_trade:\n",
    "        stocks_to_trade.append(i)\n",
    "\n",
    "    train = return_df_2[stocks_to_trade][1:train_len].copy()\n",
    "    train['diff'] = train[stocks_to_trade[0]] - train[stocks_to_trade[1]]\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    test = return_df_2[stocks_to_trade][train_len:-400].copy()\n",
    "    test['diff'] = test[stocks_to_trade[0]] - test[stocks_to_trade[1]]\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    true_test = return_df_2[stocks_to_trade][-400:].copy()\n",
    "    true_test['diff'] = true_test[stocks_to_trade[0]] - true_test[stocks_to_trade[1]]\n",
    "    true_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    feature_names = volume_to_trade\n",
    "    feature_names.append('diff')\n",
    "    \n",
    "    return train, test, true_test, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(df, lags, look_back, predict_window):\n",
    "    \n",
    "    '''\n",
    "    In order to use keras LSTM, we need to convert the input into a keras-friendly input.\n",
    "    \n",
    "    df = input dataframe\n",
    "    lags = number of lags\n",
    "    look_back = number of preceding elements to be considered\n",
    "    predict_window = size of window for predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    features = lags + 1\n",
    "    start = look_back\n",
    "    stop = len(df) - lags - predict_window\n",
    "\n",
    "    lstm_in_X = np.zeros(shape=(stop-start, look_back+1, features))\n",
    "    lstm_in_Y = np.zeros(shape=(stop-start, look_back+1))\n",
    "\n",
    "    iter_list = [num for num in range(look_back+1)][::-1]\n",
    "    for i in range(start, stop):\n",
    "        for index, j in enumerate(iter_list):\n",
    "            X = df[i - j : i - j + lags + 1, -1]\n",
    "            lstm_in_X[i - start, index] = np.ravel(X)\n",
    "            Y = df[i - j + lags + 1, -1]\n",
    "            lstm_in_Y[i-start, index] = Y\n",
    "            \n",
    "    return lstm_in_X, lstm_in_Y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_lstm(train, feature_names, lags, look_back, predict_window, lstm_nodes,\n",
    "#                dense_layers, dropout = 0.1, loss_type = 'hinge', optimizer_type = 'adam',\n",
    "#                number_epochs = 300, batch_size = 64, ):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     dataset = np.matrix(train[feature_names])\n",
    "#     lstm_in_X, lstm_in_Y, features = make_variables(dataset, lags, look_back, predict_window)\n",
    "#     train_X = lstm_in_X\n",
    "#     train_Y = lstm_in_Y\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(lstm_nodes, input_shape=(look_back+1, features)))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     for nodes in dense_layers:\n",
    "#         model.add(Dense(nodes))\n",
    "#         model.add(Dropout(dropout))\n",
    "#     model.add(Dense(look_back + 1))\n",
    "#     model.compile(loss=loss_type, optimizer= optimizer_type)\n",
    "#     model.fit(train_X, train_Y, epochs=number_epochs, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "#     pred_Y_train = model.predict(train_X)\n",
    "#     predictions = pred_Y_train[:,-1]\n",
    "#     actuals = train_Y[:,-1]\n",
    "    \n",
    "#     print(model.summary())\n",
    "    \n",
    "#     return model, dataset, train_X, train_Y, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(train_X, train_Y, lags, look_back, predict_window, lstm_nodes,\n",
    "               dense_layers, dropout = 0.1, loss_type = 'hinge', optimizer_type = 'adam',\n",
    "               number_epochs = 300, batch_size = 64, ):\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_nodes, input_shape=(look_back+1, features)))\n",
    "    model.add(Dropout(dropout))\n",
    "    for nodes in dense_layers:\n",
    "        model.add(Dense(nodes))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(look_back + 1))\n",
    "    model.compile(loss=loss_type, optimizer= optimizer_type)\n",
    "    model.fit(train_X, train_Y, epochs=number_epochs, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "    pred_Y_train = model.predict(train_X)\n",
    "    predictions = pred_Y_train[:,-1]\n",
    "    actuals = train_Y[:,-1]\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model, dataset, train_X, train_Y, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model_test, dataset,  lags, look_back, predict_window, expand_set ,):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    print(expand_set.shape[0] - 1)\n",
    "\n",
    "    for i in range(0, expand_set.shape[0] - 1):\n",
    "        print(i)\n",
    "        curr_row = expand_set[i]\n",
    "        test_row = expand_set[i+1]\n",
    "        dataset = np.append(dataset, curr_row, axis=0)\n",
    "        lstm_in_X, lstm_in_Y, features = make_variables(dataset, lags, look_back, predict_window)\n",
    "        train_X = lstm_in_X\n",
    "        train_Y = lstm_in_Y\n",
    "    \n",
    "        # Fit the data all the way up to curr_row (today)\n",
    "        if i%1 == 0:\n",
    "            model_test.fit(train_X, train_Y, epochs=10, batch_size=25, verbose=1)\n",
    "    \n",
    "        # Predict the next day (out of sample) - data for next day is in test_row\n",
    "        dataset_test = np.append(dataset, test_row, axis=0)\n",
    "        lstm_in_X_test, lstm_in_Y_test, features = make_variables(dataset_test, lags, look_back, predict_window)\n",
    "        test_X = lstm_in_X_test\n",
    "        pred_Y_test = model_test.predict(test_X)\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = test_row[-1,-1]\n",
    "        # Store predictions and actuals to for calculating money made and plotting\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model_test, dataset, expand_set, feature_names, lags, look_back, predict_window):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    print(expand_set.shape[0] - 1)\n",
    "\n",
    "    for i in range(0, expand_set.shape[0] - 1):\n",
    "        print(i)\n",
    "        curr_row = expand_set[i]\n",
    "        test_row = expand_set[i+1]\n",
    "        dataset = np.append(dataset, curr_row, axis=0)\n",
    "        lstm_in_X, lstm_in_Y, features = make_variables(dataset, lags, look_back, predict_window)\n",
    "        train_X = lstm_in_X\n",
    "        train_Y = lstm_in_Y\n",
    "    \n",
    "        # Fit the data all the way up to curr_row (today)\n",
    "        if i%1 == 0:\n",
    "            model_test.fit(train_X, train_Y, epochs=10, batch_size=25, verbose=1)\n",
    "    \n",
    "        # Predict the next day (out of sample) - data for next day is in test_row\n",
    "        dataset_test = np.append(dataset, test_row, axis=0)\n",
    "        lstm_in_X_test, lstm_in_Y_test, features = make_variables(dataset_test, lags, look_back, predict_window)\n",
    "        test_X = lstm_in_X_test\n",
    "        pred_Y_test = model_test.predict(test_X)\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = test_row[-1,-1]\n",
    "        # Store predictions and actuals to for calculating money made and plotting\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refitting_window(model, train_x, train_y, test_x, test_y):\n",
    "    predictions_test = []\n",
    "    actuals_test = []\n",
    "    \n",
    "    for i in range(0, len(test_y) - 1):\n",
    "        model.fit(train_x, train_y, \n",
    "                    epochs= 2, \n",
    "                    batch_size = 128, \n",
    "#                     validation_data=(test_x[i], test_y[i]),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)\n",
    "        pred_Y_test = model.predict(test_x)\n",
    "        train_x.append(test_x[i])\n",
    "        train_y.append(test_y[i])\n",
    "        predict_test = pred_Y_test[-1,-1]\n",
    "        actual_test = train_x[-1,-1]\n",
    "        predictions_test.append(predict_test)\n",
    "        actuals_test.append(actual_test)\n",
    "        \n",
    "    \n",
    "    return predictions_test, actuals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train, test, train_perc, lags, look_back, predict_window, \n",
    "         lstm_neurons, dense_layers):\n",
    "    \n",
    "#     stocks_to_trade = tickers\n",
    "#     return_df_2 = pd.concat([return_df, volume_df], axis=1)\n",
    "#     train, test, true_test, feature_names = slice_data(train_perc, stocks_to_trade, return_df_2)\n",
    "#     expand_set = np.matrix(test['y'])\n",
    "    model_train, dataset, train_X, train_Y, predictions_train, actuals_train = train_lstm(train, 'y', \n",
    "                                                                                          lags, look_back, \n",
    "                                                                                          predict_window,\n",
    "                                                                                          lstm_neurons, dense_layers\n",
    "                                                                                         )\n",
    "    model_test = model_train\n",
    "    predictions_test, actuals_test = make_refitting_window(model_test, dataset, expand_set, feature_names, lags, look_back, predict_window)\n",
    "    \n",
    "    return model_test, predictions_test, actuals_test, predictions_train, actuals_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = split_train_test_chronological(df_combined, ratio = .99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = split_train_test_chronological(df_combined, ratio = .95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var2(t-7)</th>\n",
       "      <th>var3(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var2(t-6)</th>\n",
       "      <th>var3(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var2(t-5)</th>\n",
       "      <th>var3(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>...</th>\n",
       "      <th>var3(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var2(t-2)</th>\n",
       "      <th>var3(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.179461</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.180275</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.179032</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.179438</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-7)  var2(t-7)  var3(t-7)  var1(t-6)  var2(t-6)  var3(t-6)  \\\n",
       "7    0.005247   0.179461   0.003901   0.005594   0.180275   0.001623   \n",
       "8    0.005594   0.180275   0.001623   0.005727   0.179032   0.002740   \n",
       "9    0.005727   0.179032   0.002740   0.005882   0.176997   0.004602   \n",
       "10   0.005882   0.176997   0.004602   0.006007   0.176997   0.004491   \n",
       "11   0.006007   0.176997   0.004491   0.005908   0.179438   0.004092   \n",
       "\n",
       "    var1(t-5)  var2(t-5)  var3(t-5)  var1(t-4)  ...  var3(t-3)  var1(t-2)  \\\n",
       "7    0.005727   0.179032   0.002740   0.005882  ...   0.004491   0.005908   \n",
       "8    0.005882   0.176997   0.004602   0.006007  ...   0.004092   0.005895   \n",
       "9    0.006007   0.176997   0.004491   0.005908  ...   0.003908   0.005905   \n",
       "10   0.005908   0.179438   0.004092   0.005895  ...   0.003899   0.004164   \n",
       "11   0.005895   0.179642   0.003908   0.005905  ...   0.002629   0.004071   \n",
       "\n",
       "    var2(t-2)  var3(t-2)  var1(t-1)  var2(t-1)  var3(t-1)   var1(t)   var2(t)  \\\n",
       "7    0.179438   0.004092   0.005895   0.179642   0.003908  0.005905  0.179642   \n",
       "8    0.179642   0.003908   0.005905   0.179642   0.003899  0.004164  0.179642   \n",
       "9    0.179642   0.003899   0.004164   0.179642   0.002629  0.004071  0.179642   \n",
       "10   0.179642   0.002629   0.004071   0.179642   0.003160  0.004373  0.179642   \n",
       "11   0.179642   0.003160   0.004373   0.179642   0.003959  0.004777  0.179642   \n",
       "\n",
       "     var3(t)  \n",
       "7   0.003899  \n",
       "8   0.002629  \n",
       "9   0.003160  \n",
       "10  0.003959  \n",
       "11  0.003199  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "values = df_combined.drop('ds', axis = 1).values\n",
    "# integer encode direction\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 7, 1)\n",
    "\n",
    "reframed_2 = series_to_supervised(values, 7, 1)\n",
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed.iloc[:,-11].values\n",
    "X = reframed.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1, 23) (12000,) (341, 1, 23) (341,) (320, 1, 23) (320,)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "y = reframed_2.iloc[:,-11].values\n",
    "X = reframed_2.drop('var1(t)', axis =1).values\n",
    "# split into train and test sets\n",
    "\n",
    "n_train_hours = 12000\n",
    "n_test_hours = 320\n",
    "train_X = X[:n_train_hours,:]\n",
    "train_y = y[:n_train_hours]\n",
    "\n",
    "val_X= X[n_train_hours:-n_test_hours,]\n",
    "val_y= y[n_train_hours:-n_test_hours]\n",
    "\n",
    "test_X = X[-n_test_hours:,:]\n",
    "test_y = y[-n_test_hours:]\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape,val_X.shape, val_y.shape ,test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_perc = 0.40\n",
    "# lags = 50\n",
    "# look_back = 6       # 0 is a look_back of 1, 1 is a look_back of 2, etc.\n",
    "# predict_window = 1\n",
    "\n",
    "# # Neural net parameters\n",
    "# lstm_neurons = 16\n",
    "# dense_layers = []\n",
    "\n",
    "# model, predictions_test, actuals_test, predictions_train, actuals_train = main(df_train, df_val, train_perc,\n",
    "#                                                                                lags, look_back, \n",
    "#                                                                                predict_window, \n",
    "#                                                                                lstm_neurons, dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00524709, 0.17946108, 0.00390136, ..., 0.00390837,\n",
       "         0.17964192, 0.00389902]],\n",
       "\n",
       "       [[0.00559421, 0.18027489, 0.00162329, ..., 0.00389902,\n",
       "         0.17964192, 0.00262855]],\n",
       "\n",
       "       [[0.00572659, 0.1790316 , 0.00274013, ..., 0.00262855,\n",
       "         0.17964192, 0.0031601 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.14719364, 0.6875989 , 0.06145546, ..., 0.06514594,\n",
       "         0.71517694, 0.06405188]],\n",
       "\n",
       "       [[0.14818656, 0.6857679 , 0.06328142, ..., 0.06405188,\n",
       "         0.71517694, 0.06482349]],\n",
       "\n",
       "       [[0.1501584 , 0.67446536, 0.06471661, ..., 0.06482349,\n",
       "         0.71517694, 0.06527327]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14945447, 0.7185677 , 0.06664537, 0.14903203, 0.71517694,\n",
       "        0.06430598, 0.14952445, 0.71517694, 0.06589244, 0.14759782,\n",
       "        0.71517694, 0.06514594, 0.14829904, 0.71517694, 0.06405188,\n",
       "        0.1499469 , 0.71517694, 0.06482349, 0.15080851, 0.71517694,\n",
       "        0.06527327, 0.71517694, 0.06773242]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_3 = Sequential()\n",
    "LSTM_model_3.add(LSTM(64, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), dropout=0.05,recurrent_dropout=0.05))\n",
    "LSTM_model_3.add(Dense(64))\n",
    "# LSTM_model_3.add(LSTM(16, activation='relu'))\n",
    "LSTM_model_3.add(Dense(1))\n",
    "LSTM_model_3.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 341 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 79.1496 - val_loss: 178.8948\n",
      "Epoch 2/100\n",
      " - 2s - loss: 81.0312 - val_loss: 178.0877\n",
      "Epoch 3/100\n",
      " - 2s - loss: 81.0999 - val_loss: 177.9849\n",
      "Epoch 4/100\n",
      " - 2s - loss: 81.1355 - val_loss: 177.9613\n",
      "Epoch 5/100\n",
      " - 2s - loss: 81.1654 - val_loss: 177.9484\n",
      "Epoch 6/100\n",
      " - 2s - loss: 83.4045 - val_loss: 178.3477\n",
      "Epoch 7/100\n",
      " - 2s - loss: 81.1642 - val_loss: 177.9843\n",
      "Epoch 8/100\n",
      " - 2s - loss: 81.2057 - val_loss: 177.9368\n",
      "Epoch 9/100\n",
      " - 2s - loss: 81.2314 - val_loss: 177.9236\n",
      "Epoch 10/100\n",
      " - 2s - loss: 81.2536 - val_loss: 177.9150\n",
      "Epoch 11/100\n",
      " - 2s - loss: 81.2735 - val_loss: 177.9084\n",
      "Epoch 12/100\n",
      " - 2s - loss: 81.3353 - val_loss: 177.9114\n",
      "Epoch 13/100\n",
      " - 2s - loss: 81.3083 - val_loss: 177.8968\n",
      "Epoch 14/100\n",
      " - 2s - loss: 81.3240 - val_loss: 177.8909\n",
      "Epoch 15/100\n",
      " - 2s - loss: 81.3378 - val_loss: 177.8867\n",
      "Epoch 16/100\n",
      " - 2s - loss: 81.3503 - val_loss: 177.8829\n",
      "Epoch 17/100\n",
      " - 2s - loss: 81.3616 - val_loss: 177.8795\n",
      "Epoch 18/100\n",
      " - 2s - loss: 81.3717 - val_loss: 177.8766\n",
      "Epoch 19/100\n",
      " - 2s - loss: 81.3807 - val_loss: 177.8742\n",
      "Epoch 20/100\n",
      " - 2s - loss: 81.3888 - val_loss: 177.8720\n",
      "Epoch 21/100\n",
      " - 2s - loss: 81.3960 - val_loss: 177.8702\n",
      "Epoch 22/100\n",
      " - 2s - loss: 81.4023 - val_loss: 177.8686\n",
      "Epoch 23/100\n",
      " - 2s - loss: 81.4079 - val_loss: 177.8673\n",
      "Epoch 24/100\n",
      " - 2s - loss: 81.4128 - val_loss: 177.8661\n",
      "Epoch 25/100\n",
      " - 2s - loss: 81.4170 - val_loss: 177.8653\n",
      "Epoch 26/100\n",
      " - 2s - loss: 81.4207 - val_loss: 177.8645\n",
      "Epoch 27/100\n",
      " - 2s - loss: 81.5203 - val_loss: 178.0117\n",
      "Epoch 28/100\n",
      " - 2s - loss: 81.4202 - val_loss: 177.8790\n",
      "Epoch 29/100\n",
      " - 2s - loss: 81.4279 - val_loss: 177.8646\n",
      "Epoch 30/100\n",
      " - 2s - loss: 81.4302 - val_loss: 177.8633\n",
      "Epoch 31/100\n",
      " - 2s - loss: 81.4317 - val_loss: 177.8630\n",
      "Epoch 32/100\n",
      " - 2s - loss: 81.4327 - val_loss: 177.8631\n",
      "Epoch 33/100\n",
      " - 2s - loss: 81.4334 - val_loss: 177.8633\n",
      "Epoch 34/100\n",
      " - 2s - loss: 81.4371 - val_loss: 177.8644\n",
      "Epoch 35/100\n",
      " - 2s - loss: 81.4339 - val_loss: 177.8638\n",
      "Epoch 36/100\n",
      " - 2s - loss: 81.4339 - val_loss: 177.8639\n",
      "Epoch 37/100\n",
      " - 2s - loss: 81.4336 - val_loss: 177.8644\n",
      "Epoch 38/100\n",
      " - 2s - loss: 81.4331 - val_loss: 177.8648\n",
      "Epoch 39/100\n",
      " - 2s - loss: 81.4325 - val_loss: 177.8653\n",
      "Epoch 40/100\n",
      " - 2s - loss: 81.4316 - val_loss: 177.8659\n",
      "Epoch 41/100\n",
      " - 2s - loss: 81.4307 - val_loss: 177.8665\n",
      "Epoch 42/100\n",
      " - 2s - loss: 81.4296 - val_loss: 177.8671\n",
      "Epoch 43/100\n",
      " - 2s - loss: 81.4283 - val_loss: 177.8678\n",
      "Epoch 44/100\n",
      " - 2s - loss: 81.4270 - val_loss: 177.8685\n",
      "Epoch 45/100\n",
      " - 2s - loss: 81.4255 - val_loss: 177.8693\n",
      "Epoch 46/100\n",
      " - 2s - loss: 81.4240 - val_loss: 177.8700\n",
      "Epoch 47/100\n",
      " - 2s - loss: 81.4502 - val_loss: 177.8371\n",
      "Epoch 48/100\n",
      " - 2s - loss: 81.4220 - val_loss: 177.8679\n",
      "Epoch 49/100\n",
      " - 2s - loss: 81.4191 - val_loss: 177.8721\n",
      "Epoch 50/100\n",
      " - 2s - loss: 81.4171 - val_loss: 177.8732\n",
      "Epoch 51/100\n",
      " - 2s - loss: 81.4152 - val_loss: 177.8741\n",
      "Epoch 52/100\n",
      " - 2s - loss: 86.1017 - val_loss: 181.0606\n",
      "Epoch 53/100\n",
      " - 2s - loss: 81.0953 - val_loss: 178.3688\n",
      "Epoch 54/100\n",
      " - 2s - loss: 81.2014 - val_loss: 177.9861\n",
      "Epoch 55/100\n",
      " - 2s - loss: 81.2323 - val_loss: 177.9418\n",
      "Epoch 56/100\n",
      " - 2s - loss: 81.2469 - val_loss: 177.9330\n",
      "Epoch 57/100\n",
      " - 2s - loss: 81.3333 - val_loss: 177.9425\n",
      "Epoch 58/100\n",
      " - 2s - loss: 81.2685 - val_loss: 177.9261\n",
      "Epoch 59/100\n",
      " - 2s - loss: 81.2786 - val_loss: 177.9214\n",
      "Epoch 60/100\n",
      " - 2s - loss: 81.2872 - val_loss: 177.9184\n",
      "Epoch 61/100\n",
      " - 2s - loss: 81.2949 - val_loss: 177.9159\n",
      "Epoch 62/100\n",
      " - 2s - loss: 81.3017 - val_loss: 177.9137\n",
      "Epoch 63/100\n",
      " - 2s - loss: 81.3077 - val_loss: 177.9119\n",
      "Epoch 64/100\n",
      " - 2s - loss: 81.3130 - val_loss: 177.9104\n",
      "Epoch 65/100\n",
      " - 2s - loss: 81.3176 - val_loss: 177.9092\n",
      "Epoch 66/100\n",
      " - 2s - loss: 81.3217 - val_loss: 177.9081\n",
      "Epoch 67/100\n",
      " - 2s - loss: 81.3252 - val_loss: 177.9072\n",
      "Epoch 68/100\n",
      " - 2s - loss: 81.3281 - val_loss: 177.9064\n",
      "Epoch 69/100\n",
      " - 2s - loss: 81.3306 - val_loss: 177.9059\n",
      "Epoch 70/100\n",
      " - 2s - loss: 81.3327 - val_loss: 177.9056\n",
      "Epoch 71/100\n",
      " - 2s - loss: 81.3343 - val_loss: 177.9053\n",
      "Epoch 72/100\n",
      " - 2s - loss: 81.3356 - val_loss: 177.9052\n",
      "Epoch 73/100\n",
      " - 2s - loss: 81.3366 - val_loss: 177.9052\n",
      "Epoch 74/100\n",
      " - 2s - loss: 95.5954 - val_loss: 184.8038\n",
      "Epoch 75/100\n",
      " - 2s - loss: 80.7340 - val_loss: 178.8449\n",
      "Epoch 76/100\n",
      " - 2s - loss: 81.0097 - val_loss: 178.1839\n",
      "Epoch 77/100\n",
      " - 2s - loss: 81.0471 - val_loss: 178.0396\n",
      "Epoch 78/100\n",
      " - 2s - loss: 81.0748 - val_loss: 178.0136\n",
      "Epoch 79/100\n",
      " - 2s - loss: 81.0959 - val_loss: 178.0023\n",
      "Epoch 80/100\n",
      " - 2s - loss: 81.1148 - val_loss: 177.9936\n",
      "Epoch 81/100\n",
      " - 2s - loss: 81.1322 - val_loss: 177.9858\n",
      "Epoch 82/100\n",
      " - 2s - loss: 81.1483 - val_loss: 177.9790\n",
      "Epoch 83/100\n",
      " - 2s - loss: 81.1630 - val_loss: 177.9729\n",
      "Epoch 84/100\n",
      " - 2s - loss: 81.1765 - val_loss: 177.9673\n",
      "Epoch 85/100\n",
      " - 2s - loss: 81.1888 - val_loss: 177.9626\n",
      "Epoch 86/100\n",
      " - 2s - loss: 81.1999 - val_loss: 177.9583\n",
      "Epoch 87/100\n",
      " - 2s - loss: 81.2101 - val_loss: 177.9545\n",
      "Epoch 88/100\n",
      " - 2s - loss: 81.2192 - val_loss: 177.9510\n",
      "Epoch 89/100\n",
      " - 2s - loss: 81.2275 - val_loss: 177.9483\n",
      "Epoch 90/100\n",
      " - 2s - loss: 81.2348 - val_loss: 177.9457\n",
      "Epoch 91/100\n",
      " - 2s - loss: 81.2414 - val_loss: 177.9434\n",
      "Epoch 92/100\n",
      " - 2s - loss: 81.2473 - val_loss: 177.9415\n",
      "Epoch 93/100\n",
      " - 2s - loss: 81.2525 - val_loss: 177.9401\n",
      "Epoch 94/100\n",
      " - 2s - loss: 81.2570 - val_loss: 177.9386\n",
      "Epoch 95/100\n",
      " - 2s - loss: 81.2609 - val_loss: 177.9377\n",
      "Epoch 96/100\n",
      " - 2s - loss: 81.2643 - val_loss: 177.9368\n",
      "Epoch 97/100\n",
      " - 2s - loss: 81.2672 - val_loss: 177.9359\n",
      "Epoch 98/100\n",
      " - 2s - loss: 81.2696 - val_loss: 177.9354\n",
      "Epoch 99/100\n",
      " - 2s - loss: 81.2717 - val_loss: 177.9351\n",
      "Epoch 100/100\n",
      " - 2s - loss: 81.2733 - val_loss: 177.9347\n"
     ]
    }
   ],
   "source": [
    "history_3 = LSTM_model_3.fit(train_X, train_y, \n",
    "                    epochs= 100, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(val_X, val_y),\n",
    "                    verbose=2,\n",
    "#                     callbacks=[earlystopper],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 2s - loss: 81.2761\n",
      "Epoch 2/2\n",
      " - 2s - loss: 81.2765\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-74d8ba26086c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_refitting_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_model_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-25ab3ef60830>\u001b[0m in \u001b[0;36mmake_refitting_window\u001b[0;34m(model, train_x, train_y, test_x, test_y)\u001b[0m\n\u001b[1;32m     12\u001b[0m                     shuffle=False)\n\u001b[1;32m     13\u001b[0m         \u001b[0mpred_Y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpredict_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_Y_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "make_refitting_window(LSTM_model_3, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3733.65 ,   42.728, 3806.16 , ..., 3782.67 ,   42.578,\n",
       "         3824.77 ]],\n",
       "\n",
       "       [[3725.8  ,   42.578, 3766.11 , ..., 3824.77 ,   42.578,\n",
       "         3863.66 ]],\n",
       "\n",
       "       [[3734.95 ,   42.578, 3793.27 , ..., 3863.66 ,   42.578,\n",
       "         3880.15 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3611.61 ,   39.267, 3858.66 , ..., 3872.64 ,   39.267,\n",
       "         3874.46 ]],\n",
       "\n",
       "       [[3610.88 ,   39.267, 3840.87 , ..., 3874.46 ,   39.267,\n",
       "         3910.71 ]],\n",
       "\n",
       "       [[3613.49 ,   39.267, 3853.27 , ..., 3910.71 ,   39.267,\n",
       "         3902.34 ]]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3733.65 ,   42.728, 3806.16 , 3725.8  ,   42.578, 3766.11 ,\n",
       "        3734.95 ,   42.578, 3793.27 , 3699.15 ,   42.578, 3780.49 ,\n",
       "        3712.18 ,   42.578, 3761.76 , 3742.8  ,   42.578, 3774.97 ,\n",
       "        3758.81 ,   42.578, 3782.67 ,   42.578, 3824.77 ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
