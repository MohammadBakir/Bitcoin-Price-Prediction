{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following Cell contains a function that will scrape bitcoin talk forum using selenium webdriver for links to individual posts that users generate to discuss bitcoin news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set chromedriver executable path. \n",
    "chromedriver = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\" # path to the chromedriver executable, set accordingly\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#Define function to extract forum discussion links. \n",
    "def get_bitcoin_news_links(url, start_date, end_date):\n",
    "    \"\"\"\n",
    "    A function utilizing selenium to scrape bitcoin 15-min interval price history from bitcoin char\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    df = pd.DataFrame(columns=['timestamp','open','high','low','close','volume_btc','volume_currency','weighted price'])\n",
    "    print('Scraping Bitcoin News Links\\n')\n",
    "\n",
    "    #Establish link\n",
    "    bitcoinnews_url=url\n",
    "\n",
    "    #Utilize selenium drive to launch link\n",
    "    driver.get(bitcoinnews_url)\n",
    "#     driver.fullscreen_window()\n",
    "    driver.find_element_by_xpath(\"//*[@id='c']\").click()\n",
    "    \n",
    "    search_bar = driver.find_element_by_id(\"i\")\n",
    "    for option in search_bar.find_elements_by_tag_name('option'):\n",
    "        if option.text == '15-min':\n",
    "            option.click() \n",
    "            break\n",
    "    \n",
    "    start_date_input = driver.find_element_by_id(\"s\")\n",
    "    start_date_input.clear()\n",
    "    start_date_input.click()\n",
    "    start_date_input.send_keys('2019-05-01')\n",
    "    start_date_input.send_keys(Keys.RETURN)\n",
    "    \n",
    "    end_date_input = driver.find_element_by_id(\"e\")\n",
    "    end_date_input.clear()\n",
    "    end_date_input.click()\n",
    "    end_date_input.send_keys('2019-05-30')\n",
    "    end_date_input.send_keys(Keys.RETURN)\n",
    "   \n",
    "    \n",
    "    element = driver.find_element_by_xpath('//*[@id=\"content_chart\"]/div/div[2]/a')\n",
    "    driver.execute_script(\"return arguments[0].scrollIntoView(0, document.documentElement.scrollHeight-10);\", element)\n",
    "    \n",
    "    #element = driver.find_element_by_xpath('//*[@id=\"content_chart\"]/div/div[2]/a')\n",
    "    #element.find_elements_by_xpath('//*[@id=\"content_chart\"]/div/div[2]/a')\n",
    "    #element.click()\n",
    "\n",
    "#     for row in rows:\n",
    "#         link = row.find_element(By.TAG_NAME, \"a\")\n",
    "#         a_elements.append(link.get_attribute('href'))\n",
    "\n",
    "        \n",
    "    print('Scraping Done!\\n')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Bitcoin News Links\n",
      "\n",
      "Scraping Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define site URL for scraping\n",
    "url=\"https://bitcoincharts.com/charts/bitstampUSD#rg60ztgSzm1g10zm2g25zv\"\n",
    "\n",
    "#Scrap News Article Posts. Only run once then comment out, will take approximately 5-10min\n",
    "forum_subject_links = get_bitcoin_news_links(url,'t','t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set chromedriver executable path. \n",
    "chromedriver = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#Define function to extract walkability, transit, and bike indexes\n",
    "def get_info_and_scores(url, neighborhood_list):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    df = pd.DataFrame(columns=['neighborhood', 'walk_score_of_100','transit_score_of_100','bike_score_of_100','median_sale_price_$'])\n",
    "    print('Scraping Neighborhoods from Redfin\\n')\n",
    "    \n",
    "    for neighborhood in neighborhood_list:\n",
    "        print('Scraping ' + neighborhood)\n",
    "        redfin_url=url\n",
    "        driver.get(redfin_url)\n",
    "        \n",
    "        search_bar = driver.find_element_by_id(\"search-box-input\")\n",
    "        search_bar.send_keys(neighborhood + ', San Francisco, CA')\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Wait 10 seconds for page to load\n",
    "        WebDriverWait(driver, 10).until(EC.url_changes(redfin_url))\n",
    "        \n",
    "        try:\n",
    "            #Find div containing walk-scores class and median sale price \n",
    "            content_walk = driver.find_element_by_xpath(\"//div[contains(@class, 'walk-score')]\")\n",
    "            content_med_sale_price = driver.find_element_by_xpath(\"//div[contains(@class, 'trends')]//li[5]//div//span[2]//span\")\n",
    "            \n",
    "            #convert div html to text\n",
    "            content_walk_text =content_walk.text\n",
    "            content_med_sale_text = content_med_sale_price.text\n",
    "            \n",
    "            #Extract out of 100 scores from div\n",
    "            info_scores = [int(s) for s in content_walk_text.split() if s.isdigit()]\n",
    "            info_scores = [info_scores[0],info_scores[2],info_scores[4]]\n",
    "            \n",
    "            #Add neighborhood and median sale price to info_list\n",
    "            info_scores.insert(0,neighborhood)\n",
    "            info_scores.insert(len(info_scores),content_med_sale_text)\n",
    "        \n",
    "            #add to walkability dataframe\n",
    "            df.loc[-1] = info_scores  # adding a row\n",
    "            df.index = df.index + 1  # shifting index\n",
    "            df = df.sort_index()\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            filler=[neighborhood,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "            df.loc[-1] = filler  # adding a row\n",
    "            df.index = df.index + 1  # shifting index\n",
    "            df = df.sort_index()\n",
    "    \n",
    "    print('Scraping Done!')\n",
    "            \n",
    "    return df\n",
    "\n",
    "#Define site URL for scraping\n",
    "url=\"https://www.redfin.com/\"\n",
    "\n",
    "#Define neighborhood list as available in the SF data\n",
    "neighborhood_list = ['Western Addition', 'Bernal Heights', 'Haight Ashbury', 'Mission',\\\n",
    "       'Potrero Hill', 'Civic Center / Van Ness', 'Castro','Upper Market',\\\n",
    "       'Inner Sunset', 'South of Market', 'Noe Valley', 'Outer Richmond',\\\n",
    "       'Presidio Heights', 'Nob Hill', 'Ocean View Terrace', 'Pacific Heights',\\\n",
    "       'Financial District', 'Twin Peaks', 'Russian Hill', 'Outer Sunset',\\\n",
    "       'North Beach', 'Glen Park', 'Marina Distric', 'Inner Richmond',\\\n",
    "       'Excelsior', 'Seacliff', 'Chinatown', 'Bayview', 'Diamond Heights',\\\n",
    "       'West of Twin Peaks', 'Outer Mission', 'Parkside', 'Lakeshore',\\\n",
    "       'Crocker Amazon', 'Golden Gate Park', 'Visitacion Valley','Presidio Heights']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
